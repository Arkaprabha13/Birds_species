{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zzgsp3InBmBk"
      },
      "outputs": [],
      "source": [
        "# Set seeds for reproducibility\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ekv8yq97B1BN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-UZgCnTxB16a"
      },
      "outputs": [],
      "source": [
        "kaggle_credentails = json.load(open(\"kaggle.json\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_lc9l51iB2um"
      },
      "outputs": [],
      "source": [
        "\n",
        "# setup Kaggle API key as environment variables\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_credentails[\"username\"]\n",
        "os.environ['KAGGLE_KEY'] = kaggle_credentails[\"key\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfrpKbbBB3iH",
        "outputId": "c7ad6d3f-eb59-413f-8300-e08fbc1dc362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 100-bird-species.zip to /content\n",
            "100% 1.95G/1.96G [01:30<00:00, 24.4MB/s]\n",
            "100% 1.96G/1.96G [01:30<00:00, 23.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d gpiosenka/100-bird-species"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v1wEd9fyCjSz"
      },
      "outputs": [],
      "source": [
        "# Unzip the downloaded dataset\n",
        "with ZipFile(\"/content/100-bird-species.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFU_SIFRB5hX",
        "outputId": "76c12774-0ce7-4a20-fac0-3a6e7baaa50c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 84635 files belonging to 525 classes.\n",
            "Found 2625 files belonging to 525 classes.\n",
            "Found 2625 files belonging to 525 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "train_dataset = image_dataset_from_directory(\n",
        " \"/content/train\",\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "   \"/content/test\",\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32)\n",
        "validata = image_dataset_from_directory(\n",
        "   \"/content/valid\",\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgbsus9EE8nX",
        "outputId": "9db25ef9-737a-4054-8ad6-3a0191777abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 84635 images belonging to 525 classes.\n",
            "Found 2625 images belonging to 525 classes.\n",
            "Found 2625 images belonging to 525 classes.\n"
          ]
        }
      ],
      "source": [
        "# Rescale\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "valid_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "# data transfer from directories to batches\n",
        "train_data = train_datagen.flow_from_directory(directory = \"/content/train\",\n",
        "                                               batch_size= 32,\n",
        "                                               target_size= (224,224),\n",
        "                                               class_mode = \"categorical\")\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(directory = \"/content/test\",\n",
        "                                               batch_size = 32,\n",
        "                                               target_size = (224,224),\n",
        "                                               class_mode = \"categorical\")\n",
        "\n",
        "val_data = valid_datagen.flow_from_directory(directory = \"/content/valid\",\n",
        "                                               batch_size = 32,\n",
        "                                               target_size = (224,224),\n",
        "                                               class_mode = \"categorical\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIE3XnZJC2-N",
        "outputId": "8cc828e8-2500-41dc-c8c0-c3c44f85aa77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ABBOTTS BOOBY' 'ABYSSINIAN GROUND HORNBILL' 'AFRICAN CROWNED CRANE'\n",
            " 'AFRICAN EMERALD CUCKOO' 'AFRICAN FIREFINCH' 'AFRICAN OYSTER CATCHER'\n",
            " 'AFRICAN PIED HORNBILL' 'AFRICAN PYGMY GOOSE' 'ALBATROSS']\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import numpy as np\n",
        "\n",
        "data_dir = pathlib.Path(\"/content/train\")\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")])) # creating a list of class names from subdirectory\n",
        "print(class_names[1:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKMHpLNODdAR",
        "outputId": "079c1f71-30fc-4764-c14e-f2ecb5702a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQskBNdhDB2R",
        "outputId": "a9ab3284-3f4b-4899-bccb-13add59168c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3, ResNet50, EfficientNetB0, MobileNetV2, DenseNet121\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "94fD7oLNDQKK"
      },
      "outputs": [],
      "source": [
        "def create_model(base_model):\n",
        "    # Remove the top layer\n",
        "    base_model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
        "\n",
        "    # Add a global spatial average pooling layer\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Add a fully-connected layer\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "    # Add a logistic layer with 525 output units (one for each bird species)\n",
        "    predictions = Dense(525, activation='softmax')(x)\n",
        "\n",
        "    # This is the model we will train\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # First: train only the top layers (which were randomly initialized)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=[F1Score(num_classes=525, average='macro'), 'accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXvAY5pjDmol",
        "outputId": "a9eaa9e1-bca1-4cf4-8203-67ae620ee97e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "2645/2645 [==============================] - 214s 79ms/step - loss: 3.6790 - f1_score: 0.3551 - accuracy: 0.3633 - val_loss: 2.2075 - val_f1_score: 0.5628 - val_accuracy: 0.5874\n",
            "Epoch 2/5\n",
            "2645/2645 [==============================] - 198s 75ms/step - loss: 1.9228 - f1_score: 0.6034 - accuracy: 0.6136 - val_loss: 1.4673 - val_f1_score: 0.6799 - val_accuracy: 0.6949\n",
            "Epoch 3/5\n",
            "2645/2645 [==============================] - 200s 76ms/step - loss: 1.4752 - f1_score: 0.6732 - accuracy: 0.6789 - val_loss: 1.1850 - val_f1_score: 0.7167 - val_accuracy: 0.7269\n",
            "Epoch 4/5\n",
            "2645/2645 [==============================] - 199s 75ms/step - loss: 1.2545 - f1_score: 0.7106 - accuracy: 0.7143 - val_loss: 1.0324 - val_f1_score: 0.7528 - val_accuracy: 0.7627\n",
            "Epoch 5/5\n",
            "2645/2645 [==============================] - 199s 75ms/step - loss: 1.1168 - f1_score: 0.7378 - accuracy: 0.7406 - val_loss: 0.9344 - val_f1_score: 0.7614 - val_accuracy: 0.7699\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 5s 0us/step\n",
            "Epoch 1/5\n",
            "2645/2645 [==============================] - 309s 115ms/step - loss: 6.2779 - f1_score: 1.4038e-04 - accuracy: 0.0026 - val_loss: 6.2634 - val_f1_score: 7.2424e-06 - val_accuracy: 0.0019\n",
            "Epoch 2/5\n",
            "2645/2645 [==============================] - 299s 113ms/step - loss: 6.2622 - f1_score: 2.1283e-05 - accuracy: 0.0030 - val_loss: 6.2635 - val_f1_score: 7.2424e-06 - val_accuracy: 0.0019\n",
            "Epoch 3/5\n",
            "2645/2645 [==============================] - 300s 113ms/step - loss: 6.2614 - f1_score: 3.6668e-05 - accuracy: 0.0031 - val_loss: 6.2636 - val_f1_score: 7.2424e-06 - val_accuracy: 0.0019\n",
            "Epoch 4/5\n",
            "2645/2645 [==============================] - 298s 113ms/step - loss: 6.2607 - f1_score: 1.1801e-05 - accuracy: 0.0031 - val_loss: 6.2638 - val_f1_score: 7.2424e-06 - val_accuracy: 0.0019\n",
            "Epoch 5/5\n",
            "2645/2645 [==============================] - 299s 113ms/step - loss: 6.2601 - f1_score: 1.1801e-05 - accuracy: 0.0031 - val_loss: 6.2640 - val_f1_score: 7.2424e-06 - val_accuracy: 0.0019\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 2s 0us/step\n",
            "Epoch 1/5\n",
            "2645/2645 [==============================] - 202s 73ms/step - loss: 6.2811 - f1_score: 4.0324e-04 - accuracy: 0.0027 - val_loss: 6.2634 - val_f1_score: 7.2424e-06 - val_accuracy: 0.0019\n",
            "Epoch 2/5\n",
            "2645/2645 [==============================] - 190s 72ms/step - loss: 6.2623 - f1_score: 2.3058e-05 - accuracy: 0.0031 - val_loss: 6.2635 - val_f1_score: 7.2424e-06 - val_accuracy: 0.0019\n",
            "Epoch 3/5\n",
            "2645/2645 [==============================] - 189s 71ms/step - loss: 6.2614 - f1_score: 2.3180e-05 - accuracy: 0.0031 - val_loss: 6.2636 - val_f1_score: 7.2424e-06 - val_accuracy: 0.0019\n",
            "Epoch 4/5\n",
            "2645/2645 [==============================] - 189s 72ms/step - loss: 6.2607 - f1_score: 2.3804e-05 - accuracy: 0.0031 - val_loss: 6.2638 - val_f1_score: 7.2424e-06 - val_accuracy: 0.0019\n",
            "Epoch 5/5\n",
            "2645/2645 [==============================] - 191s 72ms/step - loss: 6.2601 - f1_score: 3.0024e-05 - accuracy: 0.0031 - val_loss: 6.2640 - val_f1_score: 7.2424e-06 - val_accuracy: 0.0019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 1s 0us/step\n",
            "Epoch 1/5\n",
            "2645/2645 [==============================] - 186s 69ms/step - loss: 2.8522 - f1_score: 0.4028 - accuracy: 0.4067 - val_loss: 1.2947 - val_f1_score: 0.6497 - val_accuracy: 0.6640\n",
            "Epoch 2/5\n",
            "2645/2645 [==============================] - 169s 64ms/step - loss: 1.1912 - f1_score: 0.6905 - accuracy: 0.6924 - val_loss: 0.9477 - val_f1_score: 0.7364 - val_accuracy: 0.7444\n",
            "Epoch 3/5\n",
            "2645/2645 [==============================] - 184s 69ms/step - loss: 0.9160 - f1_score: 0.7550 - accuracy: 0.7561 - val_loss: 0.7971 - val_f1_score: 0.7725 - val_accuracy: 0.7798\n",
            "Epoch 4/5\n",
            "2645/2645 [==============================] - 173s 66ms/step - loss: 0.7620 - f1_score: 0.7932 - accuracy: 0.7940 - val_loss: 0.7076 - val_f1_score: 0.7951 - val_accuracy: 0.8038\n",
            "Epoch 5/5\n",
            "2645/2645 [==============================] - 172s 65ms/step - loss: 0.6589 - f1_score: 0.8183 - accuracy: 0.8192 - val_loss: 0.6715 - val_f1_score: 0.8084 - val_accuracy: 0.8152\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29084464/29084464 [==============================] - 2s 0us/step\n",
            "Epoch 1/5\n",
            "2645/2645 [==============================] - 287s 105ms/step - loss: 2.0102 - f1_score: 0.6236 - accuracy: 0.6218 - val_loss: 0.6716 - val_f1_score: 0.8486 - val_accuracy: 0.8514\n",
            "Epoch 2/5\n",
            "2645/2645 [==============================] - 269s 102ms/step - loss: 0.6374 - f1_score: 0.8512 - accuracy: 0.8520 - val_loss: 0.4570 - val_f1_score: 0.8884 - val_accuracy: 0.8918\n",
            "Epoch 3/5\n",
            "2645/2645 [==============================] - 271s 102ms/step - loss: 0.4430 - f1_score: 0.8925 - accuracy: 0.8930 - val_loss: 0.3563 - val_f1_score: 0.9055 - val_accuracy: 0.9078\n",
            "Epoch 4/5\n",
            "2645/2645 [==============================] - 272s 103ms/step - loss: 0.3405 - f1_score: 0.9179 - accuracy: 0.9182 - val_loss: 0.3189 - val_f1_score: 0.9151 - val_accuracy: 0.9170\n",
            "Epoch 5/5\n",
            "2645/2645 [==============================] - 270s 102ms/step - loss: 0.2710 - f1_score: 0.9348 - accuracy: 0.9351 - val_loss: 0.3015 - val_f1_score: 0.9124 - val_accuracy: 0.9139\n"
          ]
        }
      ],
      "source": [
        "# List of base models\n",
        "base_models = [InceptionV3, ResNet50, EfficientNetB0, MobileNetV2, DenseNet121]\n",
        "\n",
        "# Dictionary to store models and their names\n",
        "models = {}\n",
        "\n",
        "for model_name, base_model in zip(['InceptionV3', 'ResNet50', 'EfficientNetB0', 'MobileNetV2', 'DenseNet121'], base_models):\n",
        "    # Create and compile the model\n",
        "    model = create_model(base_model(weights='imagenet', include_top=False))\n",
        "    models[model_name] = model\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_data, steps_per_epoch=len(train_data), validation_data=val_data, validation_steps=len(val_data), epochs=5)\n",
        "\n",
        "    # Optionally, save the model for later use\n",
        "    model.save(f'{model_name}_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgnx_DZQDpo6",
        "outputId": "57d30b64-1e2d-4219-96e6-e3a3ca1fa692"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_items([('InceptionV3', <keras.src.engine.functional.Functional object at 0x7e84245ad840>), ('ResNet50', <keras.src.engine.functional.Functional object at 0x7e83606c5450>), ('EfficientNetB0', <keras.src.engine.functional.Functional object at 0x7e8424b08d90>), ('MobileNetV2', <keras.src.engine.functional.Functional object at 0x7e835f6a1120>), ('DenseNet121', <keras.src.engine.functional.Functional object at 0x7e8424a6af50>)])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models.items()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca7I7XwQEd90",
        "outputId": "d163da2c-669d-4bf1-b1b6-e052f5edffe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83/83 [==============================] - 7s 75ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "InceptionV3 Metrics:\n",
            "Accuracy: 0.001142857142857143\n",
            "F1 Score: 0.0012714904143475575\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         5\n",
            "           1       0.00      0.00      0.00         5\n",
            "           2       0.00      0.00      0.00         5\n",
            "           3       0.00      0.00      0.00         5\n",
            "           4       0.00      0.00      0.00         5\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.00      0.00      0.00         5\n",
            "           7       0.00      0.00      0.00         5\n",
            "           8       0.00      0.00      0.00         5\n",
            "           9       0.00      0.00      0.00         5\n",
            "          10       0.00      0.00      0.00         5\n",
            "          11       0.00      0.00      0.00         5\n",
            "          12       0.00      0.00      0.00         5\n",
            "          13       0.00      0.00      0.00         5\n",
            "          14       0.00      0.00      0.00         5\n",
            "          15       0.00      0.00      0.00         5\n",
            "          16       0.00      0.00      0.00         5\n",
            "          17       0.00      0.00      0.00         5\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00         5\n",
            "          20       0.00      0.00      0.00         5\n",
            "          21       0.00      0.00      0.00         5\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.00      0.00      0.00         5\n",
            "          24       0.00      0.00      0.00         5\n",
            "          25       0.00      0.00      0.00         5\n",
            "          26       0.00      0.00      0.00         5\n",
            "          27       0.00      0.00      0.00         5\n",
            "          28       0.00      0.00      0.00         5\n",
            "          29       0.00      0.00      0.00         5\n",
            "          30       0.00      0.00      0.00         5\n",
            "          31       0.00      0.00      0.00         5\n",
            "          32       0.00      0.00      0.00         5\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.00      0.00      0.00         5\n",
            "          35       0.00      0.00      0.00         5\n",
            "          36       0.00      0.00      0.00         5\n",
            "          37       0.00      0.00      0.00         5\n",
            "          38       0.00      0.00      0.00         5\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00         5\n",
            "          41       0.00      0.00      0.00         5\n",
            "          42       0.00      0.00      0.00         5\n",
            "          43       0.00      0.00      0.00         5\n",
            "          44       0.00      0.00      0.00         5\n",
            "          45       0.00      0.00      0.00         5\n",
            "          46       0.00      0.00      0.00         5\n",
            "          47       0.00      0.00      0.00         5\n",
            "          48       0.00      0.00      0.00         5\n",
            "          49       0.00      0.00      0.00         5\n",
            "          50       0.00      0.00      0.00         5\n",
            "          51       0.00      0.00      0.00         5\n",
            "          52       0.00      0.00      0.00         5\n",
            "          53       0.00      0.00      0.00         5\n",
            "          54       0.00      0.00      0.00         5\n",
            "          55       0.00      0.00      0.00         5\n",
            "          56       0.00      0.00      0.00         5\n",
            "          57       0.00      0.00      0.00         5\n",
            "          58       0.00      0.00      0.00         5\n",
            "          59       0.00      0.00      0.00         5\n",
            "          60       0.00      0.00      0.00         5\n",
            "          61       0.00      0.00      0.00         5\n",
            "          62       0.00      0.00      0.00         5\n",
            "          63       0.00      0.00      0.00         5\n",
            "          64       0.00      0.00      0.00         5\n",
            "          65       0.00      0.00      0.00         5\n",
            "          66       0.00      0.00      0.00         5\n",
            "          67       0.00      0.00      0.00         5\n",
            "          68       0.00      0.00      0.00         5\n",
            "          69       0.00      0.00      0.00         5\n",
            "          70       0.00      0.00      0.00         5\n",
            "          71       0.00      0.00      0.00         5\n",
            "          72       0.00      0.00      0.00         5\n",
            "          73       0.00      0.00      0.00         5\n",
            "          74       0.00      0.00      0.00         5\n",
            "          75       0.00      0.00      0.00         5\n",
            "          76       0.00      0.00      0.00         5\n",
            "          77       0.00      0.00      0.00         5\n",
            "          78       0.00      0.00      0.00         5\n",
            "          79       0.00      0.00      0.00         5\n",
            "          80       0.00      0.00      0.00         5\n",
            "          81       0.00      0.00      0.00         5\n",
            "          82       0.00      0.00      0.00         5\n",
            "          83       0.00      0.00      0.00         5\n",
            "          84       0.00      0.00      0.00         5\n",
            "          85       0.00      0.00      0.00         5\n",
            "          86       0.00      0.00      0.00         5\n",
            "          87       0.00      0.00      0.00         5\n",
            "          88       0.00      0.00      0.00         5\n",
            "          89       0.00      0.00      0.00         5\n",
            "          90       0.00      0.00      0.00         5\n",
            "          91       0.00      0.00      0.00         5\n",
            "          92       0.00      0.00      0.00         5\n",
            "          93       0.00      0.00      0.00         5\n",
            "          94       0.00      0.00      0.00         5\n",
            "          95       0.00      0.00      0.00         5\n",
            "          96       0.00      0.00      0.00         5\n",
            "          97       0.00      0.00      0.00         5\n",
            "          98       0.00      0.00      0.00         5\n",
            "          99       0.00      0.00      0.00         5\n",
            "         100       0.00      0.00      0.00         5\n",
            "         101       0.00      0.00      0.00         5\n",
            "         102       0.00      0.00      0.00         5\n",
            "         103       0.00      0.00      0.00         5\n",
            "         104       0.00      0.00      0.00         5\n",
            "         105       0.00      0.00      0.00         5\n",
            "         106       0.00      0.00      0.00         5\n",
            "         107       0.00      0.00      0.00         5\n",
            "         108       0.00      0.00      0.00         5\n",
            "         109       0.00      0.00      0.00         5\n",
            "         110       0.00      0.00      0.00         5\n",
            "         111       0.00      0.00      0.00         5\n",
            "         112       0.00      0.00      0.00         5\n",
            "         113       0.00      0.00      0.00         5\n",
            "         114       0.00      0.00      0.00         5\n",
            "         115       0.00      0.00      0.00         5\n",
            "         116       0.00      0.00      0.00         5\n",
            "         117       0.00      0.00      0.00         5\n",
            "         118       0.00      0.00      0.00         5\n",
            "         119       0.00      0.00      0.00         5\n",
            "         120       0.00      0.00      0.00         5\n",
            "         121       0.00      0.00      0.00         5\n",
            "         122       0.00      0.00      0.00         5\n",
            "         123       0.00      0.00      0.00         5\n",
            "         124       0.00      0.00      0.00         5\n",
            "         125       0.00      0.00      0.00         5\n",
            "         126       0.00      0.00      0.00         5\n",
            "         127       0.00      0.00      0.00         5\n",
            "         128       0.00      0.00      0.00         5\n",
            "         129       0.00      0.00      0.00         5\n",
            "         130       0.00      0.00      0.00         5\n",
            "         131       0.00      0.00      0.00         5\n",
            "         132       0.00      0.00      0.00         5\n",
            "         133       0.00      0.00      0.00         5\n",
            "         134       0.00      0.00      0.00         5\n",
            "         135       0.00      0.00      0.00         5\n",
            "         136       0.00      0.00      0.00         5\n",
            "         137       0.00      0.00      0.00         5\n",
            "         138       0.00      0.00      0.00         5\n",
            "         139       0.00      0.00      0.00         5\n",
            "         140       0.00      0.00      0.00         5\n",
            "         141       0.00      0.00      0.00         5\n",
            "         142       0.00      0.00      0.00         5\n",
            "         143       0.00      0.00      0.00         5\n",
            "         144       0.00      0.00      0.00         5\n",
            "         145       0.00      0.00      0.00         5\n",
            "         146       0.00      0.00      0.00         5\n",
            "         147       0.00      0.00      0.00         5\n",
            "         148       0.00      0.00      0.00         5\n",
            "         149       0.00      0.00      0.00         5\n",
            "         150       0.00      0.00      0.00         5\n",
            "         151       0.00      0.00      0.00         5\n",
            "         152       0.00      0.00      0.00         5\n",
            "         153       0.00      0.00      0.00         5\n",
            "         154       0.00      0.00      0.00         5\n",
            "         155       0.00      0.00      0.00         5\n",
            "         156       0.00      0.00      0.00         5\n",
            "         157       0.00      0.00      0.00         5\n",
            "         158       0.00      0.00      0.00         5\n",
            "         159       0.00      0.00      0.00         5\n",
            "         160       0.00      0.00      0.00         5\n",
            "         161       0.00      0.00      0.00         5\n",
            "         162       0.00      0.00      0.00         5\n",
            "         163       0.00      0.00      0.00         5\n",
            "         164       0.00      0.00      0.00         5\n",
            "         165       0.00      0.00      0.00         5\n",
            "         166       0.00      0.00      0.00         5\n",
            "         167       0.00      0.00      0.00         5\n",
            "         168       0.00      0.00      0.00         5\n",
            "         169       0.00      0.00      0.00         5\n",
            "         170       0.00      0.00      0.00         5\n",
            "         171       0.00      0.00      0.00         5\n",
            "         172       0.00      0.00      0.00         5\n",
            "         173       0.00      0.00      0.00         5\n",
            "         174       0.00      0.00      0.00         5\n",
            "         175       0.00      0.00      0.00         5\n",
            "         176       0.00      0.00      0.00         5\n",
            "         177       0.00      0.00      0.00         5\n",
            "         178       0.00      0.00      0.00         5\n",
            "         179       0.00      0.00      0.00         5\n",
            "         180       0.00      0.00      0.00         5\n",
            "         181       0.00      0.00      0.00         5\n",
            "         182       0.00      0.00      0.00         5\n",
            "         183       0.00      0.00      0.00         5\n",
            "         184       0.00      0.00      0.00         5\n",
            "         185       0.00      0.00      0.00         5\n",
            "         186       0.00      0.00      0.00         5\n",
            "         187       0.00      0.00      0.00         5\n",
            "         188       0.50      0.20      0.29         5\n",
            "         189       0.00      0.00      0.00         5\n",
            "         190       0.00      0.00      0.00         5\n",
            "         191       0.00      0.00      0.00         5\n",
            "         192       0.00      0.00      0.00         5\n",
            "         193       0.00      0.00      0.00         5\n",
            "         194       0.00      0.00      0.00         5\n",
            "         195       0.00      0.00      0.00         5\n",
            "         196       0.00      0.00      0.00         5\n",
            "         197       0.00      0.00      0.00         5\n",
            "         198       0.00      0.00      0.00         5\n",
            "         199       0.00      0.00      0.00         5\n",
            "         200       0.00      0.00      0.00         5\n",
            "         201       0.20      0.20      0.20         5\n",
            "         202       0.00      0.00      0.00         5\n",
            "         203       0.00      0.00      0.00         5\n",
            "         204       0.00      0.00      0.00         5\n",
            "         205       0.00      0.00      0.00         5\n",
            "         206       0.00      0.00      0.00         5\n",
            "         207       0.00      0.00      0.00         5\n",
            "         208       0.00      0.00      0.00         5\n",
            "         209       0.00      0.00      0.00         5\n",
            "         210       0.00      0.00      0.00         5\n",
            "         211       0.00      0.00      0.00         5\n",
            "         212       0.00      0.00      0.00         5\n",
            "         213       0.00      0.00      0.00         5\n",
            "         214       0.00      0.00      0.00         5\n",
            "         215       0.00      0.00      0.00         5\n",
            "         216       0.00      0.00      0.00         5\n",
            "         217       0.00      0.00      0.00         5\n",
            "         218       0.00      0.00      0.00         5\n",
            "         219       0.00      0.00      0.00         5\n",
            "         220       0.00      0.00      0.00         5\n",
            "         221       0.00      0.00      0.00         5\n",
            "         222       0.00      0.00      0.00         5\n",
            "         223       0.00      0.00      0.00         5\n",
            "         224       0.00      0.00      0.00         5\n",
            "         225       0.00      0.00      0.00         5\n",
            "         226       0.00      0.00      0.00         5\n",
            "         227       0.00      0.00      0.00         5\n",
            "         228       0.00      0.00      0.00         5\n",
            "         229       0.00      0.00      0.00         5\n",
            "         230       0.00      0.00      0.00         5\n",
            "         231       0.00      0.00      0.00         5\n",
            "         232       0.17      0.20      0.18         5\n",
            "         233       0.00      0.00      0.00         5\n",
            "         234       0.00      0.00      0.00         5\n",
            "         235       0.00      0.00      0.00         5\n",
            "         236       0.00      0.00      0.00         5\n",
            "         237       0.00      0.00      0.00         5\n",
            "         238       0.00      0.00      0.00         5\n",
            "         239       0.00      0.00      0.00         5\n",
            "         240       0.00      0.00      0.00         5\n",
            "         241       0.00      0.00      0.00         5\n",
            "         242       0.00      0.00      0.00         5\n",
            "         243       0.00      0.00      0.00         5\n",
            "         244       0.00      0.00      0.00         5\n",
            "         245       0.00      0.00      0.00         5\n",
            "         246       0.00      0.00      0.00         5\n",
            "         247       0.00      0.00      0.00         5\n",
            "         248       0.00      0.00      0.00         5\n",
            "         249       0.00      0.00      0.00         5\n",
            "         250       0.00      0.00      0.00         5\n",
            "         251       0.00      0.00      0.00         5\n",
            "         252       0.00      0.00      0.00         5\n",
            "         253       0.00      0.00      0.00         5\n",
            "         254       0.00      0.00      0.00         5\n",
            "         255       0.00      0.00      0.00         5\n",
            "         256       0.00      0.00      0.00         5\n",
            "         257       0.00      0.00      0.00         5\n",
            "         258       0.00      0.00      0.00         5\n",
            "         259       0.00      0.00      0.00         5\n",
            "         260       0.00      0.00      0.00         5\n",
            "         261       0.00      0.00      0.00         5\n",
            "         262       0.00      0.00      0.00         5\n",
            "         263       0.00      0.00      0.00         5\n",
            "         264       0.00      0.00      0.00         5\n",
            "         265       0.00      0.00      0.00         5\n",
            "         266       0.00      0.00      0.00         5\n",
            "         267       0.00      0.00      0.00         5\n",
            "         268       0.00      0.00      0.00         5\n",
            "         269       0.00      0.00      0.00         5\n",
            "         270       0.00      0.00      0.00         5\n",
            "         271       0.00      0.00      0.00         5\n",
            "         272       0.00      0.00      0.00         5\n",
            "         273       0.00      0.00      0.00         5\n",
            "         274       0.00      0.00      0.00         5\n",
            "         275       0.00      0.00      0.00         5\n",
            "         276       0.00      0.00      0.00         5\n",
            "         277       0.00      0.00      0.00         5\n",
            "         278       0.00      0.00      0.00         5\n",
            "         279       0.00      0.00      0.00         5\n",
            "         280       0.00      0.00      0.00         5\n",
            "         281       0.00      0.00      0.00         5\n",
            "         282       0.00      0.00      0.00         5\n",
            "         283       0.00      0.00      0.00         5\n",
            "         284       0.00      0.00      0.00         5\n",
            "         285       0.00      0.00      0.00         5\n",
            "         286       0.00      0.00      0.00         5\n",
            "         287       0.00      0.00      0.00         5\n",
            "         288       0.00      0.00      0.00         5\n",
            "         289       0.00      0.00      0.00         5\n",
            "         290       0.00      0.00      0.00         5\n",
            "         291       0.00      0.00      0.00         5\n",
            "         292       0.00      0.00      0.00         5\n",
            "         293       0.00      0.00      0.00         5\n",
            "         294       0.00      0.00      0.00         5\n",
            "         295       0.00      0.00      0.00         5\n",
            "         296       0.00      0.00      0.00         5\n",
            "         297       0.00      0.00      0.00         5\n",
            "         298       0.00      0.00      0.00         5\n",
            "         299       0.00      0.00      0.00         5\n",
            "         300       0.00      0.00      0.00         5\n",
            "         301       0.00      0.00      0.00         5\n",
            "         302       0.00      0.00      0.00         5\n",
            "         303       0.00      0.00      0.00         5\n",
            "         304       0.00      0.00      0.00         5\n",
            "         305       0.00      0.00      0.00         5\n",
            "         306       0.00      0.00      0.00         5\n",
            "         307       0.00      0.00      0.00         5\n",
            "         308       0.00      0.00      0.00         5\n",
            "         309       0.00      0.00      0.00         5\n",
            "         310       0.00      0.00      0.00         5\n",
            "         311       0.00      0.00      0.00         5\n",
            "         312       0.00      0.00      0.00         5\n",
            "         313       0.00      0.00      0.00         5\n",
            "         314       0.00      0.00      0.00         5\n",
            "         315       0.00      0.00      0.00         5\n",
            "         316       0.00      0.00      0.00         5\n",
            "         317       0.00      0.00      0.00         5\n",
            "         318       0.00      0.00      0.00         5\n",
            "         319       0.00      0.00      0.00         5\n",
            "         320       0.00      0.00      0.00         5\n",
            "         321       0.00      0.00      0.00         5\n",
            "         322       0.00      0.00      0.00         5\n",
            "         323       0.00      0.00      0.00         5\n",
            "         324       0.00      0.00      0.00         5\n",
            "         325       0.00      0.00      0.00         5\n",
            "         326       0.00      0.00      0.00         5\n",
            "         327       0.00      0.00      0.00         5\n",
            "         328       0.00      0.00      0.00         5\n",
            "         329       0.00      0.00      0.00         5\n",
            "         330       0.00      0.00      0.00         5\n",
            "         331       0.00      0.00      0.00         5\n",
            "         332       0.00      0.00      0.00         5\n",
            "         333       0.00      0.00      0.00         5\n",
            "         334       0.00      0.00      0.00         5\n",
            "         335       0.00      0.00      0.00         5\n",
            "         336       0.00      0.00      0.00         5\n",
            "         337       0.00      0.00      0.00         5\n",
            "         338       0.00      0.00      0.00         5\n",
            "         339       0.00      0.00      0.00         5\n",
            "         340       0.00      0.00      0.00         5\n",
            "         341       0.00      0.00      0.00         5\n",
            "         342       0.00      0.00      0.00         5\n",
            "         343       0.00      0.00      0.00         5\n",
            "         344       0.00      0.00      0.00         5\n",
            "         345       0.00      0.00      0.00         5\n",
            "         346       0.00      0.00      0.00         5\n",
            "         347       0.00      0.00      0.00         5\n",
            "         348       0.00      0.00      0.00         5\n",
            "         349       0.00      0.00      0.00         5\n",
            "         350       0.00      0.00      0.00         5\n",
            "         351       0.00      0.00      0.00         5\n",
            "         352       0.00      0.00      0.00         5\n",
            "         353       0.00      0.00      0.00         5\n",
            "         354       0.00      0.00      0.00         5\n",
            "         355       0.00      0.00      0.00         5\n",
            "         356       0.00      0.00      0.00         5\n",
            "         357       0.00      0.00      0.00         5\n",
            "         358       0.00      0.00      0.00         5\n",
            "         359       0.00      0.00      0.00         5\n",
            "         360       0.00      0.00      0.00         5\n",
            "         361       0.00      0.00      0.00         5\n",
            "         362       0.00      0.00      0.00         5\n",
            "         363       0.00      0.00      0.00         5\n",
            "         364       0.00      0.00      0.00         5\n",
            "         365       0.00      0.00      0.00         5\n",
            "         366       0.00      0.00      0.00         5\n",
            "         367       0.00      0.00      0.00         5\n",
            "         368       0.00      0.00      0.00         5\n",
            "         369       0.00      0.00      0.00         5\n",
            "         370       0.00      0.00      0.00         5\n",
            "         371       0.00      0.00      0.00         5\n",
            "         372       0.00      0.00      0.00         5\n",
            "         373       0.00      0.00      0.00         5\n",
            "         374       0.00      0.00      0.00         5\n",
            "         375       0.00      0.00      0.00         5\n",
            "         376       0.00      0.00      0.00         5\n",
            "         377       0.00      0.00      0.00         5\n",
            "         378       0.00      0.00      0.00         5\n",
            "         379       0.00      0.00      0.00         5\n",
            "         380       0.00      0.00      0.00         5\n",
            "         381       0.00      0.00      0.00         5\n",
            "         382       0.00      0.00      0.00         5\n",
            "         383       0.00      0.00      0.00         5\n",
            "         384       0.00      0.00      0.00         5\n",
            "         385       0.00      0.00      0.00         5\n",
            "         386       0.00      0.00      0.00         5\n",
            "         387       0.00      0.00      0.00         5\n",
            "         388       0.00      0.00      0.00         5\n",
            "         389       0.00      0.00      0.00         5\n",
            "         390       0.00      0.00      0.00         5\n",
            "         391       0.00      0.00      0.00         5\n",
            "         392       0.00      0.00      0.00         5\n",
            "         393       0.00      0.00      0.00         5\n",
            "         394       0.00      0.00      0.00         5\n",
            "         395       0.00      0.00      0.00         5\n",
            "         396       0.00      0.00      0.00         5\n",
            "         397       0.00      0.00      0.00         5\n",
            "         398       0.00      0.00      0.00         5\n",
            "         399       0.00      0.00      0.00         5\n",
            "         400       0.00      0.00      0.00         5\n",
            "         401       0.00      0.00      0.00         5\n",
            "         402       0.00      0.00      0.00         5\n",
            "         403       0.00      0.00      0.00         5\n",
            "         404       0.00      0.00      0.00         5\n",
            "         405       0.00      0.00      0.00         5\n",
            "         406       0.00      0.00      0.00         5\n",
            "         407       0.00      0.00      0.00         5\n",
            "         408       0.00      0.00      0.00         5\n",
            "         409       0.00      0.00      0.00         5\n",
            "         410       0.00      0.00      0.00         5\n",
            "         411       0.00      0.00      0.00         5\n",
            "         412       0.00      0.00      0.00         5\n",
            "         413       0.00      0.00      0.00         5\n",
            "         414       0.00      0.00      0.00         5\n",
            "         415       0.00      0.00      0.00         5\n",
            "         416       0.00      0.00      0.00         5\n",
            "         417       0.00      0.00      0.00         5\n",
            "         418       0.00      0.00      0.00         5\n",
            "         419       0.00      0.00      0.00         5\n",
            "         420       0.00      0.00      0.00         5\n",
            "         421       0.00      0.00      0.00         5\n",
            "         422       0.00      0.00      0.00         5\n",
            "         423       0.00      0.00      0.00         5\n",
            "         424       0.00      0.00      0.00         5\n",
            "         425       0.00      0.00      0.00         5\n",
            "         426       0.00      0.00      0.00         5\n",
            "         427       0.00      0.00      0.00         5\n",
            "         428       0.00      0.00      0.00         5\n",
            "         429       0.00      0.00      0.00         5\n",
            "         430       0.00      0.00      0.00         5\n",
            "         431       0.00      0.00      0.00         5\n",
            "         432       0.00      0.00      0.00         5\n",
            "         433       0.00      0.00      0.00         5\n",
            "         434       0.00      0.00      0.00         5\n",
            "         435       0.00      0.00      0.00         5\n",
            "         436       0.00      0.00      0.00         5\n",
            "         437       0.00      0.00      0.00         5\n",
            "         438       0.00      0.00      0.00         5\n",
            "         439       0.00      0.00      0.00         5\n",
            "         440       0.00      0.00      0.00         5\n",
            "         441       0.00      0.00      0.00         5\n",
            "         442       0.00      0.00      0.00         5\n",
            "         443       0.00      0.00      0.00         5\n",
            "         444       0.00      0.00      0.00         5\n",
            "         445       0.00      0.00      0.00         5\n",
            "         446       0.00      0.00      0.00         5\n",
            "         447       0.00      0.00      0.00         5\n",
            "         448       0.00      0.00      0.00         5\n",
            "         449       0.00      0.00      0.00         5\n",
            "         450       0.00      0.00      0.00         5\n",
            "         451       0.00      0.00      0.00         5\n",
            "         452       0.00      0.00      0.00         5\n",
            "         453       0.00      0.00      0.00         5\n",
            "         454       0.00      0.00      0.00         5\n",
            "         455       0.00      0.00      0.00         5\n",
            "         456       0.00      0.00      0.00         5\n",
            "         457       0.00      0.00      0.00         5\n",
            "         458       0.00      0.00      0.00         5\n",
            "         459       0.00      0.00      0.00         5\n",
            "         460       0.00      0.00      0.00         5\n",
            "         461       0.00      0.00      0.00         5\n",
            "         462       0.00      0.00      0.00         5\n",
            "         463       0.00      0.00      0.00         5\n",
            "         464       0.00      0.00      0.00         5\n",
            "         465       0.00      0.00      0.00         5\n",
            "         466       0.00      0.00      0.00         5\n",
            "         467       0.00      0.00      0.00         5\n",
            "         468       0.00      0.00      0.00         5\n",
            "         469       0.00      0.00      0.00         5\n",
            "         470       0.00      0.00      0.00         5\n",
            "         471       0.00      0.00      0.00         5\n",
            "         472       0.00      0.00      0.00         5\n",
            "         473       0.00      0.00      0.00         5\n",
            "         474       0.00      0.00      0.00         5\n",
            "         475       0.00      0.00      0.00         5\n",
            "         476       0.00      0.00      0.00         5\n",
            "         477       0.00      0.00      0.00         5\n",
            "         478       0.00      0.00      0.00         5\n",
            "         479       0.00      0.00      0.00         5\n",
            "         480       0.00      0.00      0.00         5\n",
            "         481       0.00      0.00      0.00         5\n",
            "         482       0.00      0.00      0.00         5\n",
            "         483       0.00      0.00      0.00         5\n",
            "         484       0.00      0.00      0.00         5\n",
            "         485       0.00      0.00      0.00         5\n",
            "         486       0.00      0.00      0.00         5\n",
            "         487       0.00      0.00      0.00         5\n",
            "         488       0.00      0.00      0.00         5\n",
            "         489       0.00      0.00      0.00         5\n",
            "         490       0.00      0.00      0.00         5\n",
            "         491       0.00      0.00      0.00         5\n",
            "         492       0.00      0.00      0.00         5\n",
            "         493       0.00      0.00      0.00         5\n",
            "         494       0.00      0.00      0.00         5\n",
            "         495       0.00      0.00      0.00         5\n",
            "         496       0.00      0.00      0.00         5\n",
            "         497       0.00      0.00      0.00         5\n",
            "         498       0.00      0.00      0.00         5\n",
            "         499       0.00      0.00      0.00         5\n",
            "         500       0.00      0.00      0.00         5\n",
            "         501       0.00      0.00      0.00         5\n",
            "         502       0.00      0.00      0.00         5\n",
            "         503       0.00      0.00      0.00         5\n",
            "         504       0.00      0.00      0.00         5\n",
            "         505       0.00      0.00      0.00         5\n",
            "         506       0.00      0.00      0.00         5\n",
            "         507       0.00      0.00      0.00         5\n",
            "         508       0.00      0.00      0.00         5\n",
            "         509       0.00      0.00      0.00         5\n",
            "         510       0.00      0.00      0.00         5\n",
            "         511       0.00      0.00      0.00         5\n",
            "         512       0.00      0.00      0.00         5\n",
            "         513       0.00      0.00      0.00         5\n",
            "         514       0.00      0.00      0.00         5\n",
            "         515       0.00      0.00      0.00         5\n",
            "         516       0.00      0.00      0.00         5\n",
            "         517       0.00      0.00      0.00         5\n",
            "         518       0.00      0.00      0.00         5\n",
            "         519       0.00      0.00      0.00         5\n",
            "         520       0.00      0.00      0.00         5\n",
            "         521       0.00      0.00      0.00         5\n",
            "         522       0.00      0.00      0.00         5\n",
            "         523       0.00      0.00      0.00         5\n",
            "         524       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.00      2625\n",
            "   macro avg       0.00      0.00      0.00      2625\n",
            "weighted avg       0.00      0.00      0.00      2625\n",
            "\n",
            "Confusion Matrix:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "83/83 [==============================] - 10s 107ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet50 Metrics:\n",
            "Accuracy: 0.0019047619047619048\n",
            "F1 Score: 7.2424407025167485e-06\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         5\n",
            "           1       0.00      0.00      0.00         5\n",
            "           2       0.00      0.00      0.00         5\n",
            "           3       0.00      0.00      0.00         5\n",
            "           4       0.00      0.00      0.00         5\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.00      0.00      0.00         5\n",
            "           7       0.00      0.00      0.00         5\n",
            "           8       0.00      0.00      0.00         5\n",
            "           9       0.00      0.00      0.00         5\n",
            "          10       0.00      0.00      0.00         5\n",
            "          11       0.00      0.00      0.00         5\n",
            "          12       0.00      0.00      0.00         5\n",
            "          13       0.00      0.00      0.00         5\n",
            "          14       0.00      0.00      0.00         5\n",
            "          15       0.00      0.00      0.00         5\n",
            "          16       0.00      0.00      0.00         5\n",
            "          17       0.00      0.00      0.00         5\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00         5\n",
            "          20       0.00      0.00      0.00         5\n",
            "          21       0.00      0.00      0.00         5\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.00      0.00      0.00         5\n",
            "          24       0.00      0.00      0.00         5\n",
            "          25       0.00      0.00      0.00         5\n",
            "          26       0.00      0.00      0.00         5\n",
            "          27       0.00      0.00      0.00         5\n",
            "          28       0.00      0.00      0.00         5\n",
            "          29       0.00      0.00      0.00         5\n",
            "          30       0.00      0.00      0.00         5\n",
            "          31       0.00      0.00      0.00         5\n",
            "          32       0.00      0.00      0.00         5\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.00      0.00      0.00         5\n",
            "          35       0.00      0.00      0.00         5\n",
            "          36       0.00      0.00      0.00         5\n",
            "          37       0.00      0.00      0.00         5\n",
            "          38       0.00      0.00      0.00         5\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00         5\n",
            "          41       0.00      0.00      0.00         5\n",
            "          42       0.00      0.00      0.00         5\n",
            "          43       0.00      0.00      0.00         5\n",
            "          44       0.00      0.00      0.00         5\n",
            "          45       0.00      0.00      0.00         5\n",
            "          46       0.00      0.00      0.00         5\n",
            "          47       0.00      0.00      0.00         5\n",
            "          48       0.00      0.00      0.00         5\n",
            "          49       0.00      0.00      0.00         5\n",
            "          50       0.00      0.00      0.00         5\n",
            "          51       0.00      0.00      0.00         5\n",
            "          52       0.00      0.00      0.00         5\n",
            "          53       0.00      0.00      0.00         5\n",
            "          54       0.00      0.00      0.00         5\n",
            "          55       0.00      0.00      0.00         5\n",
            "          56       0.00      0.00      0.00         5\n",
            "          57       0.00      0.00      0.00         5\n",
            "          58       0.00      0.00      0.00         5\n",
            "          59       0.00      0.00      0.00         5\n",
            "          60       0.00      0.00      0.00         5\n",
            "          61       0.00      0.00      0.00         5\n",
            "          62       0.00      0.00      0.00         5\n",
            "          63       0.00      0.00      0.00         5\n",
            "          64       0.00      0.00      0.00         5\n",
            "          65       0.00      0.00      0.00         5\n",
            "          66       0.00      0.00      0.00         5\n",
            "          67       0.00      0.00      0.00         5\n",
            "          68       0.00      0.00      0.00         5\n",
            "          69       0.00      0.00      0.00         5\n",
            "          70       0.00      0.00      0.00         5\n",
            "          71       0.00      0.00      0.00         5\n",
            "          72       0.00      0.00      0.00         5\n",
            "          73       0.00      0.00      0.00         5\n",
            "          74       0.00      0.00      0.00         5\n",
            "          75       0.00      0.00      0.00         5\n",
            "          76       0.00      0.00      0.00         5\n",
            "          77       0.00      0.00      0.00         5\n",
            "          78       0.00      0.00      0.00         5\n",
            "          79       0.00      0.00      0.00         5\n",
            "          80       0.00      0.00      0.00         5\n",
            "          81       0.00      0.00      0.00         5\n",
            "          82       0.00      0.00      0.00         5\n",
            "          83       0.00      0.00      0.00         5\n",
            "          84       0.00      0.00      0.00         5\n",
            "          85       0.00      0.00      0.00         5\n",
            "          86       0.00      0.00      0.00         5\n",
            "          87       0.00      0.00      0.00         5\n",
            "          88       0.00      0.00      0.00         5\n",
            "          89       0.00      0.00      0.00         5\n",
            "          90       0.00      0.00      0.00         5\n",
            "          91       0.00      0.00      0.00         5\n",
            "          92       0.00      0.00      0.00         5\n",
            "          93       0.00      0.00      0.00         5\n",
            "          94       0.00      0.00      0.00         5\n",
            "          95       0.00      0.00      0.00         5\n",
            "          96       0.00      0.00      0.00         5\n",
            "          97       0.00      0.00      0.00         5\n",
            "          98       0.00      0.00      0.00         5\n",
            "          99       0.00      0.00      0.00         5\n",
            "         100       0.00      0.00      0.00         5\n",
            "         101       0.00      0.00      0.00         5\n",
            "         102       0.00      0.00      0.00         5\n",
            "         103       0.00      0.00      0.00         5\n",
            "         104       0.00      0.00      0.00         5\n",
            "         105       0.00      0.00      0.00         5\n",
            "         106       0.00      0.00      0.00         5\n",
            "         107       0.00      0.00      0.00         5\n",
            "         108       0.00      0.00      0.00         5\n",
            "         109       0.00      0.00      0.00         5\n",
            "         110       0.00      0.00      0.00         5\n",
            "         111       0.00      0.00      0.00         5\n",
            "         112       0.00      0.00      0.00         5\n",
            "         113       0.00      0.00      0.00         5\n",
            "         114       0.00      0.00      0.00         5\n",
            "         115       0.00      0.00      0.00         5\n",
            "         116       0.00      0.00      0.00         5\n",
            "         117       0.00      0.00      0.00         5\n",
            "         118       0.00      0.00      0.00         5\n",
            "         119       0.00      0.00      0.00         5\n",
            "         120       0.00      0.00      0.00         5\n",
            "         121       0.00      0.00      0.00         5\n",
            "         122       0.00      0.00      0.00         5\n",
            "         123       0.00      0.00      0.00         5\n",
            "         124       0.00      0.00      0.00         5\n",
            "         125       0.00      0.00      0.00         5\n",
            "         126       0.00      0.00      0.00         5\n",
            "         127       0.00      0.00      0.00         5\n",
            "         128       0.00      0.00      0.00         5\n",
            "         129       0.00      0.00      0.00         5\n",
            "         130       0.00      0.00      0.00         5\n",
            "         131       0.00      0.00      0.00         5\n",
            "         132       0.00      0.00      0.00         5\n",
            "         133       0.00      0.00      0.00         5\n",
            "         134       0.00      0.00      0.00         5\n",
            "         135       0.00      0.00      0.00         5\n",
            "         136       0.00      0.00      0.00         5\n",
            "         137       0.00      0.00      0.00         5\n",
            "         138       0.00      0.00      0.00         5\n",
            "         139       0.00      0.00      0.00         5\n",
            "         140       0.00      0.00      0.00         5\n",
            "         141       0.00      0.00      0.00         5\n",
            "         142       0.00      0.00      0.00         5\n",
            "         143       0.00      0.00      0.00         5\n",
            "         144       0.00      0.00      0.00         5\n",
            "         145       0.00      0.00      0.00         5\n",
            "         146       0.00      0.00      0.00         5\n",
            "         147       0.00      0.00      0.00         5\n",
            "         148       0.00      0.00      0.00         5\n",
            "         149       0.00      0.00      0.00         5\n",
            "         150       0.00      0.00      0.00         5\n",
            "         151       0.00      0.00      0.00         5\n",
            "         152       0.00      0.00      0.00         5\n",
            "         153       0.00      0.00      0.00         5\n",
            "         154       0.00      0.00      0.00         5\n",
            "         155       0.00      0.00      0.00         5\n",
            "         156       0.00      0.00      0.00         5\n",
            "         157       0.00      0.00      0.00         5\n",
            "         158       0.00      0.00      0.00         5\n",
            "         159       0.00      0.00      0.00         5\n",
            "         160       0.00      0.00      0.00         5\n",
            "         161       0.00      0.00      0.00         5\n",
            "         162       0.00      0.00      0.00         5\n",
            "         163       0.00      0.00      0.00         5\n",
            "         164       0.00      0.00      0.00         5\n",
            "         165       0.00      0.00      0.00         5\n",
            "         166       0.00      0.00      0.00         5\n",
            "         167       0.00      0.00      0.00         5\n",
            "         168       0.00      0.00      0.00         5\n",
            "         169       0.00      0.00      0.00         5\n",
            "         170       0.00      0.00      0.00         5\n",
            "         171       0.00      0.00      0.00         5\n",
            "         172       0.00      0.00      0.00         5\n",
            "         173       0.00      0.00      0.00         5\n",
            "         174       0.00      0.00      0.00         5\n",
            "         175       0.00      0.00      0.00         5\n",
            "         176       0.00      0.00      0.00         5\n",
            "         177       0.00      0.00      0.00         5\n",
            "         178       0.00      0.00      0.00         5\n",
            "         179       0.00      0.00      0.00         5\n",
            "         180       0.00      0.00      0.00         5\n",
            "         181       0.00      0.00      0.00         5\n",
            "         182       0.00      0.00      0.00         5\n",
            "         183       0.00      0.00      0.00         5\n",
            "         184       0.00      0.00      0.00         5\n",
            "         185       0.00      0.00      0.00         5\n",
            "         186       0.00      0.00      0.00         5\n",
            "         187       0.00      0.00      0.00         5\n",
            "         188       0.00      0.00      0.00         5\n",
            "         189       0.00      0.00      0.00         5\n",
            "         190       0.00      0.00      0.00         5\n",
            "         191       0.00      0.00      0.00         5\n",
            "         192       0.00      0.00      0.00         5\n",
            "         193       0.00      0.00      0.00         5\n",
            "         194       0.00      0.00      0.00         5\n",
            "         195       0.00      0.00      0.00         5\n",
            "         196       0.00      0.00      0.00         5\n",
            "         197       0.00      0.00      0.00         5\n",
            "         198       0.00      0.00      0.00         5\n",
            "         199       0.00      0.00      0.00         5\n",
            "         200       0.00      0.00      0.00         5\n",
            "         201       0.00      0.00      0.00         5\n",
            "         202       0.00      0.00      0.00         5\n",
            "         203       0.00      0.00      0.00         5\n",
            "         204       0.00      0.00      0.00         5\n",
            "         205       0.00      0.00      0.00         5\n",
            "         206       0.00      0.00      0.00         5\n",
            "         207       0.00      0.00      0.00         5\n",
            "         208       0.00      0.00      0.00         5\n",
            "         209       0.00      0.00      0.00         5\n",
            "         210       0.00      0.00      0.00         5\n",
            "         211       0.00      0.00      0.00         5\n",
            "         212       0.00      0.00      0.00         5\n",
            "         213       0.00      0.00      0.00         5\n",
            "         214       0.00      0.00      0.00         5\n",
            "         215       0.00      0.00      0.00         5\n",
            "         216       0.00      0.00      0.00         5\n",
            "         217       0.00      0.00      0.00         5\n",
            "         218       0.00      0.00      0.00         5\n",
            "         219       0.00      0.00      0.00         5\n",
            "         220       0.00      0.00      0.00         5\n",
            "         221       0.00      0.00      0.00         5\n",
            "         222       0.00      0.00      0.00         5\n",
            "         223       0.00      0.00      0.00         5\n",
            "         224       0.00      0.00      0.00         5\n",
            "         225       0.00      0.00      0.00         5\n",
            "         226       0.00      0.00      0.00         5\n",
            "         227       0.00      0.00      0.00         5\n",
            "         228       0.00      0.00      0.00         5\n",
            "         229       0.00      0.00      0.00         5\n",
            "         230       0.00      0.00      0.00         5\n",
            "         231       0.00      0.00      0.00         5\n",
            "         232       0.00      0.00      0.00         5\n",
            "         233       0.00      0.00      0.00         5\n",
            "         234       0.00      0.00      0.00         5\n",
            "         235       0.00      0.00      0.00         5\n",
            "         236       0.00      0.00      0.00         5\n",
            "         237       0.00      0.00      0.00         5\n",
            "         238       0.00      0.00      0.00         5\n",
            "         239       0.00      0.00      0.00         5\n",
            "         240       0.00      0.00      0.00         5\n",
            "         241       0.00      0.00      0.00         5\n",
            "         242       0.00      0.00      0.00         5\n",
            "         243       0.00      0.00      0.00         5\n",
            "         244       0.00      0.00      0.00         5\n",
            "         245       0.00      0.00      0.00         5\n",
            "         246       0.00      0.00      0.00         5\n",
            "         247       0.00      0.00      0.00         5\n",
            "         248       0.00      0.00      0.00         5\n",
            "         249       0.00      0.00      0.00         5\n",
            "         250       0.00      0.00      0.00         5\n",
            "         251       0.00      0.00      0.00         5\n",
            "         252       0.00      0.00      0.00         5\n",
            "         253       0.00      0.00      0.00         5\n",
            "         254       0.00      0.00      0.00         5\n",
            "         255       0.00      0.00      0.00         5\n",
            "         256       0.00      0.00      0.00         5\n",
            "         257       0.00      0.00      0.00         5\n",
            "         258       0.00      0.00      0.00         5\n",
            "         259       0.00      0.00      0.00         5\n",
            "         260       0.00      0.00      0.00         5\n",
            "         261       0.00      0.00      0.00         5\n",
            "         262       0.00      0.00      0.00         5\n",
            "         263       0.00      0.00      0.00         5\n",
            "         264       0.00      0.00      0.00         5\n",
            "         265       0.00      0.00      0.00         5\n",
            "         266       0.00      0.00      0.00         5\n",
            "         267       0.00      0.00      0.00         5\n",
            "         268       0.00      0.00      0.00         5\n",
            "         269       0.00      0.00      0.00         5\n",
            "         270       0.00      0.00      0.00         5\n",
            "         271       0.00      0.00      0.00         5\n",
            "         272       0.00      0.00      0.00         5\n",
            "         273       0.00      0.00      0.00         5\n",
            "         274       0.00      0.00      0.00         5\n",
            "         275       0.00      0.00      0.00         5\n",
            "         276       0.00      0.00      0.00         5\n",
            "         277       0.00      0.00      0.00         5\n",
            "         278       0.00      0.00      0.00         5\n",
            "         279       0.00      0.00      0.00         5\n",
            "         280       0.00      0.00      0.00         5\n",
            "         281       0.00      0.00      0.00         5\n",
            "         282       0.00      0.00      0.00         5\n",
            "         283       0.00      0.00      0.00         5\n",
            "         284       0.00      0.00      0.00         5\n",
            "         285       0.00      0.00      0.00         5\n",
            "         286       0.00      0.00      0.00         5\n",
            "         287       0.00      0.00      0.00         5\n",
            "         288       0.00      0.00      0.00         5\n",
            "         289       0.00      0.00      0.00         5\n",
            "         290       0.00      0.00      0.00         5\n",
            "         291       0.00      0.00      0.00         5\n",
            "         292       0.00      0.00      0.00         5\n",
            "         293       0.00      0.00      0.00         5\n",
            "         294       0.00      0.00      0.00         5\n",
            "         295       0.00      0.00      0.00         5\n",
            "         296       0.00      0.00      0.00         5\n",
            "         297       0.00      0.00      0.00         5\n",
            "         298       0.00      0.00      0.00         5\n",
            "         299       0.00      0.00      0.00         5\n",
            "         300       0.00      0.00      0.00         5\n",
            "         301       0.00      0.00      0.00         5\n",
            "         302       0.00      0.00      0.00         5\n",
            "         303       0.00      0.00      0.00         5\n",
            "         304       0.00      0.00      0.00         5\n",
            "         305       0.00      0.00      0.00         5\n",
            "         306       0.00      0.00      0.00         5\n",
            "         307       0.00      0.00      0.00         5\n",
            "         308       0.00      0.00      0.00         5\n",
            "         309       0.00      0.00      0.00         5\n",
            "         310       0.00      0.00      0.00         5\n",
            "         311       0.00      0.00      0.00         5\n",
            "         312       0.00      0.00      0.00         5\n",
            "         313       0.00      0.00      0.00         5\n",
            "         314       0.00      0.00      0.00         5\n",
            "         315       0.00      0.00      0.00         5\n",
            "         316       0.00      0.00      0.00         5\n",
            "         317       0.00      0.00      0.00         5\n",
            "         318       0.00      0.00      0.00         5\n",
            "         319       0.00      0.00      0.00         5\n",
            "         320       0.00      0.00      0.00         5\n",
            "         321       0.00      0.00      0.00         5\n",
            "         322       0.00      0.00      0.00         5\n",
            "         323       0.00      0.00      0.00         5\n",
            "         324       0.00      0.00      0.00         5\n",
            "         325       0.00      0.00      0.00         5\n",
            "         326       0.00      0.00      0.00         5\n",
            "         327       0.00      0.00      0.00         5\n",
            "         328       0.00      0.00      0.00         5\n",
            "         329       0.00      0.00      0.00         5\n",
            "         330       0.00      0.00      0.00         5\n",
            "         331       0.00      0.00      0.00         5\n",
            "         332       0.00      0.00      0.00         5\n",
            "         333       0.00      0.00      0.00         5\n",
            "         334       0.00      0.00      0.00         5\n",
            "         335       0.00      0.00      0.00         5\n",
            "         336       0.00      0.00      0.00         5\n",
            "         337       0.00      0.00      0.00         5\n",
            "         338       0.00      0.00      0.00         5\n",
            "         339       0.00      0.00      0.00         5\n",
            "         340       0.00      0.00      0.00         5\n",
            "         341       0.00      0.00      0.00         5\n",
            "         342       0.00      0.00      0.00         5\n",
            "         343       0.00      0.00      0.00         5\n",
            "         344       0.00      0.00      0.00         5\n",
            "         345       0.00      0.00      0.00         5\n",
            "         346       0.00      0.00      0.00         5\n",
            "         347       0.00      0.00      0.00         5\n",
            "         348       0.00      0.00      0.00         5\n",
            "         349       0.00      0.00      0.00         5\n",
            "         350       0.00      0.00      0.00         5\n",
            "         351       0.00      0.00      0.00         5\n",
            "         352       0.00      0.00      0.00         5\n",
            "         353       0.00      0.00      0.00         5\n",
            "         354       0.00      0.00      0.00         5\n",
            "         355       0.00      0.00      0.00         5\n",
            "         356       0.00      0.00      0.00         5\n",
            "         357       0.00      0.00      0.00         5\n",
            "         358       0.00      0.00      0.00         5\n",
            "         359       0.00      0.00      0.00         5\n",
            "         360       0.00      0.00      0.00         5\n",
            "         361       0.00      0.00      0.00         5\n",
            "         362       0.00      0.00      0.00         5\n",
            "         363       0.00      0.00      0.00         5\n",
            "         364       0.00      0.00      0.00         5\n",
            "         365       0.00      0.00      0.00         5\n",
            "         366       0.00      0.00      0.00         5\n",
            "         367       0.00      0.00      0.00         5\n",
            "         368       0.00      0.00      0.00         5\n",
            "         369       0.00      0.00      0.00         5\n",
            "         370       0.00      0.00      0.00         5\n",
            "         371       0.00      0.00      0.00         5\n",
            "         372       0.00      0.00      0.00         5\n",
            "         373       0.00      0.00      0.00         5\n",
            "         374       0.00      0.00      0.00         5\n",
            "         375       0.00      0.00      0.00         5\n",
            "         376       0.00      0.00      0.00         5\n",
            "         377       0.00      0.00      0.00         5\n",
            "         378       0.00      0.00      0.00         5\n",
            "         379       0.00      0.00      0.00         5\n",
            "         380       0.00      0.00      0.00         5\n",
            "         381       0.00      0.00      0.00         5\n",
            "         382       0.00      0.00      0.00         5\n",
            "         383       0.00      0.00      0.00         5\n",
            "         384       0.00      0.00      0.00         5\n",
            "         385       0.00      0.00      0.00         5\n",
            "         386       0.00      0.00      0.00         5\n",
            "         387       0.00      0.00      0.00         5\n",
            "         388       0.00      0.00      0.00         5\n",
            "         389       0.00      0.00      0.00         5\n",
            "         390       0.00      0.00      0.00         5\n",
            "         391       0.00      0.00      0.00         5\n",
            "         392       0.00      0.00      0.00         5\n",
            "         393       0.00      0.00      0.00         5\n",
            "         394       0.00      0.00      0.00         5\n",
            "         395       0.00      0.00      0.00         5\n",
            "         396       0.00      0.00      0.00         5\n",
            "         397       0.00      0.00      0.00         5\n",
            "         398       0.00      0.00      0.00         5\n",
            "         399       0.00      0.00      0.00         5\n",
            "         400       0.00      0.00      0.00         5\n",
            "         401       0.00      0.00      0.00         5\n",
            "         402       0.00      0.00      0.00         5\n",
            "         403       0.00      0.00      0.00         5\n",
            "         404       0.00      0.00      0.00         5\n",
            "         405       0.00      0.00      0.00         5\n",
            "         406       0.00      0.00      0.00         5\n",
            "         407       0.00      0.00      0.00         5\n",
            "         408       0.00      0.00      0.00         5\n",
            "         409       0.00      0.00      0.00         5\n",
            "         410       0.00      0.00      0.00         5\n",
            "         411       0.00      0.00      0.00         5\n",
            "         412       0.00      0.00      0.00         5\n",
            "         413       0.00      0.00      0.00         5\n",
            "         414       0.00      0.00      0.00         5\n",
            "         415       0.00      0.00      0.00         5\n",
            "         416       0.00      0.00      0.00         5\n",
            "         417       0.00      0.00      0.00         5\n",
            "         418       0.00      0.00      0.00         5\n",
            "         419       0.00      0.00      0.00         5\n",
            "         420       0.00      0.00      0.00         5\n",
            "         421       0.00      0.00      0.00         5\n",
            "         422       0.00      0.00      0.00         5\n",
            "         423       0.00      0.00      0.00         5\n",
            "         424       0.00      0.00      0.00         5\n",
            "         425       0.00      0.00      0.00         5\n",
            "         426       0.00      0.00      0.00         5\n",
            "         427       0.00      0.00      0.00         5\n",
            "         428       0.00      0.00      0.00         5\n",
            "         429       0.00      0.00      0.00         5\n",
            "         430       0.00      0.00      0.00         5\n",
            "         431       0.00      0.00      0.00         5\n",
            "         432       0.00      0.00      0.00         5\n",
            "         433       0.00      0.00      0.00         5\n",
            "         434       0.00      1.00      0.00         5\n",
            "         435       0.00      0.00      0.00         5\n",
            "         436       0.00      0.00      0.00         5\n",
            "         437       0.00      0.00      0.00         5\n",
            "         438       0.00      0.00      0.00         5\n",
            "         439       0.00      0.00      0.00         5\n",
            "         440       0.00      0.00      0.00         5\n",
            "         441       0.00      0.00      0.00         5\n",
            "         442       0.00      0.00      0.00         5\n",
            "         443       0.00      0.00      0.00         5\n",
            "         444       0.00      0.00      0.00         5\n",
            "         445       0.00      0.00      0.00         5\n",
            "         446       0.00      0.00      0.00         5\n",
            "         447       0.00      0.00      0.00         5\n",
            "         448       0.00      0.00      0.00         5\n",
            "         449       0.00      0.00      0.00         5\n",
            "         450       0.00      0.00      0.00         5\n",
            "         451       0.00      0.00      0.00         5\n",
            "         452       0.00      0.00      0.00         5\n",
            "         453       0.00      0.00      0.00         5\n",
            "         454       0.00      0.00      0.00         5\n",
            "         455       0.00      0.00      0.00         5\n",
            "         456       0.00      0.00      0.00         5\n",
            "         457       0.00      0.00      0.00         5\n",
            "         458       0.00      0.00      0.00         5\n",
            "         459       0.00      0.00      0.00         5\n",
            "         460       0.00      0.00      0.00         5\n",
            "         461       0.00      0.00      0.00         5\n",
            "         462       0.00      0.00      0.00         5\n",
            "         463       0.00      0.00      0.00         5\n",
            "         464       0.00      0.00      0.00         5\n",
            "         465       0.00      0.00      0.00         5\n",
            "         466       0.00      0.00      0.00         5\n",
            "         467       0.00      0.00      0.00         5\n",
            "         468       0.00      0.00      0.00         5\n",
            "         469       0.00      0.00      0.00         5\n",
            "         470       0.00      0.00      0.00         5\n",
            "         471       0.00      0.00      0.00         5\n",
            "         472       0.00      0.00      0.00         5\n",
            "         473       0.00      0.00      0.00         5\n",
            "         474       0.00      0.00      0.00         5\n",
            "         475       0.00      0.00      0.00         5\n",
            "         476       0.00      0.00      0.00         5\n",
            "         477       0.00      0.00      0.00         5\n",
            "         478       0.00      0.00      0.00         5\n",
            "         479       0.00      0.00      0.00         5\n",
            "         480       0.00      0.00      0.00         5\n",
            "         481       0.00      0.00      0.00         5\n",
            "         482       0.00      0.00      0.00         5\n",
            "         483       0.00      0.00      0.00         5\n",
            "         484       0.00      0.00      0.00         5\n",
            "         485       0.00      0.00      0.00         5\n",
            "         486       0.00      0.00      0.00         5\n",
            "         487       0.00      0.00      0.00         5\n",
            "         488       0.00      0.00      0.00         5\n",
            "         489       0.00      0.00      0.00         5\n",
            "         490       0.00      0.00      0.00         5\n",
            "         491       0.00      0.00      0.00         5\n",
            "         492       0.00      0.00      0.00         5\n",
            "         493       0.00      0.00      0.00         5\n",
            "         494       0.00      0.00      0.00         5\n",
            "         495       0.00      0.00      0.00         5\n",
            "         496       0.00      0.00      0.00         5\n",
            "         497       0.00      0.00      0.00         5\n",
            "         498       0.00      0.00      0.00         5\n",
            "         499       0.00      0.00      0.00         5\n",
            "         500       0.00      0.00      0.00         5\n",
            "         501       0.00      0.00      0.00         5\n",
            "         502       0.00      0.00      0.00         5\n",
            "         503       0.00      0.00      0.00         5\n",
            "         504       0.00      0.00      0.00         5\n",
            "         505       0.00      0.00      0.00         5\n",
            "         506       0.00      0.00      0.00         5\n",
            "         507       0.00      0.00      0.00         5\n",
            "         508       0.00      0.00      0.00         5\n",
            "         509       0.00      0.00      0.00         5\n",
            "         510       0.00      0.00      0.00         5\n",
            "         511       0.00      0.00      0.00         5\n",
            "         512       0.00      0.00      0.00         5\n",
            "         513       0.00      0.00      0.00         5\n",
            "         514       0.00      0.00      0.00         5\n",
            "         515       0.00      0.00      0.00         5\n",
            "         516       0.00      0.00      0.00         5\n",
            "         517       0.00      0.00      0.00         5\n",
            "         518       0.00      0.00      0.00         5\n",
            "         519       0.00      0.00      0.00         5\n",
            "         520       0.00      0.00      0.00         5\n",
            "         521       0.00      0.00      0.00         5\n",
            "         522       0.00      0.00      0.00         5\n",
            "         523       0.00      0.00      0.00         5\n",
            "         524       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.00      2625\n",
            "   macro avg       0.00      0.00      0.00      2625\n",
            "weighted avg       0.00      0.00      0.00      2625\n",
            "\n",
            "Confusion Matrix:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "83/83 [==============================] - 8s 78ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EfficientNetB0 Metrics:\n",
            "Accuracy: 0.0019047619047619048\n",
            "F1 Score: 7.2424407025167485e-06\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         5\n",
            "           1       0.00      0.00      0.00         5\n",
            "           2       0.00      0.00      0.00         5\n",
            "           3       0.00      0.00      0.00         5\n",
            "           4       0.00      0.00      0.00         5\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.00      0.00      0.00         5\n",
            "           7       0.00      0.00      0.00         5\n",
            "           8       0.00      0.00      0.00         5\n",
            "           9       0.00      0.00      0.00         5\n",
            "          10       0.00      0.00      0.00         5\n",
            "          11       0.00      0.00      0.00         5\n",
            "          12       0.00      0.00      0.00         5\n",
            "          13       0.00      0.00      0.00         5\n",
            "          14       0.00      0.00      0.00         5\n",
            "          15       0.00      0.00      0.00         5\n",
            "          16       0.00      0.00      0.00         5\n",
            "          17       0.00      0.00      0.00         5\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00         5\n",
            "          20       0.00      0.00      0.00         5\n",
            "          21       0.00      0.00      0.00         5\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.00      0.00      0.00         5\n",
            "          24       0.00      0.00      0.00         5\n",
            "          25       0.00      0.00      0.00         5\n",
            "          26       0.00      0.00      0.00         5\n",
            "          27       0.00      0.00      0.00         5\n",
            "          28       0.00      0.00      0.00         5\n",
            "          29       0.00      0.00      0.00         5\n",
            "          30       0.00      0.00      0.00         5\n",
            "          31       0.00      0.00      0.00         5\n",
            "          32       0.00      0.00      0.00         5\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.00      0.00      0.00         5\n",
            "          35       0.00      0.00      0.00         5\n",
            "          36       0.00      0.00      0.00         5\n",
            "          37       0.00      0.00      0.00         5\n",
            "          38       0.00      0.00      0.00         5\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00         5\n",
            "          41       0.00      0.00      0.00         5\n",
            "          42       0.00      0.00      0.00         5\n",
            "          43       0.00      0.00      0.00         5\n",
            "          44       0.00      0.00      0.00         5\n",
            "          45       0.00      0.00      0.00         5\n",
            "          46       0.00      0.00      0.00         5\n",
            "          47       0.00      0.00      0.00         5\n",
            "          48       0.00      0.00      0.00         5\n",
            "          49       0.00      0.00      0.00         5\n",
            "          50       0.00      0.00      0.00         5\n",
            "          51       0.00      0.00      0.00         5\n",
            "          52       0.00      0.00      0.00         5\n",
            "          53       0.00      0.00      0.00         5\n",
            "          54       0.00      0.00      0.00         5\n",
            "          55       0.00      0.00      0.00         5\n",
            "          56       0.00      0.00      0.00         5\n",
            "          57       0.00      0.00      0.00         5\n",
            "          58       0.00      0.00      0.00         5\n",
            "          59       0.00      0.00      0.00         5\n",
            "          60       0.00      0.00      0.00         5\n",
            "          61       0.00      0.00      0.00         5\n",
            "          62       0.00      0.00      0.00         5\n",
            "          63       0.00      0.00      0.00         5\n",
            "          64       0.00      0.00      0.00         5\n",
            "          65       0.00      0.00      0.00         5\n",
            "          66       0.00      0.00      0.00         5\n",
            "          67       0.00      0.00      0.00         5\n",
            "          68       0.00      0.00      0.00         5\n",
            "          69       0.00      0.00      0.00         5\n",
            "          70       0.00      0.00      0.00         5\n",
            "          71       0.00      0.00      0.00         5\n",
            "          72       0.00      0.00      0.00         5\n",
            "          73       0.00      0.00      0.00         5\n",
            "          74       0.00      0.00      0.00         5\n",
            "          75       0.00      0.00      0.00         5\n",
            "          76       0.00      0.00      0.00         5\n",
            "          77       0.00      0.00      0.00         5\n",
            "          78       0.00      0.00      0.00         5\n",
            "          79       0.00      0.00      0.00         5\n",
            "          80       0.00      0.00      0.00         5\n",
            "          81       0.00      0.00      0.00         5\n",
            "          82       0.00      0.00      0.00         5\n",
            "          83       0.00      0.00      0.00         5\n",
            "          84       0.00      0.00      0.00         5\n",
            "          85       0.00      0.00      0.00         5\n",
            "          86       0.00      0.00      0.00         5\n",
            "          87       0.00      0.00      0.00         5\n",
            "          88       0.00      0.00      0.00         5\n",
            "          89       0.00      0.00      0.00         5\n",
            "          90       0.00      0.00      0.00         5\n",
            "          91       0.00      0.00      0.00         5\n",
            "          92       0.00      0.00      0.00         5\n",
            "          93       0.00      0.00      0.00         5\n",
            "          94       0.00      0.00      0.00         5\n",
            "          95       0.00      0.00      0.00         5\n",
            "          96       0.00      0.00      0.00         5\n",
            "          97       0.00      0.00      0.00         5\n",
            "          98       0.00      0.00      0.00         5\n",
            "          99       0.00      0.00      0.00         5\n",
            "         100       0.00      0.00      0.00         5\n",
            "         101       0.00      0.00      0.00         5\n",
            "         102       0.00      0.00      0.00         5\n",
            "         103       0.00      0.00      0.00         5\n",
            "         104       0.00      0.00      0.00         5\n",
            "         105       0.00      0.00      0.00         5\n",
            "         106       0.00      0.00      0.00         5\n",
            "         107       0.00      0.00      0.00         5\n",
            "         108       0.00      0.00      0.00         5\n",
            "         109       0.00      0.00      0.00         5\n",
            "         110       0.00      0.00      0.00         5\n",
            "         111       0.00      0.00      0.00         5\n",
            "         112       0.00      0.00      0.00         5\n",
            "         113       0.00      0.00      0.00         5\n",
            "         114       0.00      0.00      0.00         5\n",
            "         115       0.00      0.00      0.00         5\n",
            "         116       0.00      0.00      0.00         5\n",
            "         117       0.00      0.00      0.00         5\n",
            "         118       0.00      0.00      0.00         5\n",
            "         119       0.00      0.00      0.00         5\n",
            "         120       0.00      0.00      0.00         5\n",
            "         121       0.00      0.00      0.00         5\n",
            "         122       0.00      0.00      0.00         5\n",
            "         123       0.00      0.00      0.00         5\n",
            "         124       0.00      0.00      0.00         5\n",
            "         125       0.00      0.00      0.00         5\n",
            "         126       0.00      0.00      0.00         5\n",
            "         127       0.00      0.00      0.00         5\n",
            "         128       0.00      0.00      0.00         5\n",
            "         129       0.00      0.00      0.00         5\n",
            "         130       0.00      0.00      0.00         5\n",
            "         131       0.00      0.00      0.00         5\n",
            "         132       0.00      0.00      0.00         5\n",
            "         133       0.00      0.00      0.00         5\n",
            "         134       0.00      0.00      0.00         5\n",
            "         135       0.00      0.00      0.00         5\n",
            "         136       0.00      0.00      0.00         5\n",
            "         137       0.00      0.00      0.00         5\n",
            "         138       0.00      0.00      0.00         5\n",
            "         139       0.00      0.00      0.00         5\n",
            "         140       0.00      0.00      0.00         5\n",
            "         141       0.00      0.00      0.00         5\n",
            "         142       0.00      0.00      0.00         5\n",
            "         143       0.00      0.00      0.00         5\n",
            "         144       0.00      0.00      0.00         5\n",
            "         145       0.00      0.00      0.00         5\n",
            "         146       0.00      0.00      0.00         5\n",
            "         147       0.00      0.00      0.00         5\n",
            "         148       0.00      0.00      0.00         5\n",
            "         149       0.00      0.00      0.00         5\n",
            "         150       0.00      0.00      0.00         5\n",
            "         151       0.00      0.00      0.00         5\n",
            "         152       0.00      0.00      0.00         5\n",
            "         153       0.00      0.00      0.00         5\n",
            "         154       0.00      0.00      0.00         5\n",
            "         155       0.00      0.00      0.00         5\n",
            "         156       0.00      0.00      0.00         5\n",
            "         157       0.00      0.00      0.00         5\n",
            "         158       0.00      0.00      0.00         5\n",
            "         159       0.00      0.00      0.00         5\n",
            "         160       0.00      0.00      0.00         5\n",
            "         161       0.00      0.00      0.00         5\n",
            "         162       0.00      0.00      0.00         5\n",
            "         163       0.00      0.00      0.00         5\n",
            "         164       0.00      0.00      0.00         5\n",
            "         165       0.00      0.00      0.00         5\n",
            "         166       0.00      0.00      0.00         5\n",
            "         167       0.00      0.00      0.00         5\n",
            "         168       0.00      0.00      0.00         5\n",
            "         169       0.00      0.00      0.00         5\n",
            "         170       0.00      0.00      0.00         5\n",
            "         171       0.00      0.00      0.00         5\n",
            "         172       0.00      0.00      0.00         5\n",
            "         173       0.00      0.00      0.00         5\n",
            "         174       0.00      0.00      0.00         5\n",
            "         175       0.00      0.00      0.00         5\n",
            "         176       0.00      0.00      0.00         5\n",
            "         177       0.00      0.00      0.00         5\n",
            "         178       0.00      0.00      0.00         5\n",
            "         179       0.00      0.00      0.00         5\n",
            "         180       0.00      0.00      0.00         5\n",
            "         181       0.00      0.00      0.00         5\n",
            "         182       0.00      0.00      0.00         5\n",
            "         183       0.00      0.00      0.00         5\n",
            "         184       0.00      0.00      0.00         5\n",
            "         185       0.00      0.00      0.00         5\n",
            "         186       0.00      0.00      0.00         5\n",
            "         187       0.00      0.00      0.00         5\n",
            "         188       0.00      0.00      0.00         5\n",
            "         189       0.00      0.00      0.00         5\n",
            "         190       0.00      0.00      0.00         5\n",
            "         191       0.00      0.00      0.00         5\n",
            "         192       0.00      0.00      0.00         5\n",
            "         193       0.00      0.00      0.00         5\n",
            "         194       0.00      0.00      0.00         5\n",
            "         195       0.00      0.00      0.00         5\n",
            "         196       0.00      0.00      0.00         5\n",
            "         197       0.00      0.00      0.00         5\n",
            "         198       0.00      0.00      0.00         5\n",
            "         199       0.00      0.00      0.00         5\n",
            "         200       0.00      0.00      0.00         5\n",
            "         201       0.00      0.00      0.00         5\n",
            "         202       0.00      0.00      0.00         5\n",
            "         203       0.00      0.00      0.00         5\n",
            "         204       0.00      0.00      0.00         5\n",
            "         205       0.00      0.00      0.00         5\n",
            "         206       0.00      0.00      0.00         5\n",
            "         207       0.00      0.00      0.00         5\n",
            "         208       0.00      0.00      0.00         5\n",
            "         209       0.00      0.00      0.00         5\n",
            "         210       0.00      0.00      0.00         5\n",
            "         211       0.00      0.00      0.00         5\n",
            "         212       0.00      0.00      0.00         5\n",
            "         213       0.00      0.00      0.00         5\n",
            "         214       0.00      0.00      0.00         5\n",
            "         215       0.00      0.00      0.00         5\n",
            "         216       0.00      0.00      0.00         5\n",
            "         217       0.00      0.00      0.00         5\n",
            "         218       0.00      0.00      0.00         5\n",
            "         219       0.00      0.00      0.00         5\n",
            "         220       0.00      0.00      0.00         5\n",
            "         221       0.00      0.00      0.00         5\n",
            "         222       0.00      0.00      0.00         5\n",
            "         223       0.00      0.00      0.00         5\n",
            "         224       0.00      0.00      0.00         5\n",
            "         225       0.00      0.00      0.00         5\n",
            "         226       0.00      0.00      0.00         5\n",
            "         227       0.00      0.00      0.00         5\n",
            "         228       0.00      0.00      0.00         5\n",
            "         229       0.00      0.00      0.00         5\n",
            "         230       0.00      0.00      0.00         5\n",
            "         231       0.00      0.00      0.00         5\n",
            "         232       0.00      0.00      0.00         5\n",
            "         233       0.00      0.00      0.00         5\n",
            "         234       0.00      0.00      0.00         5\n",
            "         235       0.00      0.00      0.00         5\n",
            "         236       0.00      0.00      0.00         5\n",
            "         237       0.00      0.00      0.00         5\n",
            "         238       0.00      0.00      0.00         5\n",
            "         239       0.00      0.00      0.00         5\n",
            "         240       0.00      0.00      0.00         5\n",
            "         241       0.00      0.00      0.00         5\n",
            "         242       0.00      0.00      0.00         5\n",
            "         243       0.00      0.00      0.00         5\n",
            "         244       0.00      0.00      0.00         5\n",
            "         245       0.00      0.00      0.00         5\n",
            "         246       0.00      0.00      0.00         5\n",
            "         247       0.00      0.00      0.00         5\n",
            "         248       0.00      0.00      0.00         5\n",
            "         249       0.00      0.00      0.00         5\n",
            "         250       0.00      0.00      0.00         5\n",
            "         251       0.00      0.00      0.00         5\n",
            "         252       0.00      0.00      0.00         5\n",
            "         253       0.00      0.00      0.00         5\n",
            "         254       0.00      0.00      0.00         5\n",
            "         255       0.00      0.00      0.00         5\n",
            "         256       0.00      0.00      0.00         5\n",
            "         257       0.00      0.00      0.00         5\n",
            "         258       0.00      0.00      0.00         5\n",
            "         259       0.00      0.00      0.00         5\n",
            "         260       0.00      0.00      0.00         5\n",
            "         261       0.00      0.00      0.00         5\n",
            "         262       0.00      0.00      0.00         5\n",
            "         263       0.00      0.00      0.00         5\n",
            "         264       0.00      0.00      0.00         5\n",
            "         265       0.00      0.00      0.00         5\n",
            "         266       0.00      0.00      0.00         5\n",
            "         267       0.00      0.00      0.00         5\n",
            "         268       0.00      0.00      0.00         5\n",
            "         269       0.00      0.00      0.00         5\n",
            "         270       0.00      0.00      0.00         5\n",
            "         271       0.00      0.00      0.00         5\n",
            "         272       0.00      0.00      0.00         5\n",
            "         273       0.00      0.00      0.00         5\n",
            "         274       0.00      0.00      0.00         5\n",
            "         275       0.00      0.00      0.00         5\n",
            "         276       0.00      0.00      0.00         5\n",
            "         277       0.00      0.00      0.00         5\n",
            "         278       0.00      0.00      0.00         5\n",
            "         279       0.00      0.00      0.00         5\n",
            "         280       0.00      0.00      0.00         5\n",
            "         281       0.00      0.00      0.00         5\n",
            "         282       0.00      0.00      0.00         5\n",
            "         283       0.00      0.00      0.00         5\n",
            "         284       0.00      0.00      0.00         5\n",
            "         285       0.00      0.00      0.00         5\n",
            "         286       0.00      0.00      0.00         5\n",
            "         287       0.00      0.00      0.00         5\n",
            "         288       0.00      0.00      0.00         5\n",
            "         289       0.00      0.00      0.00         5\n",
            "         290       0.00      0.00      0.00         5\n",
            "         291       0.00      0.00      0.00         5\n",
            "         292       0.00      0.00      0.00         5\n",
            "         293       0.00      0.00      0.00         5\n",
            "         294       0.00      0.00      0.00         5\n",
            "         295       0.00      0.00      0.00         5\n",
            "         296       0.00      0.00      0.00         5\n",
            "         297       0.00      0.00      0.00         5\n",
            "         298       0.00      0.00      0.00         5\n",
            "         299       0.00      0.00      0.00         5\n",
            "         300       0.00      0.00      0.00         5\n",
            "         301       0.00      0.00      0.00         5\n",
            "         302       0.00      0.00      0.00         5\n",
            "         303       0.00      0.00      0.00         5\n",
            "         304       0.00      0.00      0.00         5\n",
            "         305       0.00      0.00      0.00         5\n",
            "         306       0.00      0.00      0.00         5\n",
            "         307       0.00      0.00      0.00         5\n",
            "         308       0.00      0.00      0.00         5\n",
            "         309       0.00      0.00      0.00         5\n",
            "         310       0.00      0.00      0.00         5\n",
            "         311       0.00      0.00      0.00         5\n",
            "         312       0.00      0.00      0.00         5\n",
            "         313       0.00      0.00      0.00         5\n",
            "         314       0.00      0.00      0.00         5\n",
            "         315       0.00      0.00      0.00         5\n",
            "         316       0.00      0.00      0.00         5\n",
            "         317       0.00      0.00      0.00         5\n",
            "         318       0.00      0.00      0.00         5\n",
            "         319       0.00      0.00      0.00         5\n",
            "         320       0.00      0.00      0.00         5\n",
            "         321       0.00      0.00      0.00         5\n",
            "         322       0.00      0.00      0.00         5\n",
            "         323       0.00      0.00      0.00         5\n",
            "         324       0.00      0.00      0.00         5\n",
            "         325       0.00      0.00      0.00         5\n",
            "         326       0.00      0.00      0.00         5\n",
            "         327       0.00      0.00      0.00         5\n",
            "         328       0.00      0.00      0.00         5\n",
            "         329       0.00      0.00      0.00         5\n",
            "         330       0.00      0.00      0.00         5\n",
            "         331       0.00      0.00      0.00         5\n",
            "         332       0.00      0.00      0.00         5\n",
            "         333       0.00      0.00      0.00         5\n",
            "         334       0.00      0.00      0.00         5\n",
            "         335       0.00      0.00      0.00         5\n",
            "         336       0.00      0.00      0.00         5\n",
            "         337       0.00      0.00      0.00         5\n",
            "         338       0.00      0.00      0.00         5\n",
            "         339       0.00      0.00      0.00         5\n",
            "         340       0.00      0.00      0.00         5\n",
            "         341       0.00      0.00      0.00         5\n",
            "         342       0.00      0.00      0.00         5\n",
            "         343       0.00      0.00      0.00         5\n",
            "         344       0.00      0.00      0.00         5\n",
            "         345       0.00      0.00      0.00         5\n",
            "         346       0.00      0.00      0.00         5\n",
            "         347       0.00      0.00      0.00         5\n",
            "         348       0.00      0.00      0.00         5\n",
            "         349       0.00      0.00      0.00         5\n",
            "         350       0.00      0.00      0.00         5\n",
            "         351       0.00      0.00      0.00         5\n",
            "         352       0.00      0.00      0.00         5\n",
            "         353       0.00      0.00      0.00         5\n",
            "         354       0.00      0.00      0.00         5\n",
            "         355       0.00      0.00      0.00         5\n",
            "         356       0.00      0.00      0.00         5\n",
            "         357       0.00      0.00      0.00         5\n",
            "         358       0.00      0.00      0.00         5\n",
            "         359       0.00      0.00      0.00         5\n",
            "         360       0.00      0.00      0.00         5\n",
            "         361       0.00      0.00      0.00         5\n",
            "         362       0.00      0.00      0.00         5\n",
            "         363       0.00      0.00      0.00         5\n",
            "         364       0.00      0.00      0.00         5\n",
            "         365       0.00      0.00      0.00         5\n",
            "         366       0.00      0.00      0.00         5\n",
            "         367       0.00      0.00      0.00         5\n",
            "         368       0.00      0.00      0.00         5\n",
            "         369       0.00      0.00      0.00         5\n",
            "         370       0.00      0.00      0.00         5\n",
            "         371       0.00      0.00      0.00         5\n",
            "         372       0.00      0.00      0.00         5\n",
            "         373       0.00      0.00      0.00         5\n",
            "         374       0.00      0.00      0.00         5\n",
            "         375       0.00      0.00      0.00         5\n",
            "         376       0.00      0.00      0.00         5\n",
            "         377       0.00      0.00      0.00         5\n",
            "         378       0.00      0.00      0.00         5\n",
            "         379       0.00      0.00      0.00         5\n",
            "         380       0.00      0.00      0.00         5\n",
            "         381       0.00      0.00      0.00         5\n",
            "         382       0.00      0.00      0.00         5\n",
            "         383       0.00      0.00      0.00         5\n",
            "         384       0.00      0.00      0.00         5\n",
            "         385       0.00      0.00      0.00         5\n",
            "         386       0.00      0.00      0.00         5\n",
            "         387       0.00      0.00      0.00         5\n",
            "         388       0.00      0.00      0.00         5\n",
            "         389       0.00      0.00      0.00         5\n",
            "         390       0.00      0.00      0.00         5\n",
            "         391       0.00      0.00      0.00         5\n",
            "         392       0.00      0.00      0.00         5\n",
            "         393       0.00      0.00      0.00         5\n",
            "         394       0.00      0.00      0.00         5\n",
            "         395       0.00      0.00      0.00         5\n",
            "         396       0.00      0.00      0.00         5\n",
            "         397       0.00      0.00      0.00         5\n",
            "         398       0.00      0.00      0.00         5\n",
            "         399       0.00      0.00      0.00         5\n",
            "         400       0.00      0.00      0.00         5\n",
            "         401       0.00      0.00      0.00         5\n",
            "         402       0.00      0.00      0.00         5\n",
            "         403       0.00      0.00      0.00         5\n",
            "         404       0.00      0.00      0.00         5\n",
            "         405       0.00      0.00      0.00         5\n",
            "         406       0.00      0.00      0.00         5\n",
            "         407       0.00      0.00      0.00         5\n",
            "         408       0.00      0.00      0.00         5\n",
            "         409       0.00      0.00      0.00         5\n",
            "         410       0.00      0.00      0.00         5\n",
            "         411       0.00      0.00      0.00         5\n",
            "         412       0.00      0.00      0.00         5\n",
            "         413       0.00      0.00      0.00         5\n",
            "         414       0.00      0.00      0.00         5\n",
            "         415       0.00      0.00      0.00         5\n",
            "         416       0.00      0.00      0.00         5\n",
            "         417       0.00      0.00      0.00         5\n",
            "         418       0.00      0.00      0.00         5\n",
            "         419       0.00      0.00      0.00         5\n",
            "         420       0.00      0.00      0.00         5\n",
            "         421       0.00      0.00      0.00         5\n",
            "         422       0.00      0.00      0.00         5\n",
            "         423       0.00      0.00      0.00         5\n",
            "         424       0.00      0.00      0.00         5\n",
            "         425       0.00      0.00      0.00         5\n",
            "         426       0.00      0.00      0.00         5\n",
            "         427       0.00      0.00      0.00         5\n",
            "         428       0.00      0.00      0.00         5\n",
            "         429       0.00      0.00      0.00         5\n",
            "         430       0.00      0.00      0.00         5\n",
            "         431       0.00      0.00      0.00         5\n",
            "         432       0.00      0.00      0.00         5\n",
            "         433       0.00      0.00      0.00         5\n",
            "         434       0.00      1.00      0.00         5\n",
            "         435       0.00      0.00      0.00         5\n",
            "         436       0.00      0.00      0.00         5\n",
            "         437       0.00      0.00      0.00         5\n",
            "         438       0.00      0.00      0.00         5\n",
            "         439       0.00      0.00      0.00         5\n",
            "         440       0.00      0.00      0.00         5\n",
            "         441       0.00      0.00      0.00         5\n",
            "         442       0.00      0.00      0.00         5\n",
            "         443       0.00      0.00      0.00         5\n",
            "         444       0.00      0.00      0.00         5\n",
            "         445       0.00      0.00      0.00         5\n",
            "         446       0.00      0.00      0.00         5\n",
            "         447       0.00      0.00      0.00         5\n",
            "         448       0.00      0.00      0.00         5\n",
            "         449       0.00      0.00      0.00         5\n",
            "         450       0.00      0.00      0.00         5\n",
            "         451       0.00      0.00      0.00         5\n",
            "         452       0.00      0.00      0.00         5\n",
            "         453       0.00      0.00      0.00         5\n",
            "         454       0.00      0.00      0.00         5\n",
            "         455       0.00      0.00      0.00         5\n",
            "         456       0.00      0.00      0.00         5\n",
            "         457       0.00      0.00      0.00         5\n",
            "         458       0.00      0.00      0.00         5\n",
            "         459       0.00      0.00      0.00         5\n",
            "         460       0.00      0.00      0.00         5\n",
            "         461       0.00      0.00      0.00         5\n",
            "         462       0.00      0.00      0.00         5\n",
            "         463       0.00      0.00      0.00         5\n",
            "         464       0.00      0.00      0.00         5\n",
            "         465       0.00      0.00      0.00         5\n",
            "         466       0.00      0.00      0.00         5\n",
            "         467       0.00      0.00      0.00         5\n",
            "         468       0.00      0.00      0.00         5\n",
            "         469       0.00      0.00      0.00         5\n",
            "         470       0.00      0.00      0.00         5\n",
            "         471       0.00      0.00      0.00         5\n",
            "         472       0.00      0.00      0.00         5\n",
            "         473       0.00      0.00      0.00         5\n",
            "         474       0.00      0.00      0.00         5\n",
            "         475       0.00      0.00      0.00         5\n",
            "         476       0.00      0.00      0.00         5\n",
            "         477       0.00      0.00      0.00         5\n",
            "         478       0.00      0.00      0.00         5\n",
            "         479       0.00      0.00      0.00         5\n",
            "         480       0.00      0.00      0.00         5\n",
            "         481       0.00      0.00      0.00         5\n",
            "         482       0.00      0.00      0.00         5\n",
            "         483       0.00      0.00      0.00         5\n",
            "         484       0.00      0.00      0.00         5\n",
            "         485       0.00      0.00      0.00         5\n",
            "         486       0.00      0.00      0.00         5\n",
            "         487       0.00      0.00      0.00         5\n",
            "         488       0.00      0.00      0.00         5\n",
            "         489       0.00      0.00      0.00         5\n",
            "         490       0.00      0.00      0.00         5\n",
            "         491       0.00      0.00      0.00         5\n",
            "         492       0.00      0.00      0.00         5\n",
            "         493       0.00      0.00      0.00         5\n",
            "         494       0.00      0.00      0.00         5\n",
            "         495       0.00      0.00      0.00         5\n",
            "         496       0.00      0.00      0.00         5\n",
            "         497       0.00      0.00      0.00         5\n",
            "         498       0.00      0.00      0.00         5\n",
            "         499       0.00      0.00      0.00         5\n",
            "         500       0.00      0.00      0.00         5\n",
            "         501       0.00      0.00      0.00         5\n",
            "         502       0.00      0.00      0.00         5\n",
            "         503       0.00      0.00      0.00         5\n",
            "         504       0.00      0.00      0.00         5\n",
            "         505       0.00      0.00      0.00         5\n",
            "         506       0.00      0.00      0.00         5\n",
            "         507       0.00      0.00      0.00         5\n",
            "         508       0.00      0.00      0.00         5\n",
            "         509       0.00      0.00      0.00         5\n",
            "         510       0.00      0.00      0.00         5\n",
            "         511       0.00      0.00      0.00         5\n",
            "         512       0.00      0.00      0.00         5\n",
            "         513       0.00      0.00      0.00         5\n",
            "         514       0.00      0.00      0.00         5\n",
            "         515       0.00      0.00      0.00         5\n",
            "         516       0.00      0.00      0.00         5\n",
            "         517       0.00      0.00      0.00         5\n",
            "         518       0.00      0.00      0.00         5\n",
            "         519       0.00      0.00      0.00         5\n",
            "         520       0.00      0.00      0.00         5\n",
            "         521       0.00      0.00      0.00         5\n",
            "         522       0.00      0.00      0.00         5\n",
            "         523       0.00      0.00      0.00         5\n",
            "         524       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.00      2625\n",
            "   macro avg       0.00      0.00      0.00      2625\n",
            "weighted avg       0.00      0.00      0.00      2625\n",
            "\n",
            "Confusion Matrix:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "83/83 [==============================] - 7s 74ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MobileNetV2 Metrics:\n",
            "Accuracy: 0.0034285714285714284\n",
            "F1 Score: 0.0032295482295482303\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         5\n",
            "           1       0.00      0.00      0.00         5\n",
            "           2       0.00      0.00      0.00         5\n",
            "           3       0.00      0.00      0.00         5\n",
            "           4       0.00      0.00      0.00         5\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.00      0.00      0.00         5\n",
            "           7       0.00      0.00      0.00         5\n",
            "           8       0.00      0.00      0.00         5\n",
            "           9       0.00      0.00      0.00         5\n",
            "          10       0.00      0.00      0.00         5\n",
            "          11       0.00      0.00      0.00         5\n",
            "          12       0.00      0.00      0.00         5\n",
            "          13       0.00      0.00      0.00         5\n",
            "          14       0.00      0.00      0.00         5\n",
            "          15       0.00      0.00      0.00         5\n",
            "          16       0.00      0.00      0.00         5\n",
            "          17       0.00      0.00      0.00         5\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00         5\n",
            "          20       0.00      0.00      0.00         5\n",
            "          21       0.00      0.00      0.00         5\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.00      0.00      0.00         5\n",
            "          24       0.00      0.00      0.00         5\n",
            "          25       0.00      0.00      0.00         5\n",
            "          26       0.00      0.00      0.00         5\n",
            "          27       0.00      0.00      0.00         5\n",
            "          28       0.00      0.00      0.00         5\n",
            "          29       0.00      0.00      0.00         5\n",
            "          30       0.00      0.00      0.00         5\n",
            "          31       0.00      0.00      0.00         5\n",
            "          32       0.00      0.00      0.00         5\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.00      0.00      0.00         5\n",
            "          35       0.00      0.00      0.00         5\n",
            "          36       0.00      0.00      0.00         5\n",
            "          37       0.00      0.00      0.00         5\n",
            "          38       0.00      0.00      0.00         5\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00         5\n",
            "          41       0.00      0.00      0.00         5\n",
            "          42       0.00      0.00      0.00         5\n",
            "          43       0.00      0.00      0.00         5\n",
            "          44       0.00      0.00      0.00         5\n",
            "          45       0.00      0.00      0.00         5\n",
            "          46       0.00      0.00      0.00         5\n",
            "          47       0.00      0.00      0.00         5\n",
            "          48       0.00      0.00      0.00         5\n",
            "          49       0.00      0.00      0.00         5\n",
            "          50       0.00      0.00      0.00         5\n",
            "          51       0.00      0.00      0.00         5\n",
            "          52       0.00      0.00      0.00         5\n",
            "          53       0.00      0.00      0.00         5\n",
            "          54       0.00      0.00      0.00         5\n",
            "          55       0.00      0.00      0.00         5\n",
            "          56       0.00      0.00      0.00         5\n",
            "          57       0.00      0.00      0.00         5\n",
            "          58       0.00      0.00      0.00         5\n",
            "          59       0.00      0.00      0.00         5\n",
            "          60       0.00      0.00      0.00         5\n",
            "          61       0.00      0.00      0.00         5\n",
            "          62       0.00      0.00      0.00         5\n",
            "          63       0.00      0.00      0.00         5\n",
            "          64       0.00      0.00      0.00         5\n",
            "          65       0.00      0.00      0.00         5\n",
            "          66       0.00      0.00      0.00         5\n",
            "          67       0.00      0.00      0.00         5\n",
            "          68       0.00      0.00      0.00         5\n",
            "          69       0.00      0.00      0.00         5\n",
            "          70       0.00      0.00      0.00         5\n",
            "          71       0.00      0.00      0.00         5\n",
            "          72       0.00      0.00      0.00         5\n",
            "          73       0.00      0.00      0.00         5\n",
            "          74       0.00      0.00      0.00         5\n",
            "          75       0.20      0.20      0.20         5\n",
            "          76       0.00      0.00      0.00         5\n",
            "          77       0.00      0.00      0.00         5\n",
            "          78       0.00      0.00      0.00         5\n",
            "          79       0.00      0.00      0.00         5\n",
            "          80       0.00      0.00      0.00         5\n",
            "          81       0.00      0.00      0.00         5\n",
            "          82       0.00      0.00      0.00         5\n",
            "          83       0.00      0.00      0.00         5\n",
            "          84       0.00      0.00      0.00         5\n",
            "          85       0.00      0.00      0.00         5\n",
            "          86       0.00      0.00      0.00         5\n",
            "          87       0.00      0.00      0.00         5\n",
            "          88       0.00      0.00      0.00         5\n",
            "          89       0.00      0.00      0.00         5\n",
            "          90       0.00      0.00      0.00         5\n",
            "          91       0.00      0.00      0.00         5\n",
            "          92       0.00      0.00      0.00         5\n",
            "          93       0.00      0.00      0.00         5\n",
            "          94       0.00      0.00      0.00         5\n",
            "          95       0.00      0.00      0.00         5\n",
            "          96       0.00      0.00      0.00         5\n",
            "          97       0.00      0.00      0.00         5\n",
            "          98       0.00      0.00      0.00         5\n",
            "          99       0.00      0.00      0.00         5\n",
            "         100       0.00      0.00      0.00         5\n",
            "         101       0.00      0.00      0.00         5\n",
            "         102       0.00      0.00      0.00         5\n",
            "         103       0.00      0.00      0.00         5\n",
            "         104       0.00      0.00      0.00         5\n",
            "         105       0.00      0.00      0.00         5\n",
            "         106       0.00      0.00      0.00         5\n",
            "         107       0.00      0.00      0.00         5\n",
            "         108       0.00      0.00      0.00         5\n",
            "         109       0.00      0.00      0.00         5\n",
            "         110       0.00      0.00      0.00         5\n",
            "         111       0.00      0.00      0.00         5\n",
            "         112       0.00      0.00      0.00         5\n",
            "         113       0.00      0.00      0.00         5\n",
            "         114       0.00      0.00      0.00         5\n",
            "         115       0.00      0.00      0.00         5\n",
            "         116       0.00      0.00      0.00         5\n",
            "         117       0.00      0.00      0.00         5\n",
            "         118       0.00      0.00      0.00         5\n",
            "         119       0.00      0.00      0.00         5\n",
            "         120       0.00      0.00      0.00         5\n",
            "         121       0.00      0.00      0.00         5\n",
            "         122       0.00      0.00      0.00         5\n",
            "         123       0.00      0.00      0.00         5\n",
            "         124       0.00      0.00      0.00         5\n",
            "         125       0.00      0.00      0.00         5\n",
            "         126       0.00      0.00      0.00         5\n",
            "         127       0.00      0.00      0.00         5\n",
            "         128       0.14      0.20      0.17         5\n",
            "         129       0.00      0.00      0.00         5\n",
            "         130       0.00      0.00      0.00         5\n",
            "         131       0.00      0.00      0.00         5\n",
            "         132       0.00      0.00      0.00         5\n",
            "         133       0.00      0.00      0.00         5\n",
            "         134       0.00      0.00      0.00         5\n",
            "         135       0.00      0.00      0.00         5\n",
            "         136       0.00      0.00      0.00         5\n",
            "         137       0.00      0.00      0.00         5\n",
            "         138       0.09      0.20      0.13         5\n",
            "         139       0.00      0.00      0.00         5\n",
            "         140       0.00      0.00      0.00         5\n",
            "         141       0.00      0.00      0.00         5\n",
            "         142       0.00      0.00      0.00         5\n",
            "         143       0.00      0.00      0.00         5\n",
            "         144       0.00      0.00      0.00         5\n",
            "         145       0.00      0.00      0.00         5\n",
            "         146       0.00      0.00      0.00         5\n",
            "         147       0.00      0.00      0.00         5\n",
            "         148       0.00      0.00      0.00         5\n",
            "         149       0.00      0.00      0.00         5\n",
            "         150       0.00      0.00      0.00         5\n",
            "         151       0.00      0.00      0.00         5\n",
            "         152       0.00      0.00      0.00         5\n",
            "         153       0.00      0.00      0.00         5\n",
            "         154       0.00      0.00      0.00         5\n",
            "         155       0.00      0.00      0.00         5\n",
            "         156       0.00      0.00      0.00         5\n",
            "         157       0.00      0.00      0.00         5\n",
            "         158       0.00      0.00      0.00         5\n",
            "         159       0.00      0.00      0.00         5\n",
            "         160       0.00      0.00      0.00         5\n",
            "         161       0.00      0.00      0.00         5\n",
            "         162       0.00      0.00      0.00         5\n",
            "         163       0.00      0.00      0.00         5\n",
            "         164       0.00      0.00      0.00         5\n",
            "         165       0.00      0.00      0.00         5\n",
            "         166       0.00      0.00      0.00         5\n",
            "         167       0.00      0.00      0.00         5\n",
            "         168       0.00      0.00      0.00         5\n",
            "         169       0.00      0.00      0.00         5\n",
            "         170       0.00      0.00      0.00         5\n",
            "         171       0.00      0.00      0.00         5\n",
            "         172       0.00      0.00      0.00         5\n",
            "         173       0.00      0.00      0.00         5\n",
            "         174       0.00      0.00      0.00         5\n",
            "         175       0.00      0.00      0.00         5\n",
            "         176       0.00      0.00      0.00         5\n",
            "         177       0.00      0.00      0.00         5\n",
            "         178       0.00      0.00      0.00         5\n",
            "         179       0.00      0.00      0.00         5\n",
            "         180       0.00      0.00      0.00         5\n",
            "         181       0.00      0.00      0.00         5\n",
            "         182       0.00      0.00      0.00         5\n",
            "         183       0.00      0.00      0.00         5\n",
            "         184       0.00      0.00      0.00         5\n",
            "         185       0.00      0.00      0.00         5\n",
            "         186       0.00      0.00      0.00         5\n",
            "         187       0.00      0.00      0.00         5\n",
            "         188       0.00      0.00      0.00         5\n",
            "         189       0.00      0.00      0.00         5\n",
            "         190       0.00      0.00      0.00         5\n",
            "         191       0.00      0.00      0.00         5\n",
            "         192       0.00      0.00      0.00         5\n",
            "         193       0.00      0.00      0.00         5\n",
            "         194       0.00      0.00      0.00         5\n",
            "         195       0.00      0.00      0.00         5\n",
            "         196       0.00      0.00      0.00         5\n",
            "         197       0.00      0.00      0.00         5\n",
            "         198       0.00      0.00      0.00         5\n",
            "         199       0.00      0.00      0.00         5\n",
            "         200       0.00      0.00      0.00         5\n",
            "         201       0.00      0.00      0.00         5\n",
            "         202       0.00      0.00      0.00         5\n",
            "         203       0.00      0.00      0.00         5\n",
            "         204       0.00      0.00      0.00         5\n",
            "         205       0.00      0.00      0.00         5\n",
            "         206       0.00      0.00      0.00         5\n",
            "         207       0.00      0.00      0.00         5\n",
            "         208       0.00      0.00      0.00         5\n",
            "         209       0.00      0.00      0.00         5\n",
            "         210       0.00      0.00      0.00         5\n",
            "         211       0.00      0.00      0.00         5\n",
            "         212       0.00      0.00      0.00         5\n",
            "         213       0.00      0.00      0.00         5\n",
            "         214       0.00      0.00      0.00         5\n",
            "         215       0.00      0.00      0.00         5\n",
            "         216       0.00      0.00      0.00         5\n",
            "         217       0.00      0.00      0.00         5\n",
            "         218       0.00      0.00      0.00         5\n",
            "         219       0.00      0.00      0.00         5\n",
            "         220       0.00      0.00      0.00         5\n",
            "         221       0.00      0.00      0.00         5\n",
            "         222       0.00      0.00      0.00         5\n",
            "         223       0.00      0.00      0.00         5\n",
            "         224       0.00      0.00      0.00         5\n",
            "         225       0.00      0.00      0.00         5\n",
            "         226       0.00      0.00      0.00         5\n",
            "         227       0.00      0.00      0.00         5\n",
            "         228       0.00      0.00      0.00         5\n",
            "         229       0.33      0.20      0.25         5\n",
            "         230       0.00      0.00      0.00         5\n",
            "         231       0.00      0.00      0.00         5\n",
            "         232       0.00      0.00      0.00         5\n",
            "         233       0.00      0.00      0.00         5\n",
            "         234       0.00      0.00      0.00         5\n",
            "         235       0.00      0.00      0.00         5\n",
            "         236       0.00      0.00      0.00         5\n",
            "         237       0.00      0.00      0.00         5\n",
            "         238       0.00      0.00      0.00         5\n",
            "         239       0.00      0.00      0.00         5\n",
            "         240       0.00      0.00      0.00         5\n",
            "         241       0.00      0.00      0.00         5\n",
            "         242       0.00      0.00      0.00         5\n",
            "         243       0.00      0.00      0.00         5\n",
            "         244       0.00      0.00      0.00         5\n",
            "         245       0.00      0.00      0.00         5\n",
            "         246       0.00      0.00      0.00         5\n",
            "         247       0.00      0.00      0.00         5\n",
            "         248       0.00      0.00      0.00         5\n",
            "         249       0.00      0.00      0.00         5\n",
            "         250       0.00      0.00      0.00         5\n",
            "         251       0.00      0.00      0.00         5\n",
            "         252       0.00      0.00      0.00         5\n",
            "         253       0.00      0.00      0.00         5\n",
            "         254       0.00      0.00      0.00         5\n",
            "         255       0.00      0.00      0.00         5\n",
            "         256       0.00      0.00      0.00         5\n",
            "         257       0.12      0.20      0.15         5\n",
            "         258       0.00      0.00      0.00         5\n",
            "         259       0.00      0.00      0.00         5\n",
            "         260       0.00      0.00      0.00         5\n",
            "         261       0.00      0.00      0.00         5\n",
            "         262       0.00      0.00      0.00         5\n",
            "         263       0.00      0.00      0.00         5\n",
            "         264       0.00      0.00      0.00         5\n",
            "         265       0.00      0.00      0.00         5\n",
            "         266       0.00      0.00      0.00         5\n",
            "         267       0.00      0.00      0.00         5\n",
            "         268       0.00      0.00      0.00         5\n",
            "         269       0.00      0.00      0.00         5\n",
            "         270       0.00      0.00      0.00         5\n",
            "         271       0.00      0.00      0.00         5\n",
            "         272       0.00      0.00      0.00         5\n",
            "         273       0.00      0.00      0.00         5\n",
            "         274       0.00      0.00      0.00         5\n",
            "         275       0.00      0.00      0.00         5\n",
            "         276       0.00      0.00      0.00         5\n",
            "         277       0.00      0.00      0.00         5\n",
            "         278       0.00      0.00      0.00         5\n",
            "         279       0.00      0.00      0.00         5\n",
            "         280       0.00      0.00      0.00         5\n",
            "         281       0.00      0.00      0.00         5\n",
            "         282       0.20      0.20      0.20         5\n",
            "         283       0.00      0.00      0.00         5\n",
            "         284       0.00      0.00      0.00         5\n",
            "         285       0.00      0.00      0.00         5\n",
            "         286       0.00      0.00      0.00         5\n",
            "         287       0.00      0.00      0.00         5\n",
            "         288       0.00      0.00      0.00         5\n",
            "         289       0.00      0.00      0.00         5\n",
            "         290       0.00      0.00      0.00         5\n",
            "         291       0.00      0.00      0.00         5\n",
            "         292       0.00      0.00      0.00         5\n",
            "         293       0.00      0.00      0.00         5\n",
            "         294       0.00      0.00      0.00         5\n",
            "         295       0.00      0.00      0.00         5\n",
            "         296       0.00      0.00      0.00         5\n",
            "         297       0.00      0.00      0.00         5\n",
            "         298       0.00      0.00      0.00         5\n",
            "         299       0.00      0.00      0.00         5\n",
            "         300       0.00      0.00      0.00         5\n",
            "         301       0.00      0.00      0.00         5\n",
            "         302       0.00      0.00      0.00         5\n",
            "         303       0.00      0.00      0.00         5\n",
            "         304       0.00      0.00      0.00         5\n",
            "         305       0.00      0.00      0.00         5\n",
            "         306       0.00      0.00      0.00         5\n",
            "         307       0.00      0.00      0.00         5\n",
            "         308       0.00      0.00      0.00         5\n",
            "         309       0.00      0.00      0.00         5\n",
            "         310       0.00      0.00      0.00         5\n",
            "         311       0.00      0.00      0.00         5\n",
            "         312       0.00      0.00      0.00         5\n",
            "         313       0.00      0.00      0.00         5\n",
            "         314       0.00      0.00      0.00         5\n",
            "         315       0.00      0.00      0.00         5\n",
            "         316       0.00      0.00      0.00         5\n",
            "         317       0.00      0.00      0.00         5\n",
            "         318       0.00      0.00      0.00         5\n",
            "         319       0.00      0.00      0.00         5\n",
            "         320       0.00      0.00      0.00         5\n",
            "         321       0.00      0.00      0.00         5\n",
            "         322       0.00      0.00      0.00         5\n",
            "         323       0.00      0.00      0.00         5\n",
            "         324       0.00      0.00      0.00         5\n",
            "         325       0.00      0.00      0.00         5\n",
            "         326       0.00      0.00      0.00         5\n",
            "         327       0.00      0.00      0.00         5\n",
            "         328       0.00      0.00      0.00         5\n",
            "         329       0.00      0.00      0.00         5\n",
            "         330       0.00      0.00      0.00         5\n",
            "         331       0.00      0.00      0.00         5\n",
            "         332       0.00      0.00      0.00         5\n",
            "         333       0.00      0.00      0.00         5\n",
            "         334       0.00      0.00      0.00         5\n",
            "         335       0.00      0.00      0.00         5\n",
            "         336       0.00      0.00      0.00         5\n",
            "         337       0.00      0.00      0.00         5\n",
            "         338       0.00      0.00      0.00         5\n",
            "         339       0.00      0.00      0.00         5\n",
            "         340       0.20      0.20      0.20         5\n",
            "         341       0.00      0.00      0.00         5\n",
            "         342       0.00      0.00      0.00         5\n",
            "         343       0.00      0.00      0.00         5\n",
            "         344       0.00      0.00      0.00         5\n",
            "         345       0.00      0.00      0.00         5\n",
            "         346       0.00      0.00      0.00         5\n",
            "         347       0.00      0.00      0.00         5\n",
            "         348       0.00      0.00      0.00         5\n",
            "         349       0.20      0.20      0.20         5\n",
            "         350       0.00      0.00      0.00         5\n",
            "         351       0.00      0.00      0.00         5\n",
            "         352       0.00      0.00      0.00         5\n",
            "         353       0.00      0.00      0.00         5\n",
            "         354       0.00      0.00      0.00         5\n",
            "         355       0.00      0.00      0.00         5\n",
            "         356       0.00      0.00      0.00         5\n",
            "         357       0.00      0.00      0.00         5\n",
            "         358       0.00      0.00      0.00         5\n",
            "         359       0.00      0.00      0.00         5\n",
            "         360       0.00      0.00      0.00         5\n",
            "         361       0.00      0.00      0.00         5\n",
            "         362       0.00      0.00      0.00         5\n",
            "         363       0.00      0.00      0.00         5\n",
            "         364       0.00      0.00      0.00         5\n",
            "         365       0.00      0.00      0.00         5\n",
            "         366       0.00      0.00      0.00         5\n",
            "         367       0.00      0.00      0.00         5\n",
            "         368       0.00      0.00      0.00         5\n",
            "         369       0.00      0.00      0.00         5\n",
            "         370       0.00      0.00      0.00         5\n",
            "         371       0.00      0.00      0.00         5\n",
            "         372       0.00      0.00      0.00         5\n",
            "         373       0.00      0.00      0.00         5\n",
            "         374       0.00      0.00      0.00         5\n",
            "         375       0.00      0.00      0.00         5\n",
            "         376       0.00      0.00      0.00         5\n",
            "         377       0.00      0.00      0.00         5\n",
            "         378       0.00      0.00      0.00         5\n",
            "         379       0.00      0.00      0.00         5\n",
            "         380       0.00      0.00      0.00         5\n",
            "         381       0.00      0.00      0.00         5\n",
            "         382       0.00      0.00      0.00         5\n",
            "         383       0.00      0.00      0.00         5\n",
            "         384       0.00      0.00      0.00         5\n",
            "         385       0.00      0.00      0.00         5\n",
            "         386       0.00      0.00      0.00         5\n",
            "         387       0.00      0.00      0.00         5\n",
            "         388       0.00      0.00      0.00         5\n",
            "         389       0.00      0.00      0.00         5\n",
            "         390       0.00      0.00      0.00         5\n",
            "         391       0.00      0.00      0.00         5\n",
            "         392       0.00      0.00      0.00         5\n",
            "         393       0.00      0.00      0.00         5\n",
            "         394       0.00      0.00      0.00         5\n",
            "         395       0.00      0.00      0.00         5\n",
            "         396       0.00      0.00      0.00         5\n",
            "         397       0.00      0.00      0.00         5\n",
            "         398       0.00      0.00      0.00         5\n",
            "         399       0.20      0.20      0.20         5\n",
            "         400       0.00      0.00      0.00         5\n",
            "         401       0.00      0.00      0.00         5\n",
            "         402       0.00      0.00      0.00         5\n",
            "         403       0.00      0.00      0.00         5\n",
            "         404       0.00      0.00      0.00         5\n",
            "         405       0.00      0.00      0.00         5\n",
            "         406       0.00      0.00      0.00         5\n",
            "         407       0.00      0.00      0.00         5\n",
            "         408       0.00      0.00      0.00         5\n",
            "         409       0.00      0.00      0.00         5\n",
            "         410       0.00      0.00      0.00         5\n",
            "         411       0.00      0.00      0.00         5\n",
            "         412       0.00      0.00      0.00         5\n",
            "         413       0.00      0.00      0.00         5\n",
            "         414       0.00      0.00      0.00         5\n",
            "         415       0.00      0.00      0.00         5\n",
            "         416       0.00      0.00      0.00         5\n",
            "         417       0.00      0.00      0.00         5\n",
            "         418       0.00      0.00      0.00         5\n",
            "         419       0.00      0.00      0.00         5\n",
            "         420       0.00      0.00      0.00         5\n",
            "         421       0.00      0.00      0.00         5\n",
            "         422       0.00      0.00      0.00         5\n",
            "         423       0.00      0.00      0.00         5\n",
            "         424       0.00      0.00      0.00         5\n",
            "         425       0.00      0.00      0.00         5\n",
            "         426       0.00      0.00      0.00         5\n",
            "         427       0.00      0.00      0.00         5\n",
            "         428       0.00      0.00      0.00         5\n",
            "         429       0.00      0.00      0.00         5\n",
            "         430       0.00      0.00      0.00         5\n",
            "         431       0.00      0.00      0.00         5\n",
            "         432       0.00      0.00      0.00         5\n",
            "         433       0.00      0.00      0.00         5\n",
            "         434       0.00      0.00      0.00         5\n",
            "         435       0.00      0.00      0.00         5\n",
            "         436       0.00      0.00      0.00         5\n",
            "         437       0.00      0.00      0.00         5\n",
            "         438       0.00      0.00      0.00         5\n",
            "         439       0.00      0.00      0.00         5\n",
            "         440       0.00      0.00      0.00         5\n",
            "         441       0.00      0.00      0.00         5\n",
            "         442       0.00      0.00      0.00         5\n",
            "         443       0.00      0.00      0.00         5\n",
            "         444       0.00      0.00      0.00         5\n",
            "         445       0.00      0.00      0.00         5\n",
            "         446       0.00      0.00      0.00         5\n",
            "         447       0.00      0.00      0.00         5\n",
            "         448       0.00      0.00      0.00         5\n",
            "         449       0.00      0.00      0.00         5\n",
            "         450       0.00      0.00      0.00         5\n",
            "         451       0.00      0.00      0.00         5\n",
            "         452       0.00      0.00      0.00         5\n",
            "         453       0.00      0.00      0.00         5\n",
            "         454       0.00      0.00      0.00         5\n",
            "         455       0.00      0.00      0.00         5\n",
            "         456       0.00      0.00      0.00         5\n",
            "         457       0.00      0.00      0.00         5\n",
            "         458       0.00      0.00      0.00         5\n",
            "         459       0.00      0.00      0.00         5\n",
            "         460       0.00      0.00      0.00         5\n",
            "         461       0.00      0.00      0.00         5\n",
            "         462       0.00      0.00      0.00         5\n",
            "         463       0.00      0.00      0.00         5\n",
            "         464       0.00      0.00      0.00         5\n",
            "         465       0.00      0.00      0.00         5\n",
            "         466       0.00      0.00      0.00         5\n",
            "         467       0.00      0.00      0.00         5\n",
            "         468       0.00      0.00      0.00         5\n",
            "         469       0.00      0.00      0.00         5\n",
            "         470       0.00      0.00      0.00         5\n",
            "         471       0.00      0.00      0.00         5\n",
            "         472       0.00      0.00      0.00         5\n",
            "         473       0.00      0.00      0.00         5\n",
            "         474       0.00      0.00      0.00         5\n",
            "         475       0.00      0.00      0.00         5\n",
            "         476       0.00      0.00      0.00         5\n",
            "         477       0.00      0.00      0.00         5\n",
            "         478       0.00      0.00      0.00         5\n",
            "         479       0.00      0.00      0.00         5\n",
            "         480       0.00      0.00      0.00         5\n",
            "         481       0.00      0.00      0.00         5\n",
            "         482       0.00      0.00      0.00         5\n",
            "         483       0.00      0.00      0.00         5\n",
            "         484       0.00      0.00      0.00         5\n",
            "         485       0.00      0.00      0.00         5\n",
            "         486       0.00      0.00      0.00         5\n",
            "         487       0.00      0.00      0.00         5\n",
            "         488       0.00      0.00      0.00         5\n",
            "         489       0.00      0.00      0.00         5\n",
            "         490       0.00      0.00      0.00         5\n",
            "         491       0.00      0.00      0.00         5\n",
            "         492       0.00      0.00      0.00         5\n",
            "         493       0.00      0.00      0.00         5\n",
            "         494       0.00      0.00      0.00         5\n",
            "         495       0.00      0.00      0.00         5\n",
            "         496       0.00      0.00      0.00         5\n",
            "         497       0.00      0.00      0.00         5\n",
            "         498       0.00      0.00      0.00         5\n",
            "         499       0.00      0.00      0.00         5\n",
            "         500       0.00      0.00      0.00         5\n",
            "         501       0.00      0.00      0.00         5\n",
            "         502       0.00      0.00      0.00         5\n",
            "         503       0.00      0.00      0.00         5\n",
            "         504       0.00      0.00      0.00         5\n",
            "         505       0.00      0.00      0.00         5\n",
            "         506       0.00      0.00      0.00         5\n",
            "         507       0.00      0.00      0.00         5\n",
            "         508       0.00      0.00      0.00         5\n",
            "         509       0.00      0.00      0.00         5\n",
            "         510       0.00      0.00      0.00         5\n",
            "         511       0.00      0.00      0.00         5\n",
            "         512       0.00      0.00      0.00         5\n",
            "         513       0.00      0.00      0.00         5\n",
            "         514       0.00      0.00      0.00         5\n",
            "         515       0.00      0.00      0.00         5\n",
            "         516       0.00      0.00      0.00         5\n",
            "         517       0.00      0.00      0.00         5\n",
            "         518       0.00      0.00      0.00         5\n",
            "         519       0.00      0.00      0.00         5\n",
            "         520       0.00      0.00      0.00         5\n",
            "         521       0.00      0.00      0.00         5\n",
            "         522       0.00      0.00      0.00         5\n",
            "         523       0.00      0.00      0.00         5\n",
            "         524       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.00      2625\n",
            "   macro avg       0.00      0.00      0.00      2625\n",
            "weighted avg       0.00      0.00      0.00      2625\n",
            "\n",
            "Confusion Matrix:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "83/83 [==============================] - 10s 96ms/step\n",
            "DenseNet121 Metrics:\n",
            "Accuracy: 0.0015238095238095239\n",
            "F1 Score: 0.0015738335738335737\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         5\n",
            "           1       0.00      0.00      0.00         5\n",
            "           2       0.00      0.00      0.00         5\n",
            "           3       0.00      0.00      0.00         5\n",
            "           4       0.00      0.00      0.00         5\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.00      0.00      0.00         5\n",
            "           7       0.00      0.00      0.00         5\n",
            "           8       0.00      0.00      0.00         5\n",
            "           9       0.00      0.00      0.00         5\n",
            "          10       0.00      0.00      0.00         5\n",
            "          11       0.00      0.00      0.00         5\n",
            "          12       0.00      0.00      0.00         5\n",
            "          13       0.00      0.00      0.00         5\n",
            "          14       0.00      0.00      0.00         5\n",
            "          15       0.00      0.00      0.00         5\n",
            "          16       0.00      0.00      0.00         5\n",
            "          17       0.00      0.00      0.00         5\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.00      0.00      0.00         5\n",
            "          20       0.00      0.00      0.00         5\n",
            "          21       0.00      0.00      0.00         5\n",
            "          22       0.00      0.00      0.00         5\n",
            "          23       0.00      0.00      0.00         5\n",
            "          24       0.00      0.00      0.00         5\n",
            "          25       0.00      0.00      0.00         5\n",
            "          26       0.00      0.00      0.00         5\n",
            "          27       0.00      0.00      0.00         5\n",
            "          28       0.00      0.00      0.00         5\n",
            "          29       0.00      0.00      0.00         5\n",
            "          30       0.00      0.00      0.00         5\n",
            "          31       0.00      0.00      0.00         5\n",
            "          32       0.00      0.00      0.00         5\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.00      0.00      0.00         5\n",
            "          35       0.00      0.00      0.00         5\n",
            "          36       0.00      0.00      0.00         5\n",
            "          37       0.00      0.00      0.00         5\n",
            "          38       0.00      0.00      0.00         5\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00         5\n",
            "          41       0.00      0.00      0.00         5\n",
            "          42       0.00      0.00      0.00         5\n",
            "          43       0.00      0.00      0.00         5\n",
            "          44       0.00      0.00      0.00         5\n",
            "          45       0.00      0.00      0.00         5\n",
            "          46       0.00      0.00      0.00         5\n",
            "          47       0.00      0.00      0.00         5\n",
            "          48       0.00      0.00      0.00         5\n",
            "          49       0.00      0.00      0.00         5\n",
            "          50       0.00      0.00      0.00         5\n",
            "          51       0.00      0.00      0.00         5\n",
            "          52       0.00      0.00      0.00         5\n",
            "          53       0.00      0.00      0.00         5\n",
            "          54       0.00      0.00      0.00         5\n",
            "          55       0.00      0.00      0.00         5\n",
            "          56       0.00      0.00      0.00         5\n",
            "          57       0.00      0.00      0.00         5\n",
            "          58       0.00      0.00      0.00         5\n",
            "          59       0.00      0.00      0.00         5\n",
            "          60       0.00      0.00      0.00         5\n",
            "          61       0.00      0.00      0.00         5\n",
            "          62       0.00      0.00      0.00         5\n",
            "          63       0.00      0.00      0.00         5\n",
            "          64       0.00      0.00      0.00         5\n",
            "          65       0.00      0.00      0.00         5\n",
            "          66       0.00      0.00      0.00         5\n",
            "          67       0.00      0.00      0.00         5\n",
            "          68       0.00      0.00      0.00         5\n",
            "          69       0.00      0.00      0.00         5\n",
            "          70       0.00      0.00      0.00         5\n",
            "          71       0.00      0.00      0.00         5\n",
            "          72       0.00      0.00      0.00         5\n",
            "          73       0.00      0.00      0.00         5\n",
            "          74       0.00      0.00      0.00         5\n",
            "          75       0.00      0.00      0.00         5\n",
            "          76       0.00      0.00      0.00         5\n",
            "          77       0.00      0.00      0.00         5\n",
            "          78       0.00      0.00      0.00         5\n",
            "          79       0.00      0.00      0.00         5\n",
            "          80       0.00      0.00      0.00         5\n",
            "          81       0.00      0.00      0.00         5\n",
            "          82       0.00      0.00      0.00         5\n",
            "          83       0.00      0.00      0.00         5\n",
            "          84       0.00      0.00      0.00         5\n",
            "          85       0.00      0.00      0.00         5\n",
            "          86       0.00      0.00      0.00         5\n",
            "          87       0.00      0.00      0.00         5\n",
            "          88       0.00      0.00      0.00         5\n",
            "          89       0.00      0.00      0.00         5\n",
            "          90       0.00      0.00      0.00         5\n",
            "          91       0.00      0.00      0.00         5\n",
            "          92       0.00      0.00      0.00         5\n",
            "          93       0.00      0.00      0.00         5\n",
            "          94       0.00      0.00      0.00         5\n",
            "          95       0.00      0.00      0.00         5\n",
            "          96       0.00      0.00      0.00         5\n",
            "          97       0.00      0.00      0.00         5\n",
            "          98       0.00      0.00      0.00         5\n",
            "          99       0.00      0.00      0.00         5\n",
            "         100       0.00      0.00      0.00         5\n",
            "         101       0.00      0.00      0.00         5\n",
            "         102       0.00      0.00      0.00         5\n",
            "         103       0.00      0.00      0.00         5\n",
            "         104       0.00      0.00      0.00         5\n",
            "         105       0.00      0.00      0.00         5\n",
            "         106       0.00      0.00      0.00         5\n",
            "         107       0.00      0.00      0.00         5\n",
            "         108       0.00      0.00      0.00         5\n",
            "         109       0.00      0.00      0.00         5\n",
            "         110       0.00      0.00      0.00         5\n",
            "         111       0.00      0.00      0.00         5\n",
            "         112       0.00      0.00      0.00         5\n",
            "         113       0.00      0.00      0.00         5\n",
            "         114       0.00      0.00      0.00         5\n",
            "         115       0.00      0.00      0.00         5\n",
            "         116       0.00      0.00      0.00         5\n",
            "         117       0.00      0.00      0.00         5\n",
            "         118       0.00      0.00      0.00         5\n",
            "         119       0.00      0.00      0.00         5\n",
            "         120       0.00      0.00      0.00         5\n",
            "         121       0.00      0.00      0.00         5\n",
            "         122       0.00      0.00      0.00         5\n",
            "         123       0.00      0.00      0.00         5\n",
            "         124       0.00      0.00      0.00         5\n",
            "         125       0.00      0.00      0.00         5\n",
            "         126       0.00      0.00      0.00         5\n",
            "         127       0.00      0.00      0.00         5\n",
            "         128       0.00      0.00      0.00         5\n",
            "         129       0.00      0.00      0.00         5\n",
            "         130       0.00      0.00      0.00         5\n",
            "         131       0.00      0.00      0.00         5\n",
            "         132       0.00      0.00      0.00         5\n",
            "         133       0.00      0.00      0.00         5\n",
            "         134       0.00      0.00      0.00         5\n",
            "         135       0.00      0.00      0.00         5\n",
            "         136       0.00      0.00      0.00         5\n",
            "         137       0.00      0.00      0.00         5\n",
            "         138       0.00      0.00      0.00         5\n",
            "         139       0.00      0.00      0.00         5\n",
            "         140       0.00      0.00      0.00         5\n",
            "         141       0.00      0.00      0.00         5\n",
            "         142       0.00      0.00      0.00         5\n",
            "         143       0.00      0.00      0.00         5\n",
            "         144       0.17      0.20      0.18         5\n",
            "         145       0.00      0.00      0.00         5\n",
            "         146       0.00      0.00      0.00         5\n",
            "         147       0.00      0.00      0.00         5\n",
            "         148       0.00      0.00      0.00         5\n",
            "         149       0.00      0.00      0.00         5\n",
            "         150       0.00      0.00      0.00         5\n",
            "         151       0.00      0.00      0.00         5\n",
            "         152       0.00      0.00      0.00         5\n",
            "         153       0.00      0.00      0.00         5\n",
            "         154       0.00      0.00      0.00         5\n",
            "         155       0.00      0.00      0.00         5\n",
            "         156       0.00      0.00      0.00         5\n",
            "         157       0.00      0.00      0.00         5\n",
            "         158       0.00      0.00      0.00         5\n",
            "         159       0.00      0.00      0.00         5\n",
            "         160       0.00      0.00      0.00         5\n",
            "         161       0.00      0.00      0.00         5\n",
            "         162       0.00      0.00      0.00         5\n",
            "         163       0.00      0.00      0.00         5\n",
            "         164       0.00      0.00      0.00         5\n",
            "         165       0.00      0.00      0.00         5\n",
            "         166       0.00      0.00      0.00         5\n",
            "         167       0.00      0.00      0.00         5\n",
            "         168       0.00      0.00      0.00         5\n",
            "         169       0.00      0.00      0.00         5\n",
            "         170       0.00      0.00      0.00         5\n",
            "         171       0.00      0.00      0.00         5\n",
            "         172       0.00      0.00      0.00         5\n",
            "         173       0.00      0.00      0.00         5\n",
            "         174       0.00      0.00      0.00         5\n",
            "         175       0.00      0.00      0.00         5\n",
            "         176       0.00      0.00      0.00         5\n",
            "         177       0.00      0.00      0.00         5\n",
            "         178       0.00      0.00      0.00         5\n",
            "         179       0.00      0.00      0.00         5\n",
            "         180       0.00      0.00      0.00         5\n",
            "         181       0.00      0.00      0.00         5\n",
            "         182       0.00      0.00      0.00         5\n",
            "         183       0.00      0.00      0.00         5\n",
            "         184       0.00      0.00      0.00         5\n",
            "         185       0.00      0.00      0.00         5\n",
            "         186       0.00      0.00      0.00         5\n",
            "         187       0.00      0.00      0.00         5\n",
            "         188       0.00      0.00      0.00         5\n",
            "         189       0.00      0.00      0.00         5\n",
            "         190       0.00      0.00      0.00         5\n",
            "         191       0.00      0.00      0.00         5\n",
            "         192       0.00      0.00      0.00         5\n",
            "         193       0.00      0.00      0.00         5\n",
            "         194       0.00      0.00      0.00         5\n",
            "         195       0.00      0.00      0.00         5\n",
            "         196       0.00      0.00      0.00         5\n",
            "         197       0.00      0.00      0.00         5\n",
            "         198       0.00      0.00      0.00         5\n",
            "         199       0.00      0.00      0.00         5\n",
            "         200       0.00      0.00      0.00         5\n",
            "         201       0.00      0.00      0.00         5\n",
            "         202       0.00      0.00      0.00         5\n",
            "         203       0.00      0.00      0.00         5\n",
            "         204       0.00      0.00      0.00         5\n",
            "         205       0.00      0.00      0.00         5\n",
            "         206       0.00      0.00      0.00         5\n",
            "         207       0.00      0.00      0.00         5\n",
            "         208       0.00      0.00      0.00         5\n",
            "         209       0.00      0.00      0.00         5\n",
            "         210       0.00      0.00      0.00         5\n",
            "         211       0.00      0.00      0.00         5\n",
            "         212       0.00      0.00      0.00         5\n",
            "         213       0.00      0.00      0.00         5\n",
            "         214       0.00      0.00      0.00         5\n",
            "         215       0.00      0.00      0.00         5\n",
            "         216       0.00      0.00      0.00         5\n",
            "         217       0.00      0.00      0.00         5\n",
            "         218       0.00      0.00      0.00         5\n",
            "         219       0.00      0.00      0.00         5\n",
            "         220       0.00      0.00      0.00         5\n",
            "         221       0.00      0.00      0.00         5\n",
            "         222       0.00      0.00      0.00         5\n",
            "         223       0.00      0.00      0.00         5\n",
            "         224       0.00      0.00      0.00         5\n",
            "         225       0.00      0.00      0.00         5\n",
            "         226       0.00      0.00      0.00         5\n",
            "         227       0.00      0.00      0.00         5\n",
            "         228       0.00      0.00      0.00         5\n",
            "         229       0.00      0.00      0.00         5\n",
            "         230       0.00      0.00      0.00         5\n",
            "         231       0.00      0.00      0.00         5\n",
            "         232       0.00      0.00      0.00         5\n",
            "         233       0.00      0.00      0.00         5\n",
            "         234       0.00      0.00      0.00         5\n",
            "         235       0.00      0.00      0.00         5\n",
            "         236       0.00      0.00      0.00         5\n",
            "         237       0.00      0.00      0.00         5\n",
            "         238       0.00      0.00      0.00         5\n",
            "         239       0.00      0.00      0.00         5\n",
            "         240       0.00      0.00      0.00         5\n",
            "         241       0.00      0.00      0.00         5\n",
            "         242       0.00      0.00      0.00         5\n",
            "         243       0.00      0.00      0.00         5\n",
            "         244       0.00      0.00      0.00         5\n",
            "         245       0.25      0.20      0.22         5\n",
            "         246       0.00      0.00      0.00         5\n",
            "         247       0.00      0.00      0.00         5\n",
            "         248       0.00      0.00      0.00         5\n",
            "         249       0.00      0.00      0.00         5\n",
            "         250       0.00      0.00      0.00         5\n",
            "         251       0.00      0.00      0.00         5\n",
            "         252       0.00      0.00      0.00         5\n",
            "         253       0.00      0.00      0.00         5\n",
            "         254       0.00      0.00      0.00         5\n",
            "         255       0.00      0.00      0.00         5\n",
            "         256       0.00      0.00      0.00         5\n",
            "         257       0.00      0.00      0.00         5\n",
            "         258       0.00      0.00      0.00         5\n",
            "         259       0.00      0.00      0.00         5\n",
            "         260       0.00      0.00      0.00         5\n",
            "         261       0.00      0.00      0.00         5\n",
            "         262       0.00      0.00      0.00         5\n",
            "         263       0.00      0.00      0.00         5\n",
            "         264       0.00      0.00      0.00         5\n",
            "         265       0.00      0.00      0.00         5\n",
            "         266       0.00      0.00      0.00         5\n",
            "         267       0.00      0.00      0.00         5\n",
            "         268       0.00      0.00      0.00         5\n",
            "         269       0.00      0.00      0.00         5\n",
            "         270       0.00      0.00      0.00         5\n",
            "         271       0.00      0.00      0.00         5\n",
            "         272       0.00      0.00      0.00         5\n",
            "         273       0.20      0.20      0.20         5\n",
            "         274       0.00      0.00      0.00         5\n",
            "         275       0.00      0.00      0.00         5\n",
            "         276       0.00      0.00      0.00         5\n",
            "         277       0.00      0.00      0.00         5\n",
            "         278       0.00      0.00      0.00         5\n",
            "         279       0.00      0.00      0.00         5\n",
            "         280       0.00      0.00      0.00         5\n",
            "         281       0.00      0.00      0.00         5\n",
            "         282       0.00      0.00      0.00         5\n",
            "         283       0.00      0.00      0.00         5\n",
            "         284       0.00      0.00      0.00         5\n",
            "         285       0.00      0.00      0.00         5\n",
            "         286       0.00      0.00      0.00         5\n",
            "         287       0.00      0.00      0.00         5\n",
            "         288       0.00      0.00      0.00         5\n",
            "         289       0.00      0.00      0.00         5\n",
            "         290       0.00      0.00      0.00         5\n",
            "         291       0.00      0.00      0.00         5\n",
            "         292       0.00      0.00      0.00         5\n",
            "         293       0.00      0.00      0.00         5\n",
            "         294       0.00      0.00      0.00         5\n",
            "         295       0.00      0.00      0.00         5\n",
            "         296       0.00      0.00      0.00         5\n",
            "         297       0.00      0.00      0.00         5\n",
            "         298       0.00      0.00      0.00         5\n",
            "         299       0.00      0.00      0.00         5\n",
            "         300       0.00      0.00      0.00         5\n",
            "         301       0.00      0.00      0.00         5\n",
            "         302       0.00      0.00      0.00         5\n",
            "         303       0.00      0.00      0.00         5\n",
            "         304       0.00      0.00      0.00         5\n",
            "         305       0.00      0.00      0.00         5\n",
            "         306       0.00      0.00      0.00         5\n",
            "         307       0.00      0.00      0.00         5\n",
            "         308       0.00      0.00      0.00         5\n",
            "         309       0.00      0.00      0.00         5\n",
            "         310       0.00      0.00      0.00         5\n",
            "         311       0.00      0.00      0.00         5\n",
            "         312       0.00      0.00      0.00         5\n",
            "         313       0.00      0.00      0.00         5\n",
            "         314       0.00      0.00      0.00         5\n",
            "         315       0.00      0.00      0.00         5\n",
            "         316       0.00      0.00      0.00         5\n",
            "         317       0.00      0.00      0.00         5\n",
            "         318       0.00      0.00      0.00         5\n",
            "         319       0.00      0.00      0.00         5\n",
            "         320       0.00      0.00      0.00         5\n",
            "         321       0.00      0.00      0.00         5\n",
            "         322       0.00      0.00      0.00         5\n",
            "         323       0.00      0.00      0.00         5\n",
            "         324       0.00      0.00      0.00         5\n",
            "         325       0.00      0.00      0.00         5\n",
            "         326       0.00      0.00      0.00         5\n",
            "         327       0.00      0.00      0.00         5\n",
            "         328       0.00      0.00      0.00         5\n",
            "         329       0.00      0.00      0.00         5\n",
            "         330       0.00      0.00      0.00         5\n",
            "         331       0.00      0.00      0.00         5\n",
            "         332       0.00      0.00      0.00         5\n",
            "         333       0.00      0.00      0.00         5\n",
            "         334       0.00      0.00      0.00         5\n",
            "         335       0.00      0.00      0.00         5\n",
            "         336       0.00      0.00      0.00         5\n",
            "         337       0.00      0.00      0.00         5\n",
            "         338       0.00      0.00      0.00         5\n",
            "         339       0.00      0.00      0.00         5\n",
            "         340       0.00      0.00      0.00         5\n",
            "         341       0.00      0.00      0.00         5\n",
            "         342       0.00      0.00      0.00         5\n",
            "         343       0.00      0.00      0.00         5\n",
            "         344       0.00      0.00      0.00         5\n",
            "         345       0.00      0.00      0.00         5\n",
            "         346       0.00      0.00      0.00         5\n",
            "         347       0.00      0.00      0.00         5\n",
            "         348       0.00      0.00      0.00         5\n",
            "         349       0.00      0.00      0.00         5\n",
            "         350       0.00      0.00      0.00         5\n",
            "         351       0.00      0.00      0.00         5\n",
            "         352       0.00      0.00      0.00         5\n",
            "         353       0.00      0.00      0.00         5\n",
            "         354       0.00      0.00      0.00         5\n",
            "         355       0.00      0.00      0.00         5\n",
            "         356       0.00      0.00      0.00         5\n",
            "         357       0.00      0.00      0.00         5\n",
            "         358       0.00      0.00      0.00         5\n",
            "         359       0.00      0.00      0.00         5\n",
            "         360       0.00      0.00      0.00         5\n",
            "         361       0.00      0.00      0.00         5\n",
            "         362       0.00      0.00      0.00         5\n",
            "         363       0.00      0.00      0.00         5\n",
            "         364       0.00      0.00      0.00         5\n",
            "         365       0.00      0.00      0.00         5\n",
            "         366       0.00      0.00      0.00         5\n",
            "         367       0.00      0.00      0.00         5\n",
            "         368       0.00      0.00      0.00         5\n",
            "         369       0.00      0.00      0.00         5\n",
            "         370       0.00      0.00      0.00         5\n",
            "         371       0.00      0.00      0.00         5\n",
            "         372       0.00      0.00      0.00         5\n",
            "         373       0.00      0.00      0.00         5\n",
            "         374       0.00      0.00      0.00         5\n",
            "         375       0.00      0.00      0.00         5\n",
            "         376       0.00      0.00      0.00         5\n",
            "         377       0.00      0.00      0.00         5\n",
            "         378       0.00      0.00      0.00         5\n",
            "         379       0.00      0.00      0.00         5\n",
            "         380       0.00      0.00      0.00         5\n",
            "         381       0.00      0.00      0.00         5\n",
            "         382       0.00      0.00      0.00         5\n",
            "         383       0.00      0.00      0.00         5\n",
            "         384       0.00      0.00      0.00         5\n",
            "         385       0.00      0.00      0.00         5\n",
            "         386       0.00      0.00      0.00         5\n",
            "         387       0.00      0.00      0.00         5\n",
            "         388       0.00      0.00      0.00         5\n",
            "         389       0.00      0.00      0.00         5\n",
            "         390       0.00      0.00      0.00         5\n",
            "         391       0.00      0.00      0.00         5\n",
            "         392       0.00      0.00      0.00         5\n",
            "         393       0.00      0.00      0.00         5\n",
            "         394       0.00      0.00      0.00         5\n",
            "         395       0.00      0.00      0.00         5\n",
            "         396       0.00      0.00      0.00         5\n",
            "         397       0.00      0.00      0.00         5\n",
            "         398       0.00      0.00      0.00         5\n",
            "         399       0.00      0.00      0.00         5\n",
            "         400       0.00      0.00      0.00         5\n",
            "         401       0.00      0.00      0.00         5\n",
            "         402       0.00      0.00      0.00         5\n",
            "         403       0.00      0.00      0.00         5\n",
            "         404       0.00      0.00      0.00         5\n",
            "         405       0.00      0.00      0.00         5\n",
            "         406       0.00      0.00      0.00         5\n",
            "         407       0.00      0.00      0.00         5\n",
            "         408       0.00      0.00      0.00         5\n",
            "         409       0.00      0.00      0.00         5\n",
            "         410       0.00      0.00      0.00         5\n",
            "         411       0.00      0.00      0.00         5\n",
            "         412       0.00      0.00      0.00         5\n",
            "         413       0.00      0.00      0.00         5\n",
            "         414       0.00      0.00      0.00         5\n",
            "         415       0.00      0.00      0.00         5\n",
            "         416       0.00      0.00      0.00         5\n",
            "         417       0.00      0.00      0.00         5\n",
            "         418       0.00      0.00      0.00         5\n",
            "         419       0.00      0.00      0.00         5\n",
            "         420       0.00      0.00      0.00         5\n",
            "         421       0.00      0.00      0.00         5\n",
            "         422       0.00      0.00      0.00         5\n",
            "         423       0.00      0.00      0.00         5\n",
            "         424       0.00      0.00      0.00         5\n",
            "         425       0.00      0.00      0.00         5\n",
            "         426       0.00      0.00      0.00         5\n",
            "         427       0.00      0.00      0.00         5\n",
            "         428       0.00      0.00      0.00         5\n",
            "         429       0.00      0.00      0.00         5\n",
            "         430       0.00      0.00      0.00         5\n",
            "         431       0.00      0.00      0.00         5\n",
            "         432       0.00      0.00      0.00         5\n",
            "         433       0.00      0.00      0.00         5\n",
            "         434       0.00      0.00      0.00         5\n",
            "         435       0.00      0.00      0.00         5\n",
            "         436       0.00      0.00      0.00         5\n",
            "         437       0.00      0.00      0.00         5\n",
            "         438       0.00      0.00      0.00         5\n",
            "         439       0.00      0.00      0.00         5\n",
            "         440       0.00      0.00      0.00         5\n",
            "         441       0.00      0.00      0.00         5\n",
            "         442       0.00      0.00      0.00         5\n",
            "         443       0.00      0.00      0.00         5\n",
            "         444       0.00      0.00      0.00         5\n",
            "         445       0.00      0.00      0.00         5\n",
            "         446       0.00      0.00      0.00         5\n",
            "         447       0.00      0.00      0.00         5\n",
            "         448       0.00      0.00      0.00         5\n",
            "         449       0.00      0.00      0.00         5\n",
            "         450       0.00      0.00      0.00         5\n",
            "         451       0.00      0.00      0.00         5\n",
            "         452       0.00      0.00      0.00         5\n",
            "         453       0.00      0.00      0.00         5\n",
            "         454       0.00      0.00      0.00         5\n",
            "         455       0.00      0.00      0.00         5\n",
            "         456       0.00      0.00      0.00         5\n",
            "         457       0.00      0.00      0.00         5\n",
            "         458       0.00      0.00      0.00         5\n",
            "         459       0.00      0.00      0.00         5\n",
            "         460       0.00      0.00      0.00         5\n",
            "         461       0.00      0.00      0.00         5\n",
            "         462       0.00      0.00      0.00         5\n",
            "         463       0.00      0.00      0.00         5\n",
            "         464       0.00      0.00      0.00         5\n",
            "         465       0.00      0.00      0.00         5\n",
            "         466       0.00      0.00      0.00         5\n",
            "         467       0.00      0.00      0.00         5\n",
            "         468       0.00      0.00      0.00         5\n",
            "         469       0.00      0.00      0.00         5\n",
            "         470       0.00      0.00      0.00         5\n",
            "         471       0.00      0.00      0.00         5\n",
            "         472       0.00      0.00      0.00         5\n",
            "         473       0.00      0.00      0.00         5\n",
            "         474       0.00      0.00      0.00         5\n",
            "         475       0.00      0.00      0.00         5\n",
            "         476       0.00      0.00      0.00         5\n",
            "         477       0.00      0.00      0.00         5\n",
            "         478       0.00      0.00      0.00         5\n",
            "         479       0.00      0.00      0.00         5\n",
            "         480       0.00      0.00      0.00         5\n",
            "         481       0.00      0.00      0.00         5\n",
            "         482       0.00      0.00      0.00         5\n",
            "         483       0.00      0.00      0.00         5\n",
            "         484       0.00      0.00      0.00         5\n",
            "         485       0.00      0.00      0.00         5\n",
            "         486       0.00      0.00      0.00         5\n",
            "         487       0.00      0.00      0.00         5\n",
            "         488       0.00      0.00      0.00         5\n",
            "         489       0.00      0.00      0.00         5\n",
            "         490       0.00      0.00      0.00         5\n",
            "         491       0.00      0.00      0.00         5\n",
            "         492       0.00      0.00      0.00         5\n",
            "         493       0.00      0.00      0.00         5\n",
            "         494       0.00      0.00      0.00         5\n",
            "         495       0.00      0.00      0.00         5\n",
            "         496       0.25      0.20      0.22         5\n",
            "         497       0.00      0.00      0.00         5\n",
            "         498       0.00      0.00      0.00         5\n",
            "         499       0.00      0.00      0.00         5\n",
            "         500       0.00      0.00      0.00         5\n",
            "         501       0.00      0.00      0.00         5\n",
            "         502       0.00      0.00      0.00         5\n",
            "         503       0.00      0.00      0.00         5\n",
            "         504       0.00      0.00      0.00         5\n",
            "         505       0.00      0.00      0.00         5\n",
            "         506       0.00      0.00      0.00         5\n",
            "         507       0.00      0.00      0.00         5\n",
            "         508       0.00      0.00      0.00         5\n",
            "         509       0.00      0.00      0.00         5\n",
            "         510       0.00      0.00      0.00         5\n",
            "         511       0.00      0.00      0.00         5\n",
            "         512       0.00      0.00      0.00         5\n",
            "         513       0.00      0.00      0.00         5\n",
            "         514       0.00      0.00      0.00         5\n",
            "         515       0.00      0.00      0.00         5\n",
            "         516       0.00      0.00      0.00         5\n",
            "         517       0.00      0.00      0.00         5\n",
            "         518       0.00      0.00      0.00         5\n",
            "         519       0.00      0.00      0.00         5\n",
            "         520       0.00      0.00      0.00         5\n",
            "         521       0.00      0.00      0.00         5\n",
            "         522       0.00      0.00      0.00         5\n",
            "         523       0.00      0.00      0.00         5\n",
            "         524       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.00      2625\n",
            "   macro avg       0.00      0.00      0.00      2625\n",
            "weighted avg       0.00      0.00      0.00      2625\n",
            "\n",
            "Confusion Matrix:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "for model_name, model in models.items():\n",
        "\n",
        "    # Generate predictions\n",
        "    y_prob = model.predict(test_data)\n",
        "    y_pred = np.argmax(y_prob, axis=1)\n",
        "\n",
        "    # Extract true labels\n",
        "    y_true = []\n",
        "\n",
        "    test_labels = []\n",
        "\n",
        "    for i in range(len(test_data)):\n",
        "        test_labels.extend(test_data[i][1])\n",
        "\n",
        "    # Convert one-hot encoded labels to class indices\n",
        "    y_true = np.argmax(test_labels, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    report = classification_report(y_true, y_pred)\n",
        "    matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f'{model_name} Metrics:')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'F1 Score: {f1}')\n",
        "    print('Classification Report:')\n",
        "    print(report)\n",
        "    print('Confusion Matrix:')\n",
        "    print(matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "5JQKa44mmHtX",
        "outputId": "133501c2-2d80-459b-da05-17a123d79228"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"model_history\",\n  \"rows\": 0,\n  \"fields\": []\n}",
              "type": "dataframe",
              "variable_name": "model_history"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-bfd5fcf5-dec6-4e9d-9dd8-7c34e50866b2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bfd5fcf5-dec6-4e9d-9dd8-7c34e50866b2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bfd5fcf5-dec6-4e9d-9dd8-7c34e50866b2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bfd5fcf5-dec6-4e9d-9dd8-7c34e50866b2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">Time series</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ],
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x7e8357d49270>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-4ee8ba8b-4e92-49bc-8f93-84f9bd1ee01d\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA/MAAAITCAYAAABLz0yVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAjhElEQVR4nO3df6yW9X3/8dfxnORYOwUMKCRwoHA4tvUXghB+dDPaNDPGX5O1\n",
              "ug0XUvmhW4wZ3UQy2cLqxE2lWmniwRlWR8LMwFqyJkvUbo5WNmSr7cRZOJTDObYgXa0c1inlcK7v\n",
              "H6bn21N+eI7enOMHH4/kSrnv63PO/b6ST0589r7PdeqqqqoCAAAAFOO0oR4AAAAAGBgxDwAAAIUR\n",
              "8wAAAFAYMQ8AAACFEfMAAABQGDEPAAAAhRHzAAAAUBgxDwAAAIUR80kefvjhoR4BAAAA+k3MJ9mz\n",
              "Z89QjwAAAAD9JuYBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIe\n",
              "AAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj\n",
              "5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACg\n",
              "MGIeAAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAA\n",
              "AAoj5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YB\n",
              "AACgMGIeAAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBi\n",
              "HgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAK\n",
              "I+YBAACgMB+ImN+5c2dmz56dlpaWTJ8+Pdu3bz/muscffzyTJ0/OpEmTsnDhwhw+fLjP+aqqcsUV\n",
              "V2T48OGDMDUAAAAMjQ9EzC9evDiLFi3Kjh07snTp0syfP/+oNbt3787y5cuzefPmtLW15fXXX8+a\n",
              "NWv6rPnSl76USZMmDdLUAAAAMDSGPOb379+fbdu2Zd68eUmSuXPnprOzM21tbX3WbdiwIddee21G\n",
              "jx6durq63HrrrVm/fn3v+e3bt+fpp5/OXXfddcLXO3ToULq6uvocR44cqf2FAQAAwEky5DHf2dmZ\n",
              "MWPGpKGhIUlSV1eXpqamdHR09FnX0dGR8ePH9z6eMGFC75rDhw9n4cKFaW1tTX19/Qlfb+XKlRk2\n",
              "bFifY+vWrTW+KgAAADh5hjzma2HFihW54YYb8olPfOJd1y5btiwHDhzoc8yYMWMQpgQAAIDaaBjq\n",
              "AcaNG5e9e/emu7s7DQ0NqaoqHR0daWpq6rOuqakpu3bt6n3c3t7eu+b5559PR0dHVq9ene7u7nR1\n",
              "dWXChAl58cUXM2rUqD7fp7GxMY2NjX2ee7d38wEAAOCDZMjfmT/nnHMyderUrFu3LkmycePGjB07\n",
              "Ns3NzX3WzZ07N5s2bcq+fftSVVUeffTR3HTTTUmSzZs3Z8+ePWlvb8+3vvWtnHXWWWlvbz8q5AEA\n",
              "AOBUMOQxnyStra1pbW1NS0tL7rvvvqxduzZJsmDBgmzatClJMnHixKxYsSJz5sxJc3NzRo0alcWL\n",
              "Fw/l2AAAADAk6qqqqoZ6iKG2ZMmSrFq1aqjHAAAAgH75QLwzDwAAAPSfmAcAAIDCiHkAAAAojJgH\n",
              "AACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMKI\n",
              "eQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDCiHkAAAAo\n",
              "jJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAAKIyYBwAA\n",
              "gMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDCiHkA\n",
              "AAAojJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAAKIyY\n",
              "BwAAgMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDC\n",
              "iHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAA\n",
              "KIyYBwAAgMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcA\n",
              "AIDCiHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMJ8IGJ+586dmT17dlpaWjJ9+vRs3779mOsef/zx\n",
              "TJ48OZMmTcrChQtz+PDhJMk3v/nNzJgxI5/85Cdz/vnn584770xPT89gXgIAAAAMmg9EzC9evDiL\n",
              "Fi3Kjh07snTp0syfP/+oNbt3787y5cuzefPmtLW15fXXX8+aNWuSJCNGjMjf//3f55VXXsl//Md/\n",
              "5IUXXsgTTzwxyFcBAAAAg2PIY37//v3Ztm1b5s2blySZO3duOjs709bW1mfdhg0bcu2112b06NGp\n",
              "q6vLrbfemvXr1ydJLrnkkkycODFJcvrpp2fKlClpb28/5usdOnQoXV1dfY4jR46cvAsEAACAGhvy\n",
              "mO/s7MyYMWPS0NCQJKmrq0tTU1M6Ojr6rOvo6Mj48eN7H0+YMOGoNUmyb9++bNiwIVdfffUxX2/l\n",
              "ypUZNmxYn2Pr1q01vCIAAAA4uYY85mupq6sr11xzTe68885ceumlx1yzbNmyHDhwoM8xY8aMQZ4U\n",
              "AAAA3ruGoR5g3Lhx2bt3b7q7u9PQ0JCqqtLR0ZGmpqY+65qamrJr167ex+3t7X3WHDx4MFdeeWWu\n",
              "u+66LFmy5Liv19jYmMbGxj7P1dfX1+hqAAAA4OQb8nfmzznnnEydOjXr1q1LkmzcuDFjx45Nc3Nz\n",
              "n3Vz587Npk2bsm/fvlRVlUcffTQ33XRTkuR///d/c+WVV+bKK6/M3XffPejXAAAAAINpyGM+SVpb\n",
              "W9Pa2pqWlpbcd999Wbt2bZJkwYIF2bRpU5Jk4sSJWbFiRebMmZPm5uaMGjUqixcvTpI8/PDD2bp1\n",
              "a5566qlMmTIlU6ZMyV/+5V8O2fUAAADAyVRXVVU11EMMtSVLlmTVqlVDPQYAAAD0ywfinXkAAACg\n",
              "/8Q8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbMAwAAQGHEPAAA\n",
              "ABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBhxDwAAAAURswD\n",
              "AABAYcQ8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbMAwAAQGHE\n",
              "PAAAABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBhxDwAAAAU\n",
              "RswDAABAYcQ8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbMAwAA\n",
              "QGHEPAAAABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBhxDwA\n",
              "AAAURswDAABAYcQ8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbM\n",
              "AwAAQGHEPAAAABSm4b180SuvvJLXXnstH/nIR3LhhRdm+PDhNR4LAAAAOJ5+x/zBgwfzwAMP5LHH\n",
              "HssZZ5yRc889N2+//XZ27dqVGTNmZOnSpfn0pz99MmcFAAAAMoCYv/zyy/P7v//7+c53vpNzzz23\n",
              "9/menp5s3rw5jz76aNra2rJ48eKTMigAAADwjn7H/Le//e00NjYe9fxpp52Wyy67LJdddlkOHTpU\n",
              "0+EAAACAo/X7BnjHCvn3sgYAAAB4f/od81dffXVeeOGFY55788038+CDD6a1tbVmgwEAAADH1u+P\n",
              "2d97771ZtmxZXnnllUyfPr33BnivvvpqfvjDH+YP//AP8/nPf/5kzgoAAABkADF/0UUX5Rvf+EY6\n",
              "Ozvz/PPP57XXXssZZ5yR3/u938unPvWpNDS8p79yBwAAAAzQgAv8pz/9aebNm9fnue9973u56KKL\n",
              "ajYUAAAAcHz9/p35X5g/f36/ngMAAABOjn6/M79///7s27cvb731Vv7rv/4rVVUlSQ4cOJCf/exn\n",
              "J21AAAAAoK9+x/z69evz0EMP5Uc/+lGuvfba3ueHDRuWO++886QMBwAAAByt3zF/xx135I477sgX\n",
              "v/jFLF++/GTOBAAAAJzAgG+At3z58vT09GTfvn3p7u7ufb6pqammgwEAAADHNuAb4H31q1/N8OHD\n",
              "c+GFF2batGmZNm1aLr300vc1xM6dOzN79uy0tLRk+vTp2b59+zHXPf7445k8eXImTZqUhQsX5vDh\n",
              "w/06BwAAAKeSAcf8X/zFX+TFF1/MT37yk/z4xz/Oj3/84+zfv/99DbF48eIsWrQoO3bsyNKlS495\n",
              "d/zdu3dn+fLl2bx5c9ra2vL6669nzZo173oOAAAATjUDjvmRI0fmvPPOq9kA+/fvz7Zt23r/dv3c\n",
              "uXPT2dmZtra2Pus2bNiQa6+9NqNHj05dXV1uvfXWrF+//l3PAQAAwKlmwDF//fXX56GHHsr+/fvT\n",
              "1dXVe7xXnZ2dGTNmTBoa3vn1/bq6ujQ1NaWjo6PPuo6OjowfP7738YQJE3rXnOjcrzp06FCfubu6\n",
              "unLkyJH3PD8AAAAMtgHH/J/+6Z9myZIlGT16dEaMGJHhw4dnxIgRJ2O2k2LlypUZNmxYn2Pr1q1D\n",
              "PRYAAAD024Bjvqenp/c4cuRI7/++V+PGjcvevXt774xfVVU6OjqOujt+U1NT9uzZ0/u4vb29d82J\n",
              "zv2qZcuW5cCBA32OGTNmvOf5AQAAYLANOOZr7ZxzzsnUqVOzbt26JMnGjRszduzYNDc391k3d+7c\n",
              "bNq0Kfv27UtVVXn00Udz0003veu5X9XY2Jizzjqrz1FfX39yLxIAAABqaMAxf9ppp6W+vv6o4/1o\n",
              "bW1Na2trWlpact9992Xt2rVJkgULFmTTpk1JkokTJ2bFihWZM2dOmpubM2rUqCxevPhdzwEAAMCp\n",
              "pq6qqmogX/Czn/2s999vvfVWnnjiiRw5ciR/8id/UvPhBsuSJUuyatWqoR4DAAAA+mXA78x/9KMf\n",
              "7T1GjhyZJUuWZMOGDSdjNgAAAOAY3vfvzL/66qv5n//5n1rMAgAAAPRDw0C/YMSIEamrq0uS3jvQ\n",
              "P/LII7WdCgAAADiuAcf8Sy+99P+/uKEho0ePdjd4AAAAGEQDjvnx48fn//7v/3qjfsSIETnjjDNq\n",
              "PRcAAABwHAOO+RdeeCFz587N6NGjkySvv/56Nm7cmFmzZtV8OAAAAOBoA475X9y9fs6cOUneifs/\n",
              "+qM/yr/927/VfDgAAADgaAO+m/1bb73VG/JJMnv27Lz99ts1HQoAAAA4vgHH/K/92q/l2Wef7X38\n",
              "3HPP5aMf/WhNhwIAAACOb8Afs//yl7+cG264ofcO9j09PXnqqadqPhgAAABwbAOO+R/96EfZtm1b\n",
              "Xn/99STJueeem61bt9Z8MAAAAODYBvwx++XLl2fUqFG54IILcsEFF2TkyJFZvnz5yZgNAAAAOIYB\n",
              "x/yvqqury5EjR2oxCwAAANAPA475M888My+88ELv429/+9s588wzazoUAAAAcHwD/p35v/7rv85v\n",
              "/dZv5eMf/3iSZOfOnfna175W88EAAACAYxtwzM+aNSv//d//nS1btiR55+/MDx8+vNZzAQAAAMcx\n",
              "4JhPkhEjRuSqq66q9SwAAABAP7zvG+ABAAAAg0vMAwAAQGHEPAAAABRGzAMAAEBhxDwAAAAURswD\n",
              "AABAYcQ8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbMAwAAQGHE\n",
              "PAAAABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBhxDwAAAAU\n",
              "RswDAABAYcQ8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbMAwAA\n",
              "QGHEPAAAABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBhxDwA\n",
              "AAAURswDAABAYcQ8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbM\n",
              "AwAAQGHEPAAAABRGzAMAAEBhxDwAAAAURswDAABAYcQ8AAAAFEbMAwAAQGHEPAAAABRGzAMAAEBh\n",
              "xDwAAAAURswDAABAYcQ8AAAAFEbMAwAAQGGGNOZ7enpy++23Z9KkSWlubs7q1auPu3bnzp2ZPXt2\n",
              "WlpaMn369Gzfvj1J8vbbb+f6669PS0tLLr744nzmM59JW1vbYF0CAAAADLohjfl169bllVdeyY4d\n",
              "O7J169bcf//9vZH+qxYvXpxFixZlx44dWbp0aebPn997btGiRfn+97+f7373u7nuuuuyYMGCQboC\n",
              "AAAAGHxDGvNPPvlkFi5cmPr6+px99tm58cYbs379+qPW7d+/P9u2bcu8efOSJHPnzk1nZ2fa2tpy\n",
              "+umn56qrrkpdXV2SZObMmWlvbz/uax46dChdXV19jiNHjpyU6wMAAICTYUhjvqOjI+PHj+99PGHC\n",
              "hHR0dBy1rrOzM2PGjElDQ0OSpK6uLk1NTcdc+/DDD+e666477muuXLkyw4YN63Ns3bq1BlcDAAAA\n",
              "g+OkxvysWbMycuTIYx6dnZ01f7177703bW1tWbly5XHXLFu2LAcOHOhzzJgxo+azAAAAwMnScDK/\n",
              "+ZYtW054vqmpKXv27MmsWbOSJO3t7Wlqajpq3bhx47J37950d3enoaEhVVWlo6Ojz9oHHnggTz31\n",
              "VJ599tmcccYZx33NxsbGNDY29nmuvr5+IJcFAAAAQ2pIP2b/2c9+No899liOHDmSN954I08++WRu\n",
              "vPHGo9adc845mTp1atatW5ck2bhxY8aOHZvm5uYkyapVq7J+/fo888wzGT58+GBeAgAAAAy6k/rO\n",
              "/Lu5+eab8+KLL2by5Mmpq6vLkiVLcuGFFyZJNm3alE2bNuVv/uZvkiStra2ZP39+7r333px11llZ\n",
              "u3ZtkuS1117LF77whUycODGXX355knfeff/3f//3obkoAAAAOMnqqqqqhnqIobZkyZKsWrVqqMcA\n",
              "AACAfhnSj9kDAAAAAyfmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAA\n",
              "oDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4A\n",
              "AAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPm\n",
              "AQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj5gEAAKAw\n",
              "Yh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAA\n",
              "CiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj5gEA\n",
              "AKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIe\n",
              "AAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj\n",
              "5gEAAKAwYh4AAAAKI+YBAACgMGIeAAAACiPmAQAAoDBiHgAAAAoj5gEAAKAwYh4AAAAKI+YBAACg\n",
              "MEMa8z09Pbn99tszadKkNDc3Z/Xq1cddu3PnzsyePTstLS2ZPn16tm/fftSatWvXpq6uLk8//fRJ\n",
              "nBoAAACG1pDG/Lp16/LKK69kx44d2bp1a+6///5jRnqSLF68OIsWLcqOHTuydOnSzJ8/v8/59vb2\n",
              "PPbYY5k5c+YgTA4AAABDZ0hj/sknn8zChQtTX1+fs88+OzfeeGPWr19/1Lr9+/dn27ZtmTdvXpJk\n",
              "7ty56ezsTFtbW5J33uFfsGBBHnnkkTQ2Np7wNQ8dOpSurq4+x5EjR2p/cQAAAHCSDGnMd3R0ZPz4\n",
              "8b2PJ0yYkI6OjqPWdXZ2ZsyYMWloaEiS1NXVpampqXftqlWrMmfOnEybNu1dX3PlypUZNmxYn2Pr\n",
              "1q01uiIAAAA4+U5qzM+aNSsjR4485tHZ2VmT13j55ZezcePG3H333f1av2zZshw4cKDPMWPGjJrM\n",
              "AgAAAIOh4WR+8y1btpzwfFNTU/bs2ZNZs2Yleef33puamo5aN27cuOzduzfd3d1paGhIVVXp6OhI\n",
              "U1NTnnnmmbS3t2fy5MlJkn379mXRokXZu3dvbrvttqO+V2Nj41Efxa+vr3+vlwgAAACDbkg/Zv/Z\n",
              "z342jz32WI4cOZI33ngjTz75ZG688caj1p1zzjmZOnVq1q1blyTZuHFjxo4dm+bm5tx2223Zu3dv\n",
              "2tvb097enpkzZ2bNmjXHDHkAAAA4FQxpzN988835+Mc/nsmTJ2f69OlZsmRJLrzwwiTJpk2bsmDB\n",
              "gt61ra2taW1tTUtLS+67776sXbt2qMYGAACAIVVXVVU11EMMtSVLlmTVqlVDPQYAAAD0y5C+Mw8A\n",
              "AAAMnJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAAKIyY\n",
              "BwAAgMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDC\n",
              "iHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAA\n",
              "KIyYBwAAgMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcA\n",
              "AIDCiHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5\n",
              "AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDCiHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiM\n",
              "mAcAAIDCiHkAAAAojJgHAACAwoh5AAAAKIyYBwAAgMKIeQAAACiMmAcAAIDC1FVVVQ31EEPthhtu\n",
              "yIQJE4Z6DIbAkSNHsnXr1syYMSP19fVDPQ4ck31KCexTSmGvUgL7lPHjx+eOO+444Roxz4daV1dX\n",
              "hg0blgMHDuSss84a6nHgmOxTSmCfUgp7lRLYp/SHj9kDAABAYcQ8AAAAFEbMAwAAQGHEPB9qjY2N\n",
              "+fM///M0NjYO9ShwXPYpJbBPKYW9SgnsU/rDDfAAAACgMN6ZBwAAgMKIeQAAACiMmAcAAIDCiHlO\n",
              "eT09Pbn99tszadKkNDc3Z/Xq1cddu3PnzsyePTstLS2ZPn16tm/fftSatWvXpq6uLk8//fRJnJoP\n",
              "m1rs07fffjvXX399WlpacvHFF+czn/lM2traBusSOIX152djkjz++OOZPHlyJk2alIULF+bw4cP9\n",
              "Oge18H736Te/+c3MmDEjn/zkJ3P++efnzjvvTE9Pz2BeAh8Ctfh5miRVVeWKK67I8OHDB2FqPrAq\n",
              "OMV99atfra644oqqu7u7+slPflI1NTVVL7/88jHXXn755dXatWurqqqqf/iHf6guvfTSPud3795d\n",
              "zZo1q5o5c2b1ta997SRPzodJLfbpW2+9VX3jG9+oenp6qqqqqkceeaS67LLLBmN8TnHv9rOxqqrq\n",
              "Bz/4QTVmzJhq7969VU9PT3XNNddUq1evftdzUCvvd5/+53/+Z7Vr166qqt75eTpnzpze7we18n73\n",
              "6S88+OCD1YIFC6phw4YNwtR8UHlnnlPek08+mYULF6a+vj5nn312brzxxqxfv/6odfv378+2bdsy\n",
              "b968JMncuXPT2dnZ+85mT09PFixYkEceecSfCaHmarFPTz/99Fx11VWpq6tLksycOTPt7e2DeRmc\n",
              "gt7tZ+MvbNiwIddee21Gjx6durq63Hrrrb17+ETnoBZqsU8vueSSTJw4MUly+umnZ8qUKX6GUlO1\n",
              "2KdJsn379jz99NO56667BnV+PnjEPKe8jo6OjB8/vvfxhAkT0tHRcdS6zs7OjBkzJg0NDUmSurq6\n",
              "NDU19a5dtWpV5syZk2nTpg3O4Hyo1Gqf/rKHH34411133ckbmg+F/u65E+3h/u5veK9qsU9/2b59\n",
              "+7Jhw4ZcffXVJ3dwPlRqsU8PHz6chQsXprW1NfX19YM3PB9IDUM9ALxfs2bNys6dO4957jvf+U5N\n",
              "XuPll1/Oxo0b86//+q81+X58+AzGPv1l9957b9ra2vLcc8/V/HsDnMq6urpyzTXX5M4778yll146\n",
              "1ONAHytWrMgNN9yQT3ziEz45gpinfFu2bDnh+aampuzZsyezZs1KkrS3t6epqemodePGjcvevXvT\n",
              "3d2dhoaGVFWVjo6ONDU15Zlnnkl7e3smT56c5J3/x37RokXZu3dvbrvtttpfFKecwdinv/DAAw/k\n",
              "qaeeyrPPPpszzjijthfCh05/9lzyzh7etWtX7+Nf3sMnOge1UIt9miQHDx7MlVdemeuuuy5LliwZ\n",
              "tPn5cKjFPn3++efT0dGR1atXp7u7O11dXZkwYUJefPHFjBo1alCvhw+AIf2NfRgEa9euPerGYt/7\n",
              "3veOufayyy7rc1OSadOmHXedG+BRS7Xapw8++GA1derU6o033hiMsfmQ6M/Pxl27dh11w6ZHHnnk\n",
              "Xc9BrbzffXrw4MFq9uzZ1YoVKwZzbD5k3u8+/WW7d+92A7wPOTHPKa+7u7v6gz/4g+pjH/tYNXHi\n",
              "xOqhhx7qPff1r3+9uuWWW3ofv/rqq9XMmTOryZMnV9OmTTthTIl5aqkW+7Szs7NKUk2cOLG6+OKL\n",
              "q4svvriaMWPGoF8Lp57j7blbbrml+vrXv967bs2aNdXEiROriRMnVp///Oern//85/06B7Xwfvfp\n",
              "PffcUzU0NPT+/Lz44oure+65Z0iuhVNXLX6e/oKYp66qqmqoPx0AAAAA9J+72QMAAEBhxDwAAAAU\n",
              "RswDAABAYcQ8ANDHlClTcvDgwQF/3W//9m/nb//2b2s/EABwFH9nHgDo46WXXhrqEQCAd+GdeQCg\n",
              "j7q6urz55ptJkgkTJuTP/uzPMmvWrHzsYx/LPffc07vu1VdfzezZs3P++efn+uuvT1dXV++5gwcP\n",
              "ZuHChZkxY0YuuuiiLFq0KD//+c/z/e9/P2PHjs0PfvCDJMkDDzyQK6+8Mj09PYN6jQBQOjEPAJzQ\n",
              "m2++mS1btuTFF1/M/fffnx/+8IdJkptvvjm33HJLtm/fni9+8Yt5/vnne7/mC1/4Qn791389W7du\n",
              "zXe/+9309PTk4YcfznnnnZf7778/n/vc5/Iv//Iv+cpXvpK/+7u/y2mn+U8SABgIH7MHAE7od3/3\n",
              "d5MkI0eOzMSJE7N79+6ceeaZeemllzJ//vwkyYUXXphPfepTvV/z9NNPZ8uWLVm1alWS5K233kp9\n",
              "fX2S5Hd+53fyz//8z/nN3/zNPPfccxk1atTgXhAAnALEPABwQqeffnrvv+vr69Pd3X3MdXV1db3/\n",
              "rqoqGzduTEtLy1Hruru78/LLL+fss8/ufZcfABgYn2kDAAbsrLPOyiWXXJInnngiSbJ9+/Z861vf\n",
              "6j1//fXX56/+6q96w/+nP/1p2trakiR33XVXzjvvvGzevDl//Md/3Ps8ANB/Yh4AeE+eeOKJrFmz\n",
              "JhdccEHuvvvu/MZv/EbvuS996Uv5yEc+kilTpuSiiy7Kpz/96bS3t+cf//Ef80//9E/5yle+kubm\n",
              "5qxatSqf+9zn8vbbbw/hlQBAeeqqqqqGeggAAACg/7wzDwAAAIUR8wAAAFAYMQ8AAACFEfMAAABQ\n",
              "GDEPAAAAhRHzAAAAUBgxDwAAAIUR8wAAAFAYMQ8AAACFEfMAAABQmP8HVZ1NHWMT7VYAAAAASUVO\n",
              "RK5CYII=\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-4ee8ba8b-4e92-49bc-8f93-84f9bd1ee01d\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-4ee8ba8b-4e92-49bc-8f93-84f9bd1ee01d\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ],
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "def _plot_series(series, series_name, series_index=0):\n",
              "  palette = list(sns.palettes.mpl_palette('Dark2'))\n",
              "  counted = (series['index']\n",
              "                .value_counts()\n",
              "              .reset_index(name='counts')\n",
              "              .rename({'index': 'index'}, axis=1)\n",
              "              .sort_values('index', ascending=True))\n",
              "  xs = counted['index']\n",
              "  ys = counted['counts']\n",
              "  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])\n",
              "\n",
              "fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')\n",
              "df_sorted = _df_0.sort_values('index', ascending=True)\n",
              "_plot_series(df_sorted, '')\n",
              "sns.despine(fig=fig, ax=ax)\n",
              "plt.xlabel('index')\n",
              "_ = plt.ylabel('count()')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "model_history = pd.DataFrame(data = model.history.history)\n",
        "model_history.head(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "WTENFjcgm0i1",
        "outputId": "fc352a46-b82e-4ffa-f08c-782c383a1fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83/83 [==============================] - 6s 73ms/step\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-d1055828e274>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Extract true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Calculate metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-d1055828e274>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Extract true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Calculate metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mfilepaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             img = image_utils.load_img(\n\u001b[0m\u001b[1;32m    371\u001b[0m                 \u001b[0mfilepaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         raise TypeError(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for model_name, model in models.items():\n",
        "    # Train the model and save history\n",
        "    # history = model.fit(train_data, steps_per_epoch=len(train_data),\n",
        "    #                     validation_data=val_data,\n",
        "    #                     validation_steps=len(val_data), epochs=5)\n",
        "\n",
        "    # Generate predictions\n",
        "    y_prob = model.predict(test_data)\n",
        "    y_pred = np.argmax(y_prob, axis=1)\n",
        "\n",
        "    # Extract true labels\n",
        "    y_true = np.concatenate([y for x, y in test_data], axis=0)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    report = classification_report(y_true, y_pred)\n",
        "    matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "  # Plot accuracy and loss curves\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{model_name} Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr_WdsP5nXFl",
        "outputId": "6e459913-9e34-4a3f-c8ad-f7fd1c6edb47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)       [(None, None, None, 3)]      0         []                            \n",
            "                                                                                                  \n",
            " conv2d_940 (Conv2D)         (None, None, None, 32)       864       ['input_11[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_940 (B  (None, None, None, 32)       96        ['conv2d_940[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_940 (Activation  (None, None, None, 32)       0         ['batch_normalization_940[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_941 (Conv2D)         (None, None, None, 32)       9216      ['activation_940[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_941 (B  (None, None, None, 32)       96        ['conv2d_941[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_941 (Activation  (None, None, None, 32)       0         ['batch_normalization_941[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_942 (Conv2D)         (None, None, None, 64)       18432     ['activation_941[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_942 (B  (None, None, None, 64)       192       ['conv2d_942[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_942 (Activation  (None, None, None, 64)       0         ['batch_normalization_942[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " max_pooling2d_40 (MaxPooli  (None, None, None, 64)       0         ['activation_942[0][0]']      \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_943 (Conv2D)         (None, None, None, 80)       5120      ['max_pooling2d_40[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_943 (B  (None, None, None, 80)       240       ['conv2d_943[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_943 (Activation  (None, None, None, 80)       0         ['batch_normalization_943[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_944 (Conv2D)         (None, None, None, 192)      138240    ['activation_943[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_944 (B  (None, None, None, 192)      576       ['conv2d_944[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_944 (Activation  (None, None, None, 192)      0         ['batch_normalization_944[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " max_pooling2d_41 (MaxPooli  (None, None, None, 192)      0         ['activation_944[0][0]']      \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_948 (Conv2D)         (None, None, None, 64)       12288     ['max_pooling2d_41[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_948 (B  (None, None, None, 64)       192       ['conv2d_948[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_948 (Activation  (None, None, None, 64)       0         ['batch_normalization_948[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_946 (Conv2D)         (None, None, None, 48)       9216      ['max_pooling2d_41[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_949 (Conv2D)         (None, None, None, 96)       55296     ['activation_948[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_946 (B  (None, None, None, 48)       144       ['conv2d_946[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_949 (B  (None, None, None, 96)       288       ['conv2d_949[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_946 (Activation  (None, None, None, 48)       0         ['batch_normalization_946[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_949 (Activation  (None, None, None, 96)       0         ['batch_normalization_949[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " average_pooling2d_90 (Aver  (None, None, None, 192)      0         ['max_pooling2d_41[0][0]']    \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_945 (Conv2D)         (None, None, None, 64)       12288     ['max_pooling2d_41[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_947 (Conv2D)         (None, None, None, 64)       76800     ['activation_946[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_950 (Conv2D)         (None, None, None, 96)       82944     ['activation_949[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_951 (Conv2D)         (None, None, None, 32)       6144      ['average_pooling2d_90[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_945 (B  (None, None, None, 64)       192       ['conv2d_945[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_947 (B  (None, None, None, 64)       192       ['conv2d_947[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_950 (B  (None, None, None, 96)       288       ['conv2d_950[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_951 (B  (None, None, None, 32)       96        ['conv2d_951[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_945 (Activation  (None, None, None, 64)       0         ['batch_normalization_945[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_947 (Activation  (None, None, None, 64)       0         ['batch_normalization_947[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_950 (Activation  (None, None, None, 96)       0         ['batch_normalization_950[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_951 (Activation  (None, None, None, 32)       0         ['batch_normalization_951[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " mixed0 (Concatenate)        (None, None, None, 256)      0         ['activation_945[0][0]',      \n",
            "                                                                     'activation_947[0][0]',      \n",
            "                                                                     'activation_950[0][0]',      \n",
            "                                                                     'activation_951[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_955 (Conv2D)         (None, None, None, 64)       16384     ['mixed0[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_955 (B  (None, None, None, 64)       192       ['conv2d_955[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_955 (Activation  (None, None, None, 64)       0         ['batch_normalization_955[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_953 (Conv2D)         (None, None, None, 48)       12288     ['mixed0[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_956 (Conv2D)         (None, None, None, 96)       55296     ['activation_955[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_953 (B  (None, None, None, 48)       144       ['conv2d_953[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_956 (B  (None, None, None, 96)       288       ['conv2d_956[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_953 (Activation  (None, None, None, 48)       0         ['batch_normalization_953[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_956 (Activation  (None, None, None, 96)       0         ['batch_normalization_956[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " average_pooling2d_91 (Aver  (None, None, None, 256)      0         ['mixed0[0][0]']              \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_952 (Conv2D)         (None, None, None, 64)       16384     ['mixed0[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_954 (Conv2D)         (None, None, None, 64)       76800     ['activation_953[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_957 (Conv2D)         (None, None, None, 96)       82944     ['activation_956[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_958 (Conv2D)         (None, None, None, 64)       16384     ['average_pooling2d_91[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_952 (B  (None, None, None, 64)       192       ['conv2d_952[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_954 (B  (None, None, None, 64)       192       ['conv2d_954[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_957 (B  (None, None, None, 96)       288       ['conv2d_957[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_958 (B  (None, None, None, 64)       192       ['conv2d_958[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_952 (Activation  (None, None, None, 64)       0         ['batch_normalization_952[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_954 (Activation  (None, None, None, 64)       0         ['batch_normalization_954[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_957 (Activation  (None, None, None, 96)       0         ['batch_normalization_957[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_958 (Activation  (None, None, None, 64)       0         ['batch_normalization_958[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " mixed1 (Concatenate)        (None, None, None, 288)      0         ['activation_952[0][0]',      \n",
            "                                                                     'activation_954[0][0]',      \n",
            "                                                                     'activation_957[0][0]',      \n",
            "                                                                     'activation_958[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_962 (Conv2D)         (None, None, None, 64)       18432     ['mixed1[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_962 (B  (None, None, None, 64)       192       ['conv2d_962[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_962 (Activation  (None, None, None, 64)       0         ['batch_normalization_962[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_960 (Conv2D)         (None, None, None, 48)       13824     ['mixed1[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_963 (Conv2D)         (None, None, None, 96)       55296     ['activation_962[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_960 (B  (None, None, None, 48)       144       ['conv2d_960[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_963 (B  (None, None, None, 96)       288       ['conv2d_963[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_960 (Activation  (None, None, None, 48)       0         ['batch_normalization_960[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_963 (Activation  (None, None, None, 96)       0         ['batch_normalization_963[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " average_pooling2d_92 (Aver  (None, None, None, 288)      0         ['mixed1[0][0]']              \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_959 (Conv2D)         (None, None, None, 64)       18432     ['mixed1[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_961 (Conv2D)         (None, None, None, 64)       76800     ['activation_960[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_964 (Conv2D)         (None, None, None, 96)       82944     ['activation_963[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_965 (Conv2D)         (None, None, None, 64)       18432     ['average_pooling2d_92[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_959 (B  (None, None, None, 64)       192       ['conv2d_959[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_961 (B  (None, None, None, 64)       192       ['conv2d_961[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_964 (B  (None, None, None, 96)       288       ['conv2d_964[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_965 (B  (None, None, None, 64)       192       ['conv2d_965[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_959 (Activation  (None, None, None, 64)       0         ['batch_normalization_959[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_961 (Activation  (None, None, None, 64)       0         ['batch_normalization_961[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_964 (Activation  (None, None, None, 96)       0         ['batch_normalization_964[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_965 (Activation  (None, None, None, 64)       0         ['batch_normalization_965[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " mixed2 (Concatenate)        (None, None, None, 288)      0         ['activation_959[0][0]',      \n",
            "                                                                     'activation_961[0][0]',      \n",
            "                                                                     'activation_964[0][0]',      \n",
            "                                                                     'activation_965[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_967 (Conv2D)         (None, None, None, 64)       18432     ['mixed2[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_967 (B  (None, None, None, 64)       192       ['conv2d_967[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_967 (Activation  (None, None, None, 64)       0         ['batch_normalization_967[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_968 (Conv2D)         (None, None, None, 96)       55296     ['activation_967[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_968 (B  (None, None, None, 96)       288       ['conv2d_968[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_968 (Activation  (None, None, None, 96)       0         ['batch_normalization_968[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_966 (Conv2D)         (None, None, None, 384)      995328    ['mixed2[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_969 (Conv2D)         (None, None, None, 96)       82944     ['activation_968[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_966 (B  (None, None, None, 384)      1152      ['conv2d_966[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_969 (B  (None, None, None, 96)       288       ['conv2d_969[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_966 (Activation  (None, None, None, 384)      0         ['batch_normalization_966[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_969 (Activation  (None, None, None, 96)       0         ['batch_normalization_969[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " max_pooling2d_42 (MaxPooli  (None, None, None, 288)      0         ['mixed2[0][0]']              \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " mixed3 (Concatenate)        (None, None, None, 768)      0         ['activation_966[0][0]',      \n",
            "                                                                     'activation_969[0][0]',      \n",
            "                                                                     'max_pooling2d_42[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_974 (Conv2D)         (None, None, None, 128)      98304     ['mixed3[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_974 (B  (None, None, None, 128)      384       ['conv2d_974[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_974 (Activation  (None, None, None, 128)      0         ['batch_normalization_974[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_975 (Conv2D)         (None, None, None, 128)      114688    ['activation_974[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_975 (B  (None, None, None, 128)      384       ['conv2d_975[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_975 (Activation  (None, None, None, 128)      0         ['batch_normalization_975[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_971 (Conv2D)         (None, None, None, 128)      98304     ['mixed3[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_976 (Conv2D)         (None, None, None, 128)      114688    ['activation_975[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_971 (B  (None, None, None, 128)      384       ['conv2d_971[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_976 (B  (None, None, None, 128)      384       ['conv2d_976[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_971 (Activation  (None, None, None, 128)      0         ['batch_normalization_971[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_976 (Activation  (None, None, None, 128)      0         ['batch_normalization_976[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_972 (Conv2D)         (None, None, None, 128)      114688    ['activation_971[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_977 (Conv2D)         (None, None, None, 128)      114688    ['activation_976[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_972 (B  (None, None, None, 128)      384       ['conv2d_972[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_977 (B  (None, None, None, 128)      384       ['conv2d_977[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_972 (Activation  (None, None, None, 128)      0         ['batch_normalization_972[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_977 (Activation  (None, None, None, 128)      0         ['batch_normalization_977[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " average_pooling2d_93 (Aver  (None, None, None, 768)      0         ['mixed3[0][0]']              \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_970 (Conv2D)         (None, None, None, 192)      147456    ['mixed3[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_973 (Conv2D)         (None, None, None, 192)      172032    ['activation_972[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_978 (Conv2D)         (None, None, None, 192)      172032    ['activation_977[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_979 (Conv2D)         (None, None, None, 192)      147456    ['average_pooling2d_93[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_970 (B  (None, None, None, 192)      576       ['conv2d_970[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_973 (B  (None, None, None, 192)      576       ['conv2d_973[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_978 (B  (None, None, None, 192)      576       ['conv2d_978[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_979 (B  (None, None, None, 192)      576       ['conv2d_979[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_970 (Activation  (None, None, None, 192)      0         ['batch_normalization_970[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_973 (Activation  (None, None, None, 192)      0         ['batch_normalization_973[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_978 (Activation  (None, None, None, 192)      0         ['batch_normalization_978[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_979 (Activation  (None, None, None, 192)      0         ['batch_normalization_979[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " mixed4 (Concatenate)        (None, None, None, 768)      0         ['activation_970[0][0]',      \n",
            "                                                                     'activation_973[0][0]',      \n",
            "                                                                     'activation_978[0][0]',      \n",
            "                                                                     'activation_979[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_984 (Conv2D)         (None, None, None, 160)      122880    ['mixed4[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_984 (B  (None, None, None, 160)      480       ['conv2d_984[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_984 (Activation  (None, None, None, 160)      0         ['batch_normalization_984[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_985 (Conv2D)         (None, None, None, 160)      179200    ['activation_984[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_985 (B  (None, None, None, 160)      480       ['conv2d_985[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_985 (Activation  (None, None, None, 160)      0         ['batch_normalization_985[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_981 (Conv2D)         (None, None, None, 160)      122880    ['mixed4[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_986 (Conv2D)         (None, None, None, 160)      179200    ['activation_985[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_981 (B  (None, None, None, 160)      480       ['conv2d_981[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_986 (B  (None, None, None, 160)      480       ['conv2d_986[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_981 (Activation  (None, None, None, 160)      0         ['batch_normalization_981[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_986 (Activation  (None, None, None, 160)      0         ['batch_normalization_986[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_982 (Conv2D)         (None, None, None, 160)      179200    ['activation_981[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_987 (Conv2D)         (None, None, None, 160)      179200    ['activation_986[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_982 (B  (None, None, None, 160)      480       ['conv2d_982[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_987 (B  (None, None, None, 160)      480       ['conv2d_987[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_982 (Activation  (None, None, None, 160)      0         ['batch_normalization_982[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_987 (Activation  (None, None, None, 160)      0         ['batch_normalization_987[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " average_pooling2d_94 (Aver  (None, None, None, 768)      0         ['mixed4[0][0]']              \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_980 (Conv2D)         (None, None, None, 192)      147456    ['mixed4[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_983 (Conv2D)         (None, None, None, 192)      215040    ['activation_982[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_988 (Conv2D)         (None, None, None, 192)      215040    ['activation_987[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_989 (Conv2D)         (None, None, None, 192)      147456    ['average_pooling2d_94[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_980 (B  (None, None, None, 192)      576       ['conv2d_980[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_983 (B  (None, None, None, 192)      576       ['conv2d_983[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_988 (B  (None, None, None, 192)      576       ['conv2d_988[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_989 (B  (None, None, None, 192)      576       ['conv2d_989[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_980 (Activation  (None, None, None, 192)      0         ['batch_normalization_980[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_983 (Activation  (None, None, None, 192)      0         ['batch_normalization_983[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_988 (Activation  (None, None, None, 192)      0         ['batch_normalization_988[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_989 (Activation  (None, None, None, 192)      0         ['batch_normalization_989[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " mixed5 (Concatenate)        (None, None, None, 768)      0         ['activation_980[0][0]',      \n",
            "                                                                     'activation_983[0][0]',      \n",
            "                                                                     'activation_988[0][0]',      \n",
            "                                                                     'activation_989[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_994 (Conv2D)         (None, None, None, 160)      122880    ['mixed5[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_994 (B  (None, None, None, 160)      480       ['conv2d_994[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_994 (Activation  (None, None, None, 160)      0         ['batch_normalization_994[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_995 (Conv2D)         (None, None, None, 160)      179200    ['activation_994[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_995 (B  (None, None, None, 160)      480       ['conv2d_995[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_995 (Activation  (None, None, None, 160)      0         ['batch_normalization_995[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_991 (Conv2D)         (None, None, None, 160)      122880    ['mixed5[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_996 (Conv2D)         (None, None, None, 160)      179200    ['activation_995[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_991 (B  (None, None, None, 160)      480       ['conv2d_991[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_996 (B  (None, None, None, 160)      480       ['conv2d_996[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_991 (Activation  (None, None, None, 160)      0         ['batch_normalization_991[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_996 (Activation  (None, None, None, 160)      0         ['batch_normalization_996[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_992 (Conv2D)         (None, None, None, 160)      179200    ['activation_991[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_997 (Conv2D)         (None, None, None, 160)      179200    ['activation_996[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_992 (B  (None, None, None, 160)      480       ['conv2d_992[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_997 (B  (None, None, None, 160)      480       ['conv2d_997[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_992 (Activation  (None, None, None, 160)      0         ['batch_normalization_992[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_997 (Activation  (None, None, None, 160)      0         ['batch_normalization_997[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " average_pooling2d_95 (Aver  (None, None, None, 768)      0         ['mixed5[0][0]']              \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_990 (Conv2D)         (None, None, None, 192)      147456    ['mixed5[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_993 (Conv2D)         (None, None, None, 192)      215040    ['activation_992[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_998 (Conv2D)         (None, None, None, 192)      215040    ['activation_997[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_999 (Conv2D)         (None, None, None, 192)      147456    ['average_pooling2d_95[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_990 (B  (None, None, None, 192)      576       ['conv2d_990[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_993 (B  (None, None, None, 192)      576       ['conv2d_993[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_998 (B  (None, None, None, 192)      576       ['conv2d_998[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_999 (B  (None, None, None, 192)      576       ['conv2d_999[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_990 (Activation  (None, None, None, 192)      0         ['batch_normalization_990[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_993 (Activation  (None, None, None, 192)      0         ['batch_normalization_993[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_998 (Activation  (None, None, None, 192)      0         ['batch_normalization_998[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " activation_999 (Activation  (None, None, None, 192)      0         ['batch_normalization_999[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " mixed6 (Concatenate)        (None, None, None, 768)      0         ['activation_990[0][0]',      \n",
            "                                                                     'activation_993[0][0]',      \n",
            "                                                                     'activation_998[0][0]',      \n",
            "                                                                     'activation_999[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_1004 (Conv2D)        (None, None, None, 192)      147456    ['mixed6[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_1004 (  (None, None, None, 192)      576       ['conv2d_1004[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1004 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1004[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2d_1005 (Conv2D)        (None, None, None, 192)      258048    ['activation_1004[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_1005 (  (None, None, None, 192)      576       ['conv2d_1005[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1005 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1005[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2d_1001 (Conv2D)        (None, None, None, 192)      147456    ['mixed6[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_1006 (Conv2D)        (None, None, None, 192)      258048    ['activation_1005[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_1001 (  (None, None, None, 192)      576       ['conv2d_1001[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_1006 (  (None, None, None, 192)      576       ['conv2d_1006[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1001 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1001[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " activation_1006 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1006[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2d_1002 (Conv2D)        (None, None, None, 192)      258048    ['activation_1001[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_1007 (Conv2D)        (None, None, None, 192)      258048    ['activation_1006[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_1002 (  (None, None, None, 192)      576       ['conv2d_1002[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_1007 (  (None, None, None, 192)      576       ['conv2d_1007[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1002 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1002[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " activation_1007 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1007[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " average_pooling2d_96 (Aver  (None, None, None, 768)      0         ['mixed6[0][0]']              \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_1000 (Conv2D)        (None, None, None, 192)      147456    ['mixed6[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_1003 (Conv2D)        (None, None, None, 192)      258048    ['activation_1002[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_1008 (Conv2D)        (None, None, None, 192)      258048    ['activation_1007[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_1009 (Conv2D)        (None, None, None, 192)      147456    ['average_pooling2d_96[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_1000 (  (None, None, None, 192)      576       ['conv2d_1000[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_1003 (  (None, None, None, 192)      576       ['conv2d_1003[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_1008 (  (None, None, None, 192)      576       ['conv2d_1008[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_1009 (  (None, None, None, 192)      576       ['conv2d_1009[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1000 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1000[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " activation_1003 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1003[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " activation_1008 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1008[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " activation_1009 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1009[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " mixed7 (Concatenate)        (None, None, None, 768)      0         ['activation_1000[0][0]',     \n",
            "                                                                     'activation_1003[0][0]',     \n",
            "                                                                     'activation_1008[0][0]',     \n",
            "                                                                     'activation_1009[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_1012 (Conv2D)        (None, None, None, 192)      147456    ['mixed7[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_1012 (  (None, None, None, 192)      576       ['conv2d_1012[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1012 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1012[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2d_1013 (Conv2D)        (None, None, None, 192)      258048    ['activation_1012[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_1013 (  (None, None, None, 192)      576       ['conv2d_1013[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1013 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1013[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2d_1010 (Conv2D)        (None, None, None, 192)      147456    ['mixed7[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_1014 (Conv2D)        (None, None, None, 192)      258048    ['activation_1013[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_1010 (  (None, None, None, 192)      576       ['conv2d_1010[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_1014 (  (None, None, None, 192)      576       ['conv2d_1014[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1010 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1010[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " activation_1014 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1014[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2d_1011 (Conv2D)        (None, None, None, 320)      552960    ['activation_1010[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_1015 (Conv2D)        (None, None, None, 192)      331776    ['activation_1014[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_1011 (  (None, None, None, 320)      960       ['conv2d_1011[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_1015 (  (None, None, None, 192)      576       ['conv2d_1015[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1011 (Activatio  (None, None, None, 320)      0         ['batch_normalization_1011[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " activation_1015 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1015[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " max_pooling2d_43 (MaxPooli  (None, None, None, 768)      0         ['mixed7[0][0]']              \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " mixed8 (Concatenate)        (None, None, None, 1280)     0         ['activation_1011[0][0]',     \n",
            "                                                                     'activation_1015[0][0]',     \n",
            "                                                                     'max_pooling2d_43[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1020 (Conv2D)        (None, None, None, 448)      573440    ['mixed8[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_1020 (  (None, None, None, 448)      1344      ['conv2d_1020[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1020 (Activatio  (None, None, None, 448)      0         ['batch_normalization_1020[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2d_1017 (Conv2D)        (None, None, None, 384)      491520    ['mixed8[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_1021 (Conv2D)        (None, None, None, 384)      1548288   ['activation_1020[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_1017 (  (None, None, None, 384)      1152      ['conv2d_1017[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_1021 (  (None, None, None, 384)      1152      ['conv2d_1021[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1017 (Activatio  (None, None, None, 384)      0         ['batch_normalization_1017[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " activation_1021 (Activatio  (None, None, None, 384)      0         ['batch_normalization_1021[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2d_1018 (Conv2D)        (None, None, None, 384)      442368    ['activation_1017[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_1019 (Conv2D)        (None, None, None, 384)      442368    ['activation_1017[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_1022 (Conv2D)        (None, None, None, 384)      442368    ['activation_1021[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_1023 (Conv2D)        (None, None, None, 384)      442368    ['activation_1021[0][0]']     \n",
            "                                                                                                  \n",
            " average_pooling2d_97 (Aver  (None, None, None, 1280)     0         ['mixed8[0][0]']              \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_1016 (Conv2D)        (None, None, None, 320)      409600    ['mixed8[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_1018 (  (None, None, None, 384)      1152      ['conv2d_1018[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_1019 (  (None, None, None, 384)      1152      ['conv2d_1019[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_1022 (  (None, None, None, 384)      1152      ['conv2d_1022[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_1023 (  (None, None, None, 384)      1152      ['conv2d_1023[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " conv2d_1024 (Conv2D)        (None, None, None, 192)      245760    ['average_pooling2d_97[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_1016 (  (None, None, None, 320)      960       ['conv2d_1016[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1018 (Activatio  (None, None, None, 384)      0         ['batch_normalization_1018[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " activation_1019 (Activatio  (None, None, None, 384)      0         ['batch_normalization_1019[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " activation_1022 (Activatio  (None, None, None, 384)      0         ['batch_normalization_1022[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " activation_1023 (Activatio  (None, None, None, 384)      0         ['batch_normalization_1023[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " batch_normalization_1024 (  (None, None, None, 192)      576       ['conv2d_1024[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1016 (Activatio  (None, None, None, 320)      0         ['batch_normalization_1016[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " mixed9_0 (Concatenate)      (None, None, None, 768)      0         ['activation_1018[0][0]',     \n",
            "                                                                     'activation_1019[0][0]']     \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenat  (None, None, None, 768)      0         ['activation_1022[0][0]',     \n",
            " e)                                                                  'activation_1023[0][0]']     \n",
            "                                                                                                  \n",
            " activation_1024 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1024[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " mixed9 (Concatenate)        (None, None, None, 2048)     0         ['activation_1016[0][0]',     \n",
            "                                                                     'mixed9_0[0][0]',            \n",
            "                                                                     'concatenate_20[0][0]',      \n",
            "                                                                     'activation_1024[0][0]']     \n",
            "                                                                                                  \n",
            " average_pooling2d_98 (Aver  (None, None, None, 2048)     0         ['mixed9[0][0]']              \n",
            " agePooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_1033 (Conv2D)        (None, None, None, 192)      393216    ['average_pooling2d_98[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_1033 (  (None, None, None, 192)      576       ['conv2d_1033[0][0]']         \n",
            " BatchNormalization)                                                                              \n",
            "                                                                                                  \n",
            " activation_1033 (Activatio  (None, None, None, 192)      0         ['batch_normalization_1033[0][\n",
            " n)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 192)                  0         ['activation_1033[0][0]']     \n",
            " 0 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " dense_19 (Dense)            (None, 1024)                 197632    ['global_average_pooling2d_10[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_20 (Dense)            (None, 525)                  538125    ['dense_19[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16852269 (64.29 MB)\n",
            "Trainable params: 735757 (2.81 MB)\n",
            "Non-trainable params: 16116512 (61.48 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Assuming `model` is your TensorFlow/Keras model\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "NKm6-a6SnxrR",
        "outputId": "cdad712d-52fa-480d-8159-76f977a05d91"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1919, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 525) are incompatible\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-2e5b593a1f60>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Evaluate the model on your test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# Print the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1919, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 525) are incompatible\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "# DenseNet121_model.h5\n",
        "\n",
        "for model_name, model in models.items():\n",
        "  # Load the saved model\n",
        "  loaded_model = load_model(f'/content/{model_name}_model.h5')\n",
        "\n",
        "  # Evaluate the model on your test dataset\n",
        "  loss, accuracy = loaded_model.evaluate(test_dataset)\n",
        "\n",
        "  # Print the accuracy\n",
        "  print(f'Test Accuracy: {accuracy}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWfb_Bt9pg4v",
        "outputId": "91ec802a-0e0e-43d3-b6e8-115f78a333fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83/83 [==============================] - 7s 75ms/step - loss: 0.8566 - f1_score: 0.7905 - accuracy: 0.7981\n",
            "InceptionV3 Test Loss: 0.8566338419914246\n",
            "InceptionV3 Test Accuracy: 0.7980952262878418\n",
            "83/83 [==============================] - 10s 109ms/step - loss: 6.2640 - f1_score: 7.2424e-06 - accuracy: 0.0019\n",
            "ResNet50 Test Loss: 6.2639994621276855\n",
            "ResNet50 Test Accuracy: 0.0019047618843615055\n",
            "83/83 [==============================] - 8s 73ms/step - loss: 6.2640 - f1_score: 7.2424e-06 - accuracy: 0.0019\n",
            "EfficientNetB0 Test Loss: 6.263988018035889\n",
            "EfficientNetB0 Test Accuracy: 0.0019047618843615055\n",
            "83/83 [==============================] - 7s 70ms/step - loss: 0.5855 - f1_score: 0.8279 - accuracy: 0.8339\n",
            "MobileNetV2 Test Loss: 0.5855067372322083\n",
            "MobileNetV2 Test Accuracy: 0.8339047431945801\n",
            "83/83 [==============================] - 11s 96ms/step - loss: 0.2483 - f1_score: 0.9232 - accuracy: 0.9265\n",
            "DenseNet121 Test Loss: 0.2482743263244629\n",
            "DenseNet121 Test Accuracy: 0.9264761805534363\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Load the saved model\n",
        "    loaded_model = load_model(f'/content/{model_name}_model.h5')\n",
        "\n",
        "    # Evaluate the model on your test dataset\n",
        "    results = loaded_model.evaluate(test_data)\n",
        "\n",
        "    # Extract loss and accuracy from the results\n",
        "    loss = results[0]\n",
        "    accuracy = results[2]  # Adjust the index based on the position of accuracy in the results\n",
        "\n",
        "    # Print the loss and accuracy\n",
        "    print(f'{model_name} Test Loss: {loss}')\n",
        "    print(f'{model_name} Test Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ySgXD-Gxq9WE"
      },
      "outputs": [],
      "source": [
        "for model_name,model in models.items():\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy', 'f1_score'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffIQmODCrRbJ",
        "outputId": "52a34ef9-245a-45dc-c8f4-da98f820fdf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "InceptionV3 History Keys: dict_keys([])\n",
            "ResNet50 History Keys: dict_keys([])\n",
            "EfficientNetB0 History Keys: dict_keys([])\n",
            "MobileNetV2 History Keys: dict_keys([])\n",
            "DenseNet121 History Keys: dict_keys([])\n"
          ]
        }
      ],
      "source": [
        "for model_name, model in models.items():\n",
        "    # Load history\n",
        "    history = model.history\n",
        "\n",
        "    # Print keys\n",
        "    print(f'{model_name} History Keys:', history.history.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "cyK7btixp5f9",
        "outputId": "5de583e2-d832-49de-8d83-e4992b14f99c"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'val_accuracy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-476b8f34ab9c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m'acc'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{model_name} Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_accuracy'"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Load history\n",
        "    history = model.history\n",
        "\n",
        "    # Plot accuracy and loss\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    if 'accuracy' in history.history:\n",
        "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    elif 'acc' in history.history:\n",
        "        plt.plot(history.history['acc'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{model_name} Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slYcfG99qvAY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
